{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import ast\n",
    "import yaml\n",
    "import logging\n",
    "\n",
    "import yaml\n",
    "import datetime as dt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# sc = SparkContext()\n",
    "# sc.setLogLevel('INFO')\n",
    "logger = logging.getLogger('reddit_streaming')\n",
    "\n",
    "spark_host = \"spark-master\" \n",
    "kafka_host = \"kafka\" \n",
    "subreddit = \"aws\"\n",
    "spark_version = \"3.3.2\"\n",
    "hadoop_version = \"3.3.4\"\n",
    "delta_version = \"2.3.0\"\n",
    "postgres_version = \"9.4.1212\"\n",
    "# aws_client = ast.literal_eval(secretmanager_client.get_secret_value(SecretId=\"AWS_ACCESS_KEY_ID\")[\"SecretString\"])[\"AWS_ACCESS_KEY_ID\"]\n",
    "# aws_secret = ast.literal_eval(secretmanager_client.get_secret_value(SecretId=\"AWS_SECRET_ACCESS_KEY\")[\"SecretString\"])[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "extra_jar_list = f\"org.apache.spark:spark-sql-kafka-0-10_2.12:{spark_version},org.apache.hadoop:hadoop-common:{hadoop_version},org.apache.hadoop:hadoop-aws:{hadoop_version},org.apache.hadoop:hadoop-client:{hadoop_version},io.delta:delta-core_2.12:{delta_version},org.postgresql:postgresql:{postgres_version}\"\n",
    "bucket = \"reddit-streaming-stevenhurwitt-2\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Set Java environment variables\n",
    "os.environ['JAVA_HOME'] = '/usr/local/openjdk-8'\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['SPARK_LOCAL_IP'] = 'localhost'\n",
    "os.environ['SPARK_LOCAL_DIRS'] = '/opt/workspace/tmp/spark'\n",
    "os.environ['SPARK_LOG_DIR'] = '/opt/workspace/events'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_files():\n",
    "    \"\"\"\n",
    "    initializes spark session using config.yaml and creds.json files.\n",
    "    \"\"\"\n",
    "\n",
    "    base = os.getcwd()\n",
    "    creds_path_container = os.path.join(base, \"creds.json\")\n",
    "\n",
    "    creds_dir = \"/\".join(base.split(\"/\")[:-3])\n",
    "    creds_path = os.path.join(base, \"creds.json\")\n",
    "\n",
    "    try:\n",
    "        with open(creds_path, \"r\") as f:\n",
    "            creds = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        # print(\"couldn't find: {}.\".format(creds_path))\n",
    "        try:\n",
    "            with open(creds_path_container, \"r\") as f:\n",
    "                creds = json.load(f)\n",
    "                f.close()\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            with open(\"/opt/workspace//redditStreaming/creds.json\", \"r\") as f:\n",
    "                creds = json.load(f)\n",
    "                f.close()\n",
    "\n",
    "    except:\n",
    "        print(\"failed to find creds.json.\")\n",
    "        sys.exit()\n",
    "\n",
    "    try:\n",
    "        with open(\"config.yaml\", \"r\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "            # print(\"read config file.\")\n",
    "            f.close()\n",
    "\n",
    "    except:\n",
    "        print(\"failed to find config.yaml, exiting now.\")\n",
    "        sys.exit()\n",
    "\n",
    "    return(creds, config)\n",
    "\n",
    "def init_spark(subreddit, index):\n",
    "    \"\"\"\n",
    "    initialize spark given config and credential's files\n",
    "\n",
    "    returns: spark, sparkContext (sc)\n",
    "    raises: Exception if Spark session creation fails\n",
    "    \"\"\"\n",
    "    creds, config = read_files()\n",
    "    spark_host = config[\"spark_host\"]\n",
    "    extra_jar_list = config[\"extra_jar_list\"]\n",
    "\n",
    "    # Set Java specific configurations\n",
    "    # os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "    # os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "    \n",
    "    \n",
    "    # initialize spark session\n",
    "    try:\n",
    "        spark = SparkSession.builder.appName(f\"reddit_{subreddit}\") \\\n",
    "                    .master(\"local[*]\") \\\n",
    "                    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "                    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "                    .config(\"spark.scheduler.mode\", \"FAIR\") \\\n",
    "                    .config(\"spark.scheduler.allocation.file\", \"file:///opt/workspace/redditStreaming/fairscheduler.xml\") \\\n",
    "                    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "                    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "                    .config(\"spark.executor.cores\", \"2\") \\\n",
    "                    .config(\"spark.streaming.concurrentJobs\", \"8\") \\\n",
    "                    .config(\"spark.local.dir\", \"/opt/workspace/tmp/spark\") \\\n",
    "                    .config(\"spark.worker.dir\", \"/opt/workspace/tmp/executor/{}/\".format(subreddit)) \\\n",
    "                    .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "                    .config(\"spark.eventLog.dir\", \"file:///opt/workspace/events\") \\\n",
    "                    .config(\"spark.sql.debug.maxToStringFields\", 1000) \\\n",
    "                    .config(\"spark.jars.packages\", extra_jar_list) \\\n",
    "                    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "                    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "                    # .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "                    \n",
    "\n",
    "        sc = spark.sparkContext\n",
    "        sc.setLogLevel('WARN')\n",
    "        sc.setLocalProperty(\"spark.scheduler.pool\", \"pool{}\".format(str(index)))\n",
    "        \n",
    "        print(\"Created Spark session successfully\")\n",
    "        return spark, sc\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create Spark session: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "def read_kafka_stream(spark, sc, subreddit):\n",
    "    \"\"\"\n",
    "    reads streaming data from kafka producer\n",
    "\n",
    "    params: spark, sc\n",
    "    returns: df\n",
    "    \"\"\"\n",
    "    creds, config = read_files()\n",
    "    kafka_host = config[\"kafka_host\"]\n",
    "\n",
    "    # define schema for payload data\n",
    "    payload_schema = StructType([\n",
    "        StructField(\"approved_at_utc\", FloatType(), True),\n",
    "        StructField(\"subreddit\", StringType(), False),\n",
    "        StructField(\"selftext\", StringType(), False),\n",
    "        StructField(\"author_fullname\", StringType(), False),\n",
    "        StructField(\"saved\", BooleanType(), False),\n",
    "        StructField(\"mod_reason_title\", StringType(), True),\n",
    "        StructField(\"gilded\", IntegerType(), False),\n",
    "        StructField(\"clicked\", BooleanType(), False),\n",
    "        StructField(\"title\", StringType(), False),\n",
    "        StructField(\"subreddit_name_prefixed\", StringType(), False),\n",
    "        StructField(\"hidden\", BooleanType(), False),\n",
    "        StructField(\"pwls\", IntegerType(), False),\n",
    "        StructField(\"link_flair_css_class\", StringType(), False),\n",
    "        StructField(\"downs\", IntegerType(), False),\n",
    "        StructField(\"thumbnail_height\", IntegerType(), True),\n",
    "        StructField(\"top_awarded_type\", StringType(), True),\n",
    "        StructField(\"hide_score\", BooleanType(), False),\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"quarantine\", BooleanType(), False),\n",
    "        StructField(\"link_flair_text_color\", StringType(), True),\n",
    "        StructField(\"upvote_ratio\", FloatType(), False),\n",
    "        StructField(\"author_flair_background_color\", StringType(), True),\n",
    "        StructField(\"ups\", IntegerType(), False),\n",
    "        StructField(\"total_awards_received\", IntegerType(), False),\n",
    "        StructField(\"thumbnail_width\", IntegerType(), True),\n",
    "        StructField(\"author_flair_template_id\", StringType(), True),\n",
    "        StructField(\"is_original_content\", BooleanType(), False),\n",
    "        StructField(\"secure_media\", StringType(), True),\n",
    "        StructField(\"is_reddit_media_domain\", BooleanType(), False),\n",
    "        StructField(\"is_meta\", BooleanType(), False),\n",
    "        StructField(\"category\", StringType(), True),\n",
    "        StructField(\"link_flair_text\", StringType(), True),\n",
    "        StructField(\"can_mod_post\", BooleanType(), False),\n",
    "        StructField(\"score\", IntegerType(), False),\n",
    "        StructField(\"approved_by\", StringType(), True),\n",
    "        StructField(\"is_created_from_ads_ui\", BooleanType(), False),\n",
    "        StructField(\"author_premium\", BooleanType(), False),\n",
    "        StructField(\"thumbnail\", StringType(), True),\n",
    "        StructField(\"edited\", BooleanType(), False),\n",
    "        StructField(\"author_flair_css_class\", StringType(), True),\n",
    "        StructField(\"post_hint\", StringType(), False),\n",
    "        StructField(\"content_categories\", StringType(), True),\n",
    "        StructField(\"is_self\", BooleanType(), False),\n",
    "        StructField(\"subreddit_type\", StringType(), False),\n",
    "        StructField(\"created\", FloatType(), False),\n",
    "        StructField(\"link_flair_type\", StringType(), True),\n",
    "        StructField(\"wls\", IntegerType(), False),\n",
    "        StructField(\"removed_by_category\", StringType(), True),\n",
    "        StructField(\"banned_by\", StringType(), True),\n",
    "        StructField(\"author_flair_type\", StringType(), True),\n",
    "        StructField(\"domain\", StringType(), True),\n",
    "        StructField(\"allow_live_comments\", BooleanType(), False),\n",
    "        StructField(\"selftext_html\", StringType(), True),\n",
    "        StructField(\"likes\", IntegerType(), True),\n",
    "        StructField(\"suggested_sort\", StringType(), True),\n",
    "        StructField(\"banned_at_utc\", FloatType(), True),\n",
    "        StructField(\"url_overridden_by_dest\", StringType(), True),\n",
    "        StructField(\"view_count\", IntegerType(), True),\n",
    "        StructField(\"archived\", BooleanType(), False),\n",
    "        StructField(\"no_follow\", BooleanType(), False),\n",
    "        StructField(\"is_crosspostable\", BooleanType(), False),\n",
    "        StructField(\"pinned\", BooleanType(), False),\n",
    "        StructField(\"over_18\", BooleanType(), False),\n",
    "        StructField(\"media_only\", BooleanType(), False),\n",
    "        StructField(\"link_flair_template_id\", StringType(), True),\n",
    "        StructField(\"can_gild\", BooleanType(), False),\n",
    "        StructField(\"spoiler\", BooleanType(), False),\n",
    "        StructField(\"locked\", BooleanType(), False),\n",
    "        StructField(\"author_flair_text\", StringType(), True),\n",
    "        StructField(\"visited\", BooleanType(), False),\n",
    "        StructField(\"removed_by\", StringType(), True),\n",
    "        StructField(\"mod_note\", StringType(), True),\n",
    "        StructField(\"distinguished\", StringType(), True),\n",
    "        StructField(\"subreddit_id\", StringType(), False),\n",
    "        StructField(\"author_is_blocked\", BooleanType(), False),\n",
    "        StructField(\"mod_reason_by\", StringType(), True),\n",
    "        StructField(\"num_reports\", IntegerType(), True),\n",
    "        StructField(\"removal_reason\", StringType(), True),\n",
    "        StructField(\"link_flair_background_color\", StringType(), True),\n",
    "        StructField(\"id\", StringType(), False),\n",
    "        StructField(\"is_robot_indexable\", BooleanType(), False),\n",
    "        StructField(\"report_reasons\", StringType(), True),\n",
    "        StructField(\"author\", StringType(), False),\n",
    "        StructField(\"discussion_type\", StringType(), True),\n",
    "        StructField(\"num_comments\", IntegerType(), False),\n",
    "        StructField(\"send_replies\", BooleanType(), False),\n",
    "        StructField(\"whitelist_status\", StringType(), False),\n",
    "        StructField(\"contest_mode\", BooleanType(), False),\n",
    "        StructField(\"author_patreon_flair\", BooleanType(), False),\n",
    "        StructField(\"author_flair_text_color\", StringType(), True),\n",
    "        StructField(\"permalink\", StringType(), False),\n",
    "        StructField(\"parent_whitelist_status\", StringType(), False),\n",
    "        StructField(\"stickied\", BooleanType(), False),\n",
    "        StructField(\"url\", StringType(), False),\n",
    "        StructField(\"subreddit_subscribers\", IntegerType(), False),\n",
    "        StructField(\"created_utc\", FloatType(), False),\n",
    "        StructField(\"num_crossposts\", IntegerType(), False),\n",
    "        StructField(\"media\", StringType(), True),\n",
    "        StructField(\"is_video\", BooleanType(), False),\n",
    "    ])\n",
    "\n",
    "    # read json from kafka and select all columns\n",
    "    df = spark \\\n",
    "            .readStream \\\n",
    "                .format(\"kafka\") \\\n",
    "                .option(\"kafka.bootstrap.servers\", \"{}:9092\".format(kafka_host)) \\\n",
    "                .option(\"subscribe\", \"reddit_\" + subreddit) \\\n",
    "                .option(\"startingOffsets\", \"latest\") \\\n",
    "                .option(\"failOnDataLoss\", \"false\") \\\n",
    "                .load() \\\n",
    "                .selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "                .select(from_json(col(\"json\"), payload_schema).alias(\"data\")) \\\n",
    "                .select(\"data.*\") \n",
    "\n",
    "    return(df)\n",
    "\n",
    "def write_stream(df, subreddit):\n",
    "    \"\"\"\n",
    "    writes streaming data to s3 data lake\n",
    "\n",
    "    params: df\n",
    "    \"\"\"\n",
    "\n",
    "    creds, config = read_files()\n",
    "\n",
    "    bucket = config[\"bucket\"]\n",
    "    # logger.info(\"bucket: {}\".format(bucket))\n",
    "    logger.info(\"subreddit: {}\".format(subreddit))\n",
    "    write_path = f\"s3a://{bucket}/{subreddit}\"\n",
    "    logger.info(\"write path: {}\".format(write_path))\n",
    "\n",
    "    # write subset of df to console\n",
    "    df.withColumn(\"created_utc\", col(\"created_utc\").cast(\"timestamp\")) \\\n",
    "        .select(\"subreddit\", \"title\", \"score\", \"created_utc\") \\\n",
    "        .writeStream \\\n",
    "        .trigger(processingTime='180 seconds') \\\n",
    "        .option(\"truncate\", \"true\") \\\n",
    "        .option(\"checkpointLocation\", \"file:///opt/workspace/checkpoints/{}_console\".format(subreddit)) \\\n",
    "        .outputMode(\"update\") \\\n",
    "        .format(\"console\") \\\n",
    "        .queryName(subreddit + \"_console\") \\\n",
    "        .start()\n",
    "\n",
    "    # write to s3 delta\n",
    "    df.writeStream \\\n",
    "        .trigger(processingTime=\"180 seconds\") \\\n",
    "        .format(\"delta\") \\\n",
    "        .option(\"path\", write_path) \\\n",
    "        .option(\"checkpointLocation\", \"file:///opt/workspace/checkpoints/{}\".format(subreddit)) \\\n",
    "        .option(\"header\", True) \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .queryName(subreddit + \"_delta\") \\\n",
    "        .start(f\"/opt/workspace/data/{subreddit}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Spark for subreddit: aws\n",
      "Failed to create Spark session: Java gateway process exited before sending its port number\n",
      "Error processing subreddit aws: Java gateway process exited before sending its port number\n",
      "Initializing Spark for subreddit: bikinibottomtwitter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 71: /usr/lib/jvm/java-11-openjdk-amd64/bin/java: No such file or directory\n",
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 96: CMD: bad array subscript\n",
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 71: /usr/lib/jvm/java-11-openjdk-amd64/bin/java: No such file or directory\n",
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 96: CMD: bad array subscript\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to create Spark session: Java gateway process exited before sending its port number\n",
      "Error processing subreddit bikinibottomtwitter: Java gateway process exited before sending its port number\n",
      "Initializing Spark for subreddit: BlackPeopleTwitter\n",
      "Failed to create Spark session: Java gateway process exited before sending its port number\n",
      "Error processing subreddit BlackPeopleTwitter: Java gateway process exited before sending its port number\n",
      "Initializing Spark for subreddit: WhitePeopleTwitter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 71: /usr/lib/jvm/java-11-openjdk-amd64/bin/java: No such file or directory\n",
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 96: CMD: bad array subscript\n",
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 71: /usr/lib/jvm/java-11-openjdk-amd64/bin/java: No such file or directory\n",
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 96: CMD: bad array subscript\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to create Spark session: Java gateway process exited before sending its port number\n",
      "Error processing subreddit WhitePeopleTwitter: Java gateway process exited before sending its port number\n",
      "Initializing Spark for subreddit: news\n",
      "Failed to create Spark session: Java gateway process exited before sending its port number\n",
      "Error processing subreddit news: Java gateway process exited before sending its port number\n",
      "Initializing Spark for subreddit: ProgrammerHumor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 71: /usr/lib/jvm/java-11-openjdk-amd64/bin/java: No such file or directory\n",
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 96: CMD: bad array subscript\n",
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 71: /usr/lib/jvm/java-11-openjdk-amd64/bin/java: No such file or directory\n",
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 96: CMD: bad array subscript\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to create Spark session: Java gateway process exited before sending its port number\n",
      "Error processing subreddit ProgrammerHumor: Java gateway process exited before sending its port number\n",
      "Initializing Spark for subreddit: technology\n",
      "Failed to create Spark session: Java gateway process exited before sending its port number\n",
      "Error processing subreddit technology: Java gateway process exited before sending its port number\n",
      "Initializing Spark for subreddit: worldsnews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 71: /usr/lib/jvm/java-11-openjdk-amd64/bin/java: No such file or directory\n",
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 96: CMD: bad array subscript\n",
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 71: /usr/lib/jvm/java-11-openjdk-amd64/bin/java: No such file or directory\n",
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 96: CMD: bad array subscript\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to create Spark session: Java gateway process exited before sending its port number\n",
      "Error processing subreddit worldsnews: Java gateway process exited before sending its port number\n",
      "Fatal error: No streams were successfully created\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'stop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1449/1041619538.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstreams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No streams were successfully created\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: No streams were successfully created",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1449/1041619538.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Fatal error: {str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'spark'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'stop'"
     ]
    }
   ],
   "source": [
    "spark = None\n",
    "streams = []\n",
    "\n",
    "try:\n",
    "    creds, config = read_files()\n",
    "    subreddit_list = config[\"subreddit\"]\n",
    "    extra_jar_list = config[\"extra_jar_list\"]\n",
    "\n",
    "    # Initialize Delta Lake (add this before creating SparkSession)\n",
    "    builder = SparkSession.builder \\\n",
    "        .appName(\"reddit_streaming\") \\\n",
    "        .config(\"spark.jars.packages\", extra_jar_list) \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "    # Validate configuration\n",
    "    if not subreddit_list or not isinstance(subreddit_list, list):\n",
    "        raise ValueError(\"Invalid subreddit configuration\")\n",
    "        \n",
    "    for i, s in enumerate(subreddit_list):\n",
    "        try:\n",
    "            print(f\"Initializing Spark for subreddit: {s}\")\n",
    "            spark, sc = init_spark(s, i)\n",
    "            stage_df = read_kafka_stream(spark, sc, s)\n",
    "            write_stream(stage_df, s)\n",
    "            streams.append(stream)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing subreddit {s}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    if not streams:\n",
    "        raise Exception(\"No streams were successfully created\")\n",
    "\n",
    "    print(\"All streams initialized, awaiting termination...\")\n",
    "    spark.streams.awaitAnyTermination()\n",
    "\n",
    "    # Only await termination if at least one stream was created\n",
    "    if 'spark' in locals():\n",
    "        spark.streams.awaitAnyTermination()\n",
    "    else:\n",
    "        print(\"No streams were successfully created\")\n",
    "        sys.exit(1)\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nShutting down gracefully...\")\n",
    "    for stream in streams:\n",
    "        stream.stop()\n",
    "    if 'spark' in locals():\n",
    "        spark.stop()\n",
    "    sys.exit(0)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Fatal error: {str(e)}\")\n",
    "    if 'spark' in locals():\n",
    "        spark.stop()\n",
    "    sys.exit(1)\n",
    "\n",
    "finally:\n",
    "    for stream in streams:\n",
    "        try:\n",
    "            stream.stop()\n",
    "        except:\n",
    "            pass\n",
    "    if spark:\n",
    "        try:\n",
    "            spark.stop()\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.7/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-common added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.apache.hadoop#hadoop-client added as a dependency\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e402f1b5-a4c7-483e-9411-98e978cf86be;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.2 in local-m2-cache\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.2 in local-m2-cache\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in local-m2-cache\n",
      "\tfound org.lz4#lz4-java;1.8.0 in local-m2-cache\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in local-m2-cache\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in local-m2-cache\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in local-m2-cache\n",
      "\tfound commons-logging#commons-logging;1.1.3 in local-m2-cache\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in local-m2-cache\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-common;3.3.4 in local-m2-cache\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.3.4 in local-m2-cache\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in local-m2-cache\n",
      "\tfound com.google.guava#guava;27.0-jre in local-m2-cache\n",
      "\tfound com.google.guava#failureaccess;1.0 in local-m2-cache\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in local-m2-cache\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in local-m2-cache\n",
      "\tfound org.checkerframework#checker-qual;2.5.2 in local-m2-cache\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in local-m2-cache\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.17 in local-m2-cache\n",
      "\tfound commons-cli#commons-cli;1.2 in local-m2-cache\n",
      "\tfound org.apache.commons#commons-math3;3.1.1 in local-m2-cache\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in local-m2-cache\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in local-m2-cache\n",
      "\tfound commons-codec#commons-codec;1.15 in local-m2-cache\n",
      "\tfound commons-io#commons-io;2.8.0 in local-m2-cache\n",
      "\tfound commons-net#commons-net;3.6 in local-m2-cache\n",
      "\tfound commons-collections#commons-collections;3.2.2 in local-m2-cache\n",
      "\tfound javax.servlet#javax.servlet-api;3.1.0 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-server;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-http;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-io;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-servlet;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-security;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-webapp;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-xml;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound com.sun.jersey#jersey-core;1.19 in local-m2-cache\n",
      "\tfound javax.ws.rs#jsr311-api;1.1.1 in local-m2-cache\n",
      "\tfound com.sun.jersey#jersey-servlet;1.19 in local-m2-cache\n",
      "\tfound com.sun.jersey#jersey-server;1.19 in local-m2-cache\n",
      "\tfound com.sun.jersey#jersey-json;1.19 in local-m2-cache\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in local-m2-cache\n",
      "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in local-m2-cache\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.11 in local-m2-cache\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in local-m2-cache\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in local-m2-cache\n",
      "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in central\n",
      "\tfound ch.qos.reload4j#reload4j;1.2.22 in local-m2-cache\n",
      "\tfound commons-beanutils#commons-beanutils;1.9.4 in local-m2-cache\n",
      "\tfound org.apache.commons#commons-configuration2;2.1.1 in local-m2-cache\n",
      "\tfound org.apache.commons#commons-lang3;3.12.0 in local-m2-cache\n",
      "\tfound org.apache.commons#commons-text;1.4 in local-m2-cache\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in local-m2-cache\n",
      "\tfound org.slf4j#slf4j-reload4j;1.7.36 in local-m2-cache\n",
      "\tfound org.apache.avro#avro;1.7.7 in local-m2-cache\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\n",
      "\tfound org.apache.commons#commons-compress;1.21 in local-m2-cache\n",
      "\tfound com.google.re2j#re2j;1.1 in local-m2-cache\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in local-m2-cache\n",
      "\tfound com.google.code.gson#gson;2.8.9 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-auth;3.3.4 in local-m2-cache\n",
      "\tfound com.nimbusds#nimbus-jose-jwt;9.8.1 in local-m2-cache\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in local-m2-cache\n",
      "\tfound net.minidev#json-smart;2.4.7 in local-m2-cache\n",
      "\tfound net.minidev#accessors-smart;2.4.7 in local-m2-cache\n",
      "\tfound org.ow2.asm#asm;5.0.4 in central\n",
      "\tfound org.apache.zookeeper#zookeeper;3.5.6 in local-m2-cache\n",
      "\tfound org.apache.zookeeper#zookeeper-jute;3.5.6 in central\n",
      "\tfound org.apache.yetus#audience-annotations;0.5.0 in local-m2-cache\n",
      "\tfound org.apache.curator#curator-framework;4.2.0 in local-m2-cache\n",
      "\tfound org.apache.curator#curator-client;4.2.0 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-simplekdc;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-client;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerby-config;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-core;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerby-pkix;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerby-asn1;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerby-util;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-common;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-crypto;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-util;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#token-provider;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-admin;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-server;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-identity;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerby-xdr;1.0.1 in local-m2-cache\n",
      "\tfound com.jcraft#jsch;0.1.55 in local-m2-cache\n",
      "\tfound org.apache.curator#curator-recipes;4.2.0 in local-m2-cache\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.12.7 in local-m2-cache\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.12.7 in local-m2-cache\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.12.7 in local-m2-cache\n",
      "\tfound org.codehaus.woodstox#stax2-api;4.2.1 in local-m2-cache\n",
      "\tfound com.fasterxml.woodstox#woodstox-core;5.3.0 in local-m2-cache\n",
      "\tfound dnsjava#dnsjava;2.1.7 in local-m2-cache\n",
      "\tfound jakarta.activation#jakarta.activation-api;1.2.1 in local-m2-cache\n",
      "\tfound javax.servlet.jsp#jsp-api;2.1 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in local-m2-cache\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in local-m2-cache\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-client;3.3.4 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-hdfs-client;3.3.4 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-yarn-api;3.3.4 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-yarn-client;3.3.4 in local-m2-cache\n",
      "\tfound org.eclipse.jetty.websocket#websocket-client;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-client;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.eclipse.jetty.websocket#websocket-common;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.eclipse.jetty.websocket#websocket-api;9.4.43.v20210629 in local-m2-cache\n",
      "\tfound org.jline#jline;3.9.0 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-mapreduce-client-core;3.3.4 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-yarn-common;3.3.4 in local-m2-cache\n",
      "\tfound com.sun.jersey#jersey-client;1.19 in local-m2-cache\n",
      "\tfound com.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.12.7 in local-m2-cache\n",
      "\tfound jakarta.xml.bind#jakarta.xml.bind-api;2.3.2 in local-m2-cache\n",
      "\tfound com.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.12.7 in local-m2-cache\n",
      "\tfound com.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.12.7 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-mapreduce-client-jobclient;3.3.4 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-mapreduce-client-common;3.3.4 in local-m2-cache\n",
      "\tfound io.delta#delta-core_2.12;2.3.0 in local-m2-cache\n",
      "\tfound io.delta#delta-storage;2.3.0 in local-m2-cache\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in local-m2-cache\n",
      "\tfound org.postgresql#postgresql;9.4.1212 in central\n",
      ":: resolution report :: resolve 4050ms :: artifacts dl 191ms\n",
      "\t:: modules in use:\n",
      "\tch.qos.reload4j#reload4j;1.2.22 from local-m2-cache in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from local-m2-cache in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.12.7 from local-m2-cache in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.12.7 from local-m2-cache in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.12.7 from local-m2-cache in [default]\n",
      "\tcom.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.12.7 from local-m2-cache in [default]\n",
      "\tcom.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.12.7 from local-m2-cache in [default]\n",
      "\tcom.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.12.7 from local-m2-cache in [default]\n",
      "\tcom.fasterxml.woodstox#woodstox-core;5.3.0 from local-m2-cache in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from local-m2-cache in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from local-m2-cache in [default]\n",
      "\tcom.google.code.gson#gson;2.8.9 from local-m2-cache in [default]\n",
      "\tcom.google.guava#failureaccess;1.0 from local-m2-cache in [default]\n",
      "\tcom.google.guava#guava;27.0-jre from local-m2-cache in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from local-m2-cache in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from local-m2-cache in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from local-m2-cache in [default]\n",
      "\tcom.google.re2j#re2j;1.1 from local-m2-cache in [default]\n",
      "\tcom.jcraft#jsch;0.1.55 from local-m2-cache in [default]\n",
      "\tcom.nimbusds#nimbus-jose-jwt;9.8.1 from local-m2-cache in [default]\n",
      "\tcom.sun.jersey#jersey-client;1.19 from local-m2-cache in [default]\n",
      "\tcom.sun.jersey#jersey-core;1.19 from local-m2-cache in [default]\n",
      "\tcom.sun.jersey#jersey-json;1.19 from local-m2-cache in [default]\n",
      "\tcom.sun.jersey#jersey-server;1.19 from local-m2-cache in [default]\n",
      "\tcom.sun.jersey#jersey-servlet;1.19 from local-m2-cache in [default]\n",
      "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from local-m2-cache in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.3 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.9.4 from local-m2-cache in [default]\n",
      "\tcommons-cli#commons-cli;1.2 from local-m2-cache in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from local-m2-cache in [default]\n",
      "\tcommons-collections#commons-collections;3.2.2 from local-m2-cache in [default]\n",
      "\tcommons-io#commons-io;2.8.0 from local-m2-cache in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from local-m2-cache in [default]\n",
      "\tcommons-net#commons-net;3.6 from local-m2-cache in [default]\n",
      "\tdnsjava#dnsjava;2.1.7 from local-m2-cache in [default]\n",
      "\tio.delta#delta-core_2.12;2.3.0 from local-m2-cache in [default]\n",
      "\tio.delta#delta-storage;2.3.0 from local-m2-cache in [default]\n",
      "\tjakarta.activation#jakarta.activation-api;1.2.1 from local-m2-cache in [default]\n",
      "\tjakarta.xml.bind#jakarta.xml.bind-api;2.3.2 from local-m2-cache in [default]\n",
      "\tjavax.servlet#javax.servlet-api;3.1.0 from local-m2-cache in [default]\n",
      "\tjavax.servlet.jsp#jsp-api;2.1 from local-m2-cache in [default]\n",
      "\tjavax.ws.rs#jsr311-api;1.1.1 from local-m2-cache in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.11 from local-m2-cache in [default]\n",
      "\tnet.minidev#accessors-smart;2.4.7 from local-m2-cache in [default]\n",
      "\tnet.minidev#json-smart;2.4.7 from local-m2-cache in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from local-m2-cache in [default]\n",
      "\torg.apache.avro#avro;1.7.7 from local-m2-cache in [default]\n",
      "\torg.apache.commons#commons-compress;1.21 from local-m2-cache in [default]\n",
      "\torg.apache.commons#commons-configuration2;2.1.1 from local-m2-cache in [default]\n",
      "\torg.apache.commons#commons-lang3;3.12.0 from local-m2-cache in [default]\n",
      "\torg.apache.commons#commons-math3;3.1.1 from local-m2-cache in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from local-m2-cache in [default]\n",
      "\torg.apache.commons#commons-text;1.4 from local-m2-cache in [default]\n",
      "\torg.apache.curator#curator-client;4.2.0 from local-m2-cache in [default]\n",
      "\torg.apache.curator#curator-framework;4.2.0 from local-m2-cache in [default]\n",
      "\torg.apache.curator#curator-recipes;4.2.0 from local-m2-cache in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.3.4 from local-m2-cache in [default]\n",
      "\torg.apache.hadoop#hadoop-auth;3.3.4 from local-m2-cache in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from local-m2-cache in [default]\n",
      "\torg.apache.hadoop#hadoop-client;3.3.4 from local-m2-cache in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from local-m2-cache in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from local-m2-cache in [default]\n",
      "\torg.apache.hadoop#hadoop-common;3.3.4 from local-m2-cache in [default]\n",
      "\torg.apache.hadoop#hadoop-hdfs-client;3.3.4 from local-m2-cache in [default]\n",
      "\torg.apache.hadoop#hadoop-mapreduce-client-common;3.3.4 from local-m2-cache in [default]\n",
      "\torg.apache.hadoop#hadoop-mapreduce-client-core;3.3.4 from local-m2-cache in [default]\n",
      "\torg.apache.hadoop#hadoop-mapreduce-client-jobclient;3.3.4 from local-m2-cache in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-api;3.3.4 from local-m2-cache in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-client;3.3.4 from local-m2-cache in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-common;3.3.4 from local-m2-cache in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from local-m2-cache in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 from local-m2-cache in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from local-m2-cache in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from local-m2-cache in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-admin;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-client;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-common;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-core;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-crypto;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-identity;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-server;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-simplekdc;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-util;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerby-asn1;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerby-config;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerby-pkix;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerby-util;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerby-xdr;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#token-provider;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.2 from local-m2-cache in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.2 from local-m2-cache in [default]\n",
      "\torg.apache.yetus#audience-annotations;0.5.0 from local-m2-cache in [default]\n",
      "\torg.apache.zookeeper#zookeeper;3.5.6 from local-m2-cache in [default]\n",
      "\torg.apache.zookeeper#zookeeper-jute;3.5.6 from central in [default]\n",
      "\torg.checkerframework#checker-qual;2.5.2 from local-m2-cache in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from local-m2-cache in [default]\n",
      "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from local-m2-cache in [default]\n",
      "\torg.codehaus.jackson#jackson-xc;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from local-m2-cache in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.17 from local-m2-cache in [default]\n",
      "\torg.codehaus.woodstox#stax2-api;4.2.1 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty#jetty-client;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty#jetty-http;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty#jetty-io;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty#jetty-security;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty#jetty-server;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty#jetty-servlet;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty#jetty-webapp;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty#jetty-xml;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty.websocket#websocket-api;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty.websocket#websocket-client;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.eclipse.jetty.websocket#websocket-common;9.4.43.v20210629 from local-m2-cache in [default]\n",
      "\torg.jline#jline;3.9.0 from local-m2-cache in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from local-m2-cache in [default]\n",
      "\torg.ow2.asm#asm;5.0.4 from central in [default]\n",
      "\torg.postgresql#postgresql;9.4.1212 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 from local-m2-cache in [default]\n",
      "\torg.slf4j#slf4j-reload4j;1.7.36 from local-m2-cache in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from local-m2-cache in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from local-m2-cache in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from local-m2-cache in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.slf4j#slf4j-api;1.7.32 by [org.slf4j#slf4j-api;1.7.36] in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.2] in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 by [org.xerial.snappy#snappy-java;1.1.8.4] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |  128  |   0   |   0   |   3   ||  125  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: WARNINGS\n",
      "\t\t[NOT FOUND  ] org.apache.kafka#kafka-clients;2.8.1!kafka-clients.jar (5ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/root/.m2/repository/org/apache/kafka/kafka-clients/2.8.1/kafka-clients-2.8.1.jar\n",
      "\n",
      "\t\t[NOT FOUND  ] org.apache.commons#commons-math3;3.1.1!commons-math3.jar (0ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/root/.m2/repository/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar\n",
      "\n",
      "\t\t[NOT FOUND  ] commons-io#commons-io;2.8.0!commons-io.jar (1ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/root/.m2/repository/commons-io/commons-io/2.8.0/commons-io-2.8.0.jar\n",
      "\n",
      "\t\t[NOT FOUND  ] org.apache.commons#commons-text;1.4!commons-text.jar (0ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/root/.m2/repository/org/apache/commons/commons-text/1.4/commons-text-1.4.jar\n",
      "\n",
      "\t\t[NOT FOUND  ] org.apache.avro#avro;1.7.7!avro.jar(bundle) (1ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/root/.m2/repository/org/apache/avro/avro/1.7.7/avro-1.7.7.jar\n",
      "\n",
      "\t\t[NOT FOUND  ] org.apache.curator#curator-recipes;4.2.0!curator-recipes.jar(bundle) (8ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/root/.m2/repository/org/apache/curator/curator-recipes/4.2.0/curator-recipes-4.2.0.jar\n",
      "\n",
      "\t\t[NOT FOUND  ] com.google.code.findbugs#jsr305;3.0.2!jsr305.jar (0ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/root/.m2/repository/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar\n",
      "\n",
      "\t\t[NOT FOUND  ] org.apache.zookeeper#zookeeper;3.5.6!zookeeper.jar (0ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/root/.m2/repository/org/apache/zookeeper/zookeeper/3.5.6/zookeeper-3.5.6.jar\n",
      "\n",
      "\t\t[NOT FOUND  ] com.fasterxml.jackson.core#jackson-databind;2.12.7!jackson-databind.jar(bundle) (1ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/root/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.12.7/jackson-databind-2.12.7.jar\n",
      "\n",
      "\t\t[NOT FOUND  ] org.checkerframework#checker-qual;2.5.2!checker-qual.jar (0ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/root/.m2/repository/org/checkerframework/checker-qual/2.5.2/checker-qual-2.5.2.jar\n",
      "\n",
      "\t\t[NOT FOUND  ] org.apache.curator#curator-framework;4.2.0!curator-framework.jar(bundle) (4ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/root/.m2/repository/org/apache/curator/curator-framework/4.2.0/curator-framework-4.2.0.jar\n",
      "\n",
      "\t\t[NOT FOUND  ] com.fasterxml.jackson.core#jackson-annotations;2.12.7!jackson-annotations.jar(bundle) (5ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/root/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.12.7/jackson-annotations-2.12.7.jar\n",
      "\n",
      "\t\t[NOT FOUND  ] com.fasterxml.jackson.core#jackson-core;2.12.7!jackson-core.jar(bundle) (0ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/root/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.12.7/jackson-core-2.12.7.jar\n",
      "\n",
      "\t\t::::::::::::::::::::::::::::::::::::::::::::::\n",
      "\n",
      "\t\t::              FAILED DOWNLOADS            ::\n",
      "\n",
      "\t\t:: ^ see resolution messages for details  ^ ::\n",
      "\n",
      "\t\t::::::::::::::::::::::::::::::::::::::::::::::\n",
      "\n",
      "\t\t:: org.apache.kafka#kafka-clients;2.8.1!kafka-clients.jar\n",
      "\n",
      "\t\t:: com.google.code.findbugs#jsr305;3.0.2!jsr305.jar\n",
      "\n",
      "\t\t:: org.checkerframework#checker-qual;2.5.2!checker-qual.jar\n",
      "\n",
      "\t\t:: org.apache.commons#commons-math3;3.1.1!commons-math3.jar\n",
      "\n",
      "\t\t:: commons-io#commons-io;2.8.0!commons-io.jar\n",
      "\n",
      "\t\t:: org.apache.commons#commons-text;1.4!commons-text.jar\n",
      "\n",
      "\t\t:: org.apache.avro#avro;1.7.7!avro.jar(bundle)\n",
      "\n",
      "\t\t:: org.apache.zookeeper#zookeeper;3.5.6!zookeeper.jar\n",
      "\n",
      "\t\t:: org.apache.curator#curator-framework;4.2.0!curator-framework.jar(bundle)\n",
      "\n",
      "\t\t:: org.apache.curator#curator-recipes;4.2.0!curator-recipes.jar(bundle)\n",
      "\n",
      "\t\t:: com.fasterxml.jackson.core#jackson-databind;2.12.7!jackson-databind.jar(bundle)\n",
      "\n",
      "\t\t:: com.fasterxml.jackson.core#jackson-annotations;2.12.7!jackson-annotations.jar(bundle)\n",
      "\n",
      "\t\t:: com.fasterxml.jackson.core#jackson-core;2.12.7!jackson-core.jar(bundle)\n",
      "\n",
      "\t\t::::::::::::::::::::::::::::::::::::::::::::::\n",
      "\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      "Exception in thread \"main\" java.lang.RuntimeException: [download failed: org.apache.kafka#kafka-clients;2.8.1!kafka-clients.jar, download failed: com.google.code.findbugs#jsr305;3.0.2!jsr305.jar, download failed: org.checkerframework#checker-qual;2.5.2!checker-qual.jar, download failed: org.apache.commons#commons-math3;3.1.1!commons-math3.jar, download failed: commons-io#commons-io;2.8.0!commons-io.jar, download failed: org.apache.commons#commons-text;1.4!commons-text.jar, download failed: org.apache.avro#avro;1.7.7!avro.jar(bundle), download failed: org.apache.zookeeper#zookeeper;3.5.6!zookeeper.jar, download failed: org.apache.curator#curator-framework;4.2.0!curator-framework.jar(bundle), download failed: org.apache.curator#curator-recipes;4.2.0!curator-recipes.jar(bundle), download failed: com.fasterxml.jackson.core#jackson-databind;2.12.7!jackson-databind.jar(bundle), download failed: com.fasterxml.jackson.core#jackson-annotations;2.12.7!jackson-annotations.jar(bundle), download failed: com.fasterxml.jackson.core#jackson-core;2.12.7!jackson-core.jar(bundle)]\n",
      "\tat org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:1456)\n",
      "\tat org.apache.spark.util.DependencyUtils$.resolveMavenDependencies(DependencyUtils.scala:185)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:308)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:901)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1449/2839786809.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m                     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.sql.extensions\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"io.delta.sql.DeltaSparkSessionExtension\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.sql.catalog.spark_catalog\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"org.apache.spark.sql.delta.catalog.DeltaCatalog\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                     \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.delta.logStore.class\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                     \u001b[0;34m.\u001b[0m\u001b[0menableHiveSupport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                     \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m                     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    193\u001b[0m             )\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             self._do_init(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Java gateway process exited before sending its port number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn_info_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "creds, config = read_files()\n",
    "spark_host = config[\"spark_host\"]\n",
    "extra_jar_list = config[\"extra_jar_list\"]\n",
    "\n",
    "# Get list of local JARs\n",
    "jar_dir = \"/opt/workspace/jars\"\n",
    "local_jars = \",\".join([os.path.join(jar_dir, f) for f in os.listdir(jar_dir) if f.endswith('.jar')])\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(f\"reddit_{subreddit}\") \\\n",
    "                    .master(\"local[*]\") \\\n",
    "                    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "                    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "                    .config(\"spark.scheduler.mode\", \"FAIR\") \\\n",
    "                    .config(\"spark.scheduler.allocation.file\", \"file:///opt/workspace/redditStreaming/fairscheduler.xml\") \\\n",
    "                    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "                    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "                    .config(\"spark.executor.cores\", \"2\") \\\n",
    "                    .config(\"spark.streaming.concurrentJobs\", \"8\") \\\n",
    "                    .config(\"spark.local.dir\", \"/opt/workspace/tmp/spark\") \\\n",
    "                    .config(\"spark.worker.dir\", \"/opt/workspace/tmp/executor/{}/\".format(subreddit)) \\\n",
    "                    .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "                    .config(\"spark.eventLog.dir\", \"file:///opt/workspace/events\") \\\n",
    "                    .config(\"spark.sql.debug.maxToStringFields\", 1000) \\\n",
    "                    .config(\"spark.jars\", local_jars) \\\n",
    "                    .config(\"spark.driver.extraClassPath\", local_jars) \\\n",
    "                    .config(\"spark.executor.extraClassPath\", local_jars) \\\n",
    "                    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "                    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "                    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "                    # .config(\"spark.jars.packages\", extra_jar_list) \\\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('WARN')\n",
    "sc.setLocalProperty(\"spark.scheduler.pool\", \"pool{}\".format(str(1)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\", line 540, in send_command\n",
      "    \"Error while sending or receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to connect to Kafka: An error occurred while calling o886.read\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Failed to connect to Kafka",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5930/3023942866.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Test connection before proceeding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtest_kafka_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkafka_host\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Failed to connect to Kafka\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: Failed to connect to Kafka"
     ]
    }
   ],
   "source": [
    "def test_kafka_connection(kafka_host):\n",
    "    \"\"\"Test Kafka connectivity\"\"\"\n",
    "    try:\n",
    "        # Try reading from Kafka to verify connection\n",
    "        test_df = spark.read \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", f\"{kafka_host}:9092\") \\\n",
    "            .option(\"subscribe\", \"test\") \\\n",
    "            .option(\"failOnDataLoss\", \"false\") \\\n",
    "            .load()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to connect to Kafka: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Test connection before proceeding\n",
    "if not test_kafka_connection(kafka_host):\n",
    "    raise Exception(\"Failed to connect to Kafka\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5930/221017953.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;31m# read json from kafka and select all columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mreadStream\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kafka\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mreadStream\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataStreamReader\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \"\"\"\n\u001b[0;32m-> 1087\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataStreamReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, spark)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"SparkSession\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1034\u001b[0m          \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \"\"\"\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_new_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36m_create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             self.gateway_property, self)\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_to_java_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_thread_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36mconnect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m                 self.socket = self.ssl_context.wrap_socket(\n\u001b[1;32m    437\u001b[0m                     self.socket, server_hostname=self.java_address)\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_address\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_connected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "creds, config = read_files()\n",
    "kafka_host = config[\"kafka_host\"]\n",
    "\n",
    "# define schema for payload data\n",
    "payload_schema = StructType([\n",
    "    StructField(\"approved_at_utc\", FloatType(), True),\n",
    "    StructField(\"subreddit\", StringType(), False),\n",
    "    StructField(\"selftext\", StringType(), False),\n",
    "    StructField(\"author_fullname\", StringType(), False),\n",
    "    StructField(\"saved\", BooleanType(), False),\n",
    "    StructField(\"mod_reason_title\", StringType(), True),\n",
    "    StructField(\"gilded\", IntegerType(), False),\n",
    "    StructField(\"clicked\", BooleanType(), False),\n",
    "    StructField(\"title\", StringType(), False),\n",
    "    StructField(\"subreddit_name_prefixed\", StringType(), False),\n",
    "    StructField(\"hidden\", BooleanType(), False),\n",
    "    StructField(\"pwls\", IntegerType(), False),\n",
    "    StructField(\"link_flair_css_class\", StringType(), False),\n",
    "    StructField(\"downs\", IntegerType(), False),\n",
    "    StructField(\"thumbnail_height\", IntegerType(), True),\n",
    "    StructField(\"top_awarded_type\", StringType(), True),\n",
    "    StructField(\"hide_score\", BooleanType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"quarantine\", BooleanType(), False),\n",
    "    StructField(\"link_flair_text_color\", StringType(), True),\n",
    "    StructField(\"upvote_ratio\", FloatType(), False),\n",
    "    StructField(\"author_flair_background_color\", StringType(), True),\n",
    "    StructField(\"ups\", IntegerType(), False),\n",
    "    StructField(\"total_awards_received\", IntegerType(), False),\n",
    "    StructField(\"thumbnail_width\", IntegerType(), True),\n",
    "    StructField(\"author_flair_template_id\", StringType(), True),\n",
    "    StructField(\"is_original_content\", BooleanType(), False),\n",
    "    StructField(\"secure_media\", StringType(), True),\n",
    "    StructField(\"is_reddit_media_domain\", BooleanType(), False),\n",
    "    StructField(\"is_meta\", BooleanType(), False),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"link_flair_text\", StringType(), True),\n",
    "    StructField(\"can_mod_post\", BooleanType(), False),\n",
    "    StructField(\"score\", IntegerType(), False),\n",
    "    StructField(\"approved_by\", StringType(), True),\n",
    "    StructField(\"is_created_from_ads_ui\", BooleanType(), False),\n",
    "    StructField(\"author_premium\", BooleanType(), False),\n",
    "    StructField(\"thumbnail\", StringType(), True),\n",
    "    StructField(\"edited\", BooleanType(), False),\n",
    "    StructField(\"author_flair_css_class\", StringType(), True),\n",
    "    StructField(\"post_hint\", StringType(), False),\n",
    "    StructField(\"content_categories\", StringType(), True),\n",
    "    StructField(\"is_self\", BooleanType(), False),\n",
    "    StructField(\"subreddit_type\", StringType(), False),\n",
    "    StructField(\"created\", FloatType(), False),\n",
    "    StructField(\"link_flair_type\", StringType(), True),\n",
    "    StructField(\"wls\", IntegerType(), False),\n",
    "    StructField(\"removed_by_category\", StringType(), True),\n",
    "    StructField(\"banned_by\", StringType(), True),\n",
    "    StructField(\"author_flair_type\", StringType(), True),\n",
    "    StructField(\"domain\", StringType(), True),\n",
    "    StructField(\"allow_live_comments\", BooleanType(), False),\n",
    "    StructField(\"selftext_html\", StringType(), True),\n",
    "    StructField(\"likes\", IntegerType(), True),\n",
    "    StructField(\"suggested_sort\", StringType(), True),\n",
    "    StructField(\"banned_at_utc\", FloatType(), True),\n",
    "    StructField(\"url_overridden_by_dest\", StringType(), True),\n",
    "    StructField(\"view_count\", IntegerType(), True),\n",
    "    StructField(\"archived\", BooleanType(), False),\n",
    "    StructField(\"no_follow\", BooleanType(), False),\n",
    "    StructField(\"is_crosspostable\", BooleanType(), False),\n",
    "    StructField(\"pinned\", BooleanType(), False),\n",
    "    StructField(\"over_18\", BooleanType(), False),\n",
    "    StructField(\"media_only\", BooleanType(), False),\n",
    "    StructField(\"link_flair_template_id\", StringType(), True),\n",
    "    StructField(\"can_gild\", BooleanType(), False),\n",
    "    StructField(\"spoiler\", BooleanType(), False),\n",
    "    StructField(\"locked\", BooleanType(), False),\n",
    "    StructField(\"author_flair_text\", StringType(), True),\n",
    "    StructField(\"visited\", BooleanType(), False),\n",
    "    StructField(\"removed_by\", StringType(), True),\n",
    "    StructField(\"mod_note\", StringType(), True),\n",
    "    StructField(\"distinguished\", StringType(), True),\n",
    "    StructField(\"subreddit_id\", StringType(), False),\n",
    "    StructField(\"author_is_blocked\", BooleanType(), False),\n",
    "    StructField(\"mod_reason_by\", StringType(), True),\n",
    "    StructField(\"num_reports\", IntegerType(), True),\n",
    "    StructField(\"removal_reason\", StringType(), True),\n",
    "    StructField(\"link_flair_background_color\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), False),\n",
    "    StructField(\"is_robot_indexable\", BooleanType(), False),\n",
    "    StructField(\"report_reasons\", StringType(), True),\n",
    "    StructField(\"author\", StringType(), False),\n",
    "    StructField(\"discussion_type\", StringType(), True),\n",
    "    StructField(\"num_comments\", IntegerType(), False),\n",
    "    StructField(\"send_replies\", BooleanType(), False),\n",
    "    StructField(\"whitelist_status\", StringType(), False),\n",
    "    StructField(\"contest_mode\", BooleanType(), False),\n",
    "    StructField(\"author_patreon_flair\", BooleanType(), False),\n",
    "    StructField(\"author_flair_text_color\", StringType(), True),\n",
    "    StructField(\"permalink\", StringType(), False),\n",
    "    StructField(\"parent_whitelist_status\", StringType(), False),\n",
    "    StructField(\"stickied\", BooleanType(), False),\n",
    "    StructField(\"url\", StringType(), False),\n",
    "    StructField(\"subreddit_subscribers\", IntegerType(), False),\n",
    "    StructField(\"created_utc\", FloatType(), False),\n",
    "    StructField(\"num_crossposts\", IntegerType(), False),\n",
    "    StructField(\"media\", StringType(), True),\n",
    "    StructField(\"is_video\", BooleanType(), False),\n",
    "])\n",
    "\n",
    "# read json from kafka and select all columns\n",
    "df = spark \\\n",
    "        .readStream \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", \"{}:9092\".format(kafka_host)) \\\n",
    "            .option(\"subscribe\", \"reddit_\" + subreddit) \\\n",
    "            .option(\"startingOffsets\", \"latest\") \\\n",
    "            .option(\"failOnDataLoss\", \"false\") \\\n",
    "            .load() \\\n",
    "            .selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "            .select(from_json(col(\"json\"), payload_schema).alias(\"data\")) \\\n",
    "            .select(\"data.*\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"test\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o904.showString.\n: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1638)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:67)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:86)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:86)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:85)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:51)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:122)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:93)\n\tat org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:109)\n\tat org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:106)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:294)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:294)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)\n\tat scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)\n\tat scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:122)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:118)\n\tat org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:136)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:154)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:204)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:249)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:55)\n\t... 65 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5930/1971940084.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o904.showString.\n: org.apache.spark.SparkException: Cannot find catalog plugin class for catalog 'spark_catalog': org.apache.spark.sql.delta.catalog.DeltaCatalog\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.catalogPluginClassNotFoundForCatalogError(QueryExecutionErrors.scala:1638)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:65)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.loadV2SessionCatalog(CatalogManager.scala:67)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$2(CatalogManager.scala:86)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.$anonfun$v2SessionCatalog$1(CatalogManager.scala:86)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.v2SessionCatalog(CatalogManager.scala:85)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.catalog(CatalogManager.scala:51)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentCatalog(CatalogManager.scala:122)\n\tat org.apache.spark.sql.connector.catalog.CatalogManager.currentNamespace(CatalogManager.scala:93)\n\tat org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:109)\n\tat org.apache.spark.sql.catalyst.optimizer.ReplaceCurrentLike.apply(finishAnalysis.scala:106)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.$anonfun$apply$1(Optimizer.scala:294)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:294)\n\tat org.apache.spark.sql.catalyst.optimizer.Optimizer$FinishAnalysis$.apply(Optimizer.scala:276)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)\n\tat scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)\n\tat scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:126)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:122)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:118)\n\tat org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:136)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:154)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:151)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:204)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:249)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.catalog.DeltaCatalog\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.connector.catalog.Catalogs$.load(Catalogs.scala:55)\n\t... 65 more\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"test\")], [\"id\", \"value\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
