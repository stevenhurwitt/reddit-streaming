{
    "metadata": {
        "kernelspec": {
            "name": "SQL",
            "display_name": "SQL",
            "language": "sql"
        },
        "language_info": {
            "name": "sql",
            "version": ""
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
            }
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# database upload"
            ],
            "metadata": {
                "azdata_cell_guid": "ad00c8cb-3c7c-4b3c-9b7b-90d363a11400"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.types import *\n",
                "from pyspark.sql.functions import *\n",
                "import psycopg2\n",
                "import datetime as dt\n",
                "from delta import *\n",
                "import boto3\n",
                "import pprint\n",
                "import yaml\n",
                "import time\n",
                "import json\n",
                "import sys\n",
                "import ast\n",
                "import os\n",
                "\n",
                "pp = pprint.PrettyPrinter(indent = 1)\n",
                "print(\"imported modules.\")"
            ],
            "metadata": {
                "azdata_cell_guid": "ae72a8bf-4240-4823-8ecb-6adc23ce1ae9",
                "language": "sql"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "The code failed because of a fatal error:\n\tError sending http request and maximum retry encountered..\n\nSome things to try:\na) Make sure Spark has enough available resources for Jupyter to create a Spark context.\nb) Contact your Jupyter administrator to make sure the Spark magics library is configured correctly.\nc) Restart the kernel.\n"
                }
            ],
            "execution_count": 1
        },
        {
            "cell_type": "markdown",
            "source": [
                "## creds"
            ],
            "metadata": {
                "azdata_cell_guid": "03ad6b6b-2eef-4d01-b56f-7cdcc3a204e9"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "creds_path = os.path.join(\"/opt\", \"workspace\", \"redditStreaming\", \"creds.json\")\n",
                "\n",
                "try:\n",
                "    with open(creds_path, \"r\") as f:\n",
                "        creds = json.load(f)\n",
                "        print(\"read creds.json.\")\n",
                "        f.close()\n",
                "\n",
                "except:\n",
                "    creds_path = \"/home/steven/Documents/reddit-streaming/redditStreaming/creds.json\"\n",
                "    with open(creds_path, \"r\") as f:\n",
                "        creds = json.load(f)\n",
                "        print(\"read creds.json.\")\n",
                "        f.close()\n",
                "\n",
                "print(\"read creds successfully.\")"
            ],
            "metadata": {
                "azdata_cell_guid": "c7b59afb-04cd-4487-bf28-3760a7198061",
                "language": "sql"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "read creds.json.\nread creds successfully.\n"
                }
            ],
            "execution_count": 2
        },
        {
            "cell_type": "markdown",
            "source": [
                "## spark"
            ],
            "metadata": {
                "azdata_cell_guid": "31cc4147-4b27-4efc-9870-1861546c4ddd"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "spark_host = \"spark-master\"\n",
                "# spark_host = \"spark-master\"\n",
                "aws_client = creds[\"aws_client\"]\n",
                "aws_secret = creds[\"aws_secret\"]\n",
                "index = 0\n",
                "subreddit = \"technology\"\n",
                "\n",
                "# initialize spark session\n",
                "try:\n",
                "    spark = SparkSession.builder.appName(\"reddit_{}\".format(subreddit)) \\\n",
                "                .master(\"spark://{}:7077\".format(spark_host)) \\\n",
                "                .config(\"spark.scheduler.mode\", \"FAIR\") \\\n",
                "                .config(\"spark.scheduler.allocation.file\", \"file:///opt/workspace/redditStreaming/fairscheduler.xml\") \\\n",
                "                .config(\"spark.executor.memory\", \"4096m\") \\\n",
                "                .config(\"spark.executor.cores\", \"4\") \\\n",
                "                .config(\"spark.local.dir\", \"/opt/workspace/tmp/driver/{}/\".format(subreddit)) \\\n",
                "                .config(\"spark.worker.dir\", \"/opt/workspace/tmp/executor/{}/\".format(subreddit)) \\\n",
                "                .config(\"spark.eventLog.enabled\", \"true\") \\\n",
                "                .config(\"spark.eventLog.dir\", \"file:///opt/workspace/events/{}/\".format(subreddit)) \\\n",
                "                .config(\"spark.sql.debug.maxToStringFields\", 1000) \\\n",
                "                .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,org.apache.hadoop:hadoop-common:3.3.1,org.apache.hadoop:hadoop-aws:3.3.1,org.apache.hadoop:hadoop-client:3.3.1,io.delta:delta-core_2.12:1.2.1,org.postgresql:postgresql:42.5.0\") \\\n",
                "                .config(\"spark.hadoop.fs.s3a.access.key\", aws_client) \\\n",
                "                .config(\"spark.hadoop.fs.s3a.secret.key\", aws_secret) \\\n",
                "                .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
                "                .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
                "                .config('spark.hadoop.fs.s3a.buffer.dir', '/opt/workspace/tmp/blocks') \\\n",
                "                .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
                "                .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
                "                .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
                "                .enableHiveSupport() \\\n",
                "                .getOrCreate()\n",
                "\n",
                "    sc = spark.sparkContext\n",
                "    # .config('spark.hadoop.fs.s3a.fast.upload.buffer', 'bytebuffer') \\\n",
                "\n",
                "    sc.setLogLevel('WARN')\n",
                "    sc.setLocalProperty(\"spark.scheduler.pool\", \"pool{}\".format(str(index)))\n",
                "    # sc._jsc.hadoopConfiguration().set(\"fs.s3a.awsAccessKeyId\", aws_client)\n",
                "    # sc._jsc.hadoopConfiguration().set(\"fs.s3a.awsSecretAccessKey\", aws_secret)\n",
                "    # sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.us-east-2.amazonaws.com\")\n",
                "    print(\"created spark successfully\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(e)"
            ],
            "metadata": {
                "azdata_cell_guid": "930cf4ac-4f92-4995-a340-03a056e8e678",
                "language": "sql"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "22/12/12 01:59:10 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\ncreated spark successfully\n"
                }
            ],
            "execution_count": 8
        },
        {
            "cell_type": "code",
            "source": [
                "spark.stop()"
            ],
            "metadata": {
                "azdata_cell_guid": "6a371b96-ad9a-4a0a-970b-12c66eb86777",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": 14
        },
        {
            "cell_type": "markdown",
            "source": [
                "## read clean df"
            ],
            "metadata": {
                "azdata_cell_guid": "47b2576d-2495-49fd-9c96-b40258870631"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "def write_spark_jdbc(subreddit):\n",
                "\n",
                "    creds_path = os.path.join(\"/opt\", \"workspace\", \"redditStreaming\", \"creds.json\")\n",
                "\n",
                "    try:\n",
                "        with open(creds_path, \"r\") as f:\n",
                "            creds = json.load(f)\n",
                "            print(\"read creds.json.\")\n",
                "            f.close()\n",
                "\n",
                "    except:\n",
                "        creds_path = \"/home/steven/Documents/reddit-streaming/redditStreaming/creds.json\"\n",
                "        with open(creds_path, \"r\") as f:\n",
                "            creds = json.load(f)\n",
                "            print(\"read creds.json.\")\n",
                "            f.close()\n",
                "\n",
                "    secretmanager_client = boto3.client(\"secretsmanager\", \n",
                "                                    region_name = \"us-east-2\", \n",
                "                                    aws_access_key_id = creds[\"aws_client\"], \n",
                "                                    aws_secret_access_key = creds[\"aws_secret\"])\n",
                "    \n",
                "    df = spark.read.format(\"delta\").option(\"header\", True).load(\"s3a://reddit-streaming-stevenhurwitt/\" + subreddit + \"_clean\")\n",
                "\n",
                "    db_creds = ast.literal_eval(secretmanager_client.get_secret_value(SecretId=\"dev/reddit/postgres\")[\"SecretString\"])\n",
                "    connect_str = \"jdbc:postgresql://{}:5432/reddit\".format(db_creds[\"host\"])\n",
                "\n",
                "    try:\n",
                "        df.write.format(\"jdbc\") \\\n",
                "            .mode(\"overwrite\") \\\n",
                "            .option(\"url\", connect_str) \\\n",
                "            .option(\"dbtable\", \"reddit.{}\".format(subreddit)) \\\n",
                "            .option(\"user\", db_creds[\"username\"]) \\\n",
                "            .option(\"password\", db_creds[\"password\"]) \\\n",
                "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
                "            .save()\n",
                "\n",
                "        print(\"wrote df to postgresql table.\")\n",
                "\n",
                "    except Exception as e:\n",
                "        print(e)"
            ],
            "metadata": {
                "azdata_cell_guid": "f4a21fcf-2512-4c62-99f5-395d13ceacd6",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": 4
        },
        {
            "cell_type": "code",
            "source": [
                "with open(\"config.yaml\") as g:\n",
                "    config = yaml.safe_load(g)\n",
                "    g.close()"
            ],
            "metadata": {
                "azdata_cell_guid": "2ff3f229-af09-41fc-8277-7b7d4947d949",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": 11
        },
        {
            "cell_type": "code",
            "source": [
                "config[\"subreddit\"]"
            ],
            "metadata": {
                "azdata_cell_guid": "00fe96bc-dd9c-45ea-98f0-dc7c3a727a36",
                "language": "sql"
            },
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 12,
                    "data": {
                        "text/plain": "['technology',\n 'ProgrammerHumor',\n 'news',\n 'worldnews',\n 'BikiniBottomTwitter',\n 'BlackPeopleTwitter',\n 'WhitePeopleTwitter',\n 'aws']"
                    },
                    "metadata": {}
                }
            ],
            "execution_count": 12
        },
        {
            "cell_type": "code",
            "source": [
                "for s in config[\"subreddit\"]:\n",
                "    print(\"subreddit: {}\".format(s))\n",
                "    write_spark_jdbc(s)"
            ],
            "metadata": {
                "azdata_cell_guid": "f231af5a-3c5e-40bf-9406-72f55261b7ad",
                "language": "sql"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "subreddit: technology\nread creds.json.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "                                                                                \r"
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "wrote df to postgresql table.\nsubreddit: ProgrammerHumor\nread creds.json.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "                                                                                \r"
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "wrote df to postgresql table.\nsubreddit: news\nread creds.json.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "                                                                                \r"
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "wrote df to postgresql table.\nsubreddit: worldnews\nread creds.json.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "                                                                                \r"
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "wrote df to postgresql table.\nsubreddit: BikiniBottomTwitter\nread creds.json.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "                                                                                \r"
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "wrote df to postgresql table.\nsubreddit: BlackPeopleTwitter\nread creds.json.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "                                                                                \r"
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "wrote df to postgresql table.\nsubreddit: WhitePeopleTwitter\nread creds.json.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "                                                                                \r"
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "wrote df to postgresql table.\nsubreddit: aws\nread creds.json.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "                                                                                \r"
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "wrote df to postgresql table.\n"
                }
            ],
            "execution_count": 13
        },
        {
            "cell_type": "markdown",
            "source": [
                "## write to postgres table"
            ],
            "metadata": {
                "azdata_cell_guid": "cbe1b93d-c62d-4e0e-bafa-661d4c14d578"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "with open(\"config.yaml\", \"r\") as g:\n",
                "    config = yaml.safe_load(g)\n",
                "    g.close()"
            ],
            "metadata": {
                "azdata_cell_guid": "62db58ef-f383-480a-9251-54a81f20237b",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": 6
        },
        {
            "cell_type": "markdown",
            "source": [
                "### create schema"
            ],
            "metadata": {
                "azdata_cell_guid": "64508a5f-71e3-4be9-8b43-4588d088e5c7"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "conn = psycopg2.connect(host = config[\"postgres_host\"], user = config[\"postgres_user\"], password = config[\"postgres_password\"], database=\"postgres\")\n"
            ],
            "metadata": {
                "azdata_cell_guid": "8b8f8867-a8f6-41d3-b55e-1a9b6fec63c2",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": 17
        },
        {
            "cell_type": "code",
            "source": [
                "connect_str = \"jdbc:postgresql://{}:5432/postgres\".format(config[\"postgres_host\"])\n",
                "\n",
                "try:\n",
                "    df.write.format(\"jdbc\") \\\n",
                "        .mode(\"overwrite\") \\\n",
                "        .option(\"url\", connect_str) \\\n",
                "        .option(\"dbtable\", \"public.{}\".format(subreddit)) \\\n",
                "        .option(\"user\", config[\"postgres_user\"]) \\\n",
                "        .option(\"password\", config[\"postgres_password\"]) \\\n",
                "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
                "        .save()\n",
                "\n",
                "    print(\"wrote df to postgresql table.\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(e)"
            ],
            "metadata": {
                "azdata_cell_guid": "5e39a567-d873-416a-a428-93dc1f132094",
                "language": "sql"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "[Stage 20:==================================================>       (7 + 1) / 8]\r"
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "wrote df to postgresql table.\n"
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": "                                                                                \r"
                }
            ],
            "execution_count": 25
        },
        {
            "cell_type": "markdown",
            "source": [
                "## stop spark"
            ],
            "metadata": {
                "azdata_cell_guid": "e54fedea-14a9-4052-b2b4-6d3b8f30821e"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "try:\n",
                "    spark.stop()\n",
                "\n",
                "except Exception as e:\n",
                "    print(e)"
            ],
            "metadata": {
                "azdata_cell_guid": "65e23599-1cd8-4de0-92a1-d728bcb5ac62",
                "language": "sql"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "SparkSession does not exist in the JVM\n"
                }
            ],
            "execution_count": 24
        },
        {
            "cell_type": "code",
            "source": [],
            "metadata": {
                "azdata_cell_guid": "f0273f14-2d98-4b40-9e25-cbd3c2ef393d",
                "language": "sql"
            },
            "outputs": [],
            "execution_count": null
        }
    ]
}