{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "ad00c8cb-3c7c-4b3c-9b7b-90d363a11400"
            },
            "source": [
                "# database upload"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {
                "azdata_cell_guid": "ae72a8bf-4240-4823-8ecb-6adc23ce1ae9",
                "language": "sql"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "imported modules.\n"
                    ]
                }
            ],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.types import *\n",
                "from pyspark.sql.functions import *\n",
                "import psycopg2\n",
                "import datetime as dt\n",
                "from delta import *\n",
                "import boto3\n",
                "import pprint\n",
                "import yaml\n",
                "import time\n",
                "import json\n",
                "import sys\n",
                "import ast\n",
                "import os\n",
                "\n",
                "pp = pprint.PrettyPrinter(indent = 1)\n",
                "print(\"imported modules.\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "03ad6b6b-2eef-4d01-b56f-7cdcc3a204e9"
            },
            "source": [
                "# creds & config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {
                "azdata_cell_guid": "c7b59afb-04cd-4487-bf28-3760a7198061",
                "language": "sql"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "read creds.json.\n",
                        "read config.yaml.\n",
                        "read creds & config.\n",
                        "read creds successfully.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "23/02/17 05:25:24 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
                    ]
                }
            ],
            "source": [
                "creds_path = os.path.join(\"/opt\", \"workspace\", \"redditStreaming\", \"src\", \"main\", \"python\", \"reddit\", \"creds.json\")\n",
                "config_path = os.path.join(\"/opt\", \"workspace\", \"redditStreaming\", \"src\", \"main\", \"python\", \"reddit\", \"config.yaml\")\n",
                "\n",
                "try:\n",
                "    with open(creds_path, \"r\") as f:\n",
                "        creds = json.load(f)\n",
                "        print(\"read creds.json.\")\n",
                "        f.close()\n",
                "\n",
                "    with open(config_path, \"r\") as g:\n",
                "        config = yaml.safe_load(g)\n",
                "        print(\"read config.yaml.\")\n",
                "        g.close()\n",
                "\n",
                "    print(\"read creds & config.\")\n",
                "\n",
                "except:\n",
                "    creds_path = \"/home/steven/Documents/reddit-streaming/redditStreaming/src/main/python/reddit/creds.json\"\n",
                "    with open(creds_path, \"r\") as f:\n",
                "        creds = json.load(f)\n",
                "        print(\"read creds.json.\")\n",
                "        f.close()\n",
                "\n",
                "print(\"read creds successfully.\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "31cc4147-4b27-4efc-9870-1861546c4ddd"
            },
            "source": [
                "# spark session"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {
                "azdata_cell_guid": "930cf4ac-4f92-4995-a340-03a056e8e678",
                "language": "sql"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "created spark successfully\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 0:>                                                          (0 + 0) / 3]\r"
                    ]
                }
            ],
            "source": [
                "spark_host = \"spark-master\"\n",
                "# spark_host = \"spark-master\"\n",
                "aws_client = creds[\"aws_client\"]\n",
                "aws_secret = creds[\"aws_secret\"]\n",
                "index = 0\n",
                "subreddit = \"technology\"\n",
                "\n",
                "# initialize spark session\n",
                "try:\n",
                "    spark = SparkSession.builder.appName(\"reddit_{}\".format(subreddit)) \\\n",
                "                .master(\"spark://{}:7077\".format(spark_host)) \\\n",
                "                .config(\"spark.scheduler.mode\", \"FAIR\") \\\n",
                "                .config(\"spark.scheduler.allocation.file\", \"file:///opt/workspace/redditStreaming/fairscheduler.xml\") \\\n",
                "                .config(\"spark.executor.memory\", \"4096m\") \\\n",
                "                .config(\"spark.executor.cores\", \"4\") \\\n",
                "                .config(\"spark.local.dir\", \"/opt/workspace/tmp/driver/{}/\".format(subreddit)) \\\n",
                "                .config(\"spark.worker.dir\", \"/opt/workspace/tmp/executor/{}/\".format(subreddit)) \\\n",
                "                .config(\"spark.eventLog.enabled\", \"true\") \\\n",
                "                .config(\"spark.eventLog.dir\", \"file:///opt/workspace/events/{}/\".format(subreddit)) \\\n",
                "                .config(\"spark.sql.debug.maxToStringFields\", 1000) \\\n",
                "                .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,org.apache.hadoop:hadoop-common:3.3.1,org.apache.hadoop:hadoop-aws:3.3.1,org.apache.hadoop:hadoop-client:3.3.1,io.delta:delta-core_2.12:1.2.1,org.postgresql:postgresql:42.5.0\") \\\n",
                "                .config(\"spark.hadoop.fs.s3a.access.key\", aws_client) \\\n",
                "                .config(\"spark.hadoop.fs.s3a.secret.key\", aws_secret) \\\n",
                "                .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
                "                .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
                "                .config('spark.hadoop.fs.s3a.buffer.dir', '/opt/workspace/tmp/blocks') \\\n",
                "                .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
                "                .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
                "                .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
                "                .enableHiveSupport() \\\n",
                "                .getOrCreate()\n",
                "\n",
                "    sc = spark.sparkContext\n",
                "    # .config('spark.hadoop.fs.s3a.fast.upload.buffer', 'bytebuffer') \\\n",
                "\n",
                "    sc.setLogLevel('WARN')\n",
                "    sc.setLocalProperty(\"spark.scheduler.pool\", \"pool{}\".format(str(index)))\n",
                "    # sc._jsc.hadoopConfiguration().set(\"fs.s3a.awsAccessKeyId\", aws_client)\n",
                "    # sc._jsc.hadoopConfiguration().set(\"fs.s3a.awsSecretAccessKey\", aws_secret)\n",
                "    # sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.us-east-2.amazonaws.com\")\n",
                "    print(\"created spark successfully\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(e)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# spark"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "            <div>\n",
                            "                <p><b>SparkSession - hive</b></p>\n",
                            "                \n",
                            "        <div>\n",
                            "            <p><b>SparkContext</b></p>\n",
                            "\n",
                            "            <p><a href=\"http://c297a84e2833:4042\">Spark UI</a></p>\n",
                            "\n",
                            "            <dl>\n",
                            "              <dt>Version</dt>\n",
                            "                <dd><code>v3.3.1</code></dd>\n",
                            "              <dt>Master</dt>\n",
                            "                <dd><code>spark://spark-master:7077</code></dd>\n",
                            "              <dt>AppName</dt>\n",
                            "                <dd><code>reddit_technology</code></dd>\n",
                            "            </dl>\n",
                            "        </div>\n",
                            "        \n",
                            "            </div>\n",
                            "        "
                        ],
                        "text/plain": [
                            "<pyspark.sql.session.SparkSession at 0x7f20d656c160>"
                        ]
                    },
                    "execution_count": 43,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "spark"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# read df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "23/02/17 05:25:39 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "s3a://reddit-streaming-stevenhurwitt-new/aws_clean\n"
                    ]
                }
            ],
            "source": [
                "bucket = config[\"bucket\"]\n",
                "folder = \"aws_clean\"\n",
                "read_path = \"s3a://\" + bucket + \"/\" + folder\n",
                "print(read_path)\n",
                "# df = spark.read.format(\"delta\").option(\"header\", True).load(read_path)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# show df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": [
                "# df.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# write df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "s3a://reddit-streaming-stevenhurwitt-newfinal_aws_clean/\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "23/02/17 05:25:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:26:09 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
                    ]
                }
            ],
            "source": [
                "write_path = \"s3a://\" + bucket + \"final_aws_clean/\"\n",
                "print(write_path)\n",
                "# df.write.format(\"delta\").option(\"header\", True).save(write_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "6a371b96-ad9a-4a0a-970b-12c66eb86777",
                "language": "sql"
            },
            "outputs": [],
            "source": [
                "# spark.stop()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "47b2576d-2495-49fd-9c96-b40258870631"
            },
            "source": [
                "## read clean df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {
                "azdata_cell_guid": "f4a21fcf-2512-4c62-99f5-395d13ceacd6",
                "language": "sql",
                "vscode": {
                    "languageId": "sql"
                }
            },
            "outputs": [],
            "source": [
                "def write_spark_jdbc(subreddit):\n",
                "\n",
                "    creds_path = os.path.join(\"/opt\", \"workspace\", \"redditStreaming\", \"creds.json\")\n",
                "\n",
                "    try:\n",
                "        with open(creds_path, \"r\") as f:\n",
                "            creds = json.load(f)\n",
                "            print(\"read creds.json.\")\n",
                "            f.close()\n",
                "\n",
                "    except:\n",
                "        creds_path = \"/home/steven/Documents/reddit-streaming/redditStreaming/creds.json\"\n",
                "        with open(creds_path, \"r\") as f:\n",
                "            creds = json.load(f)\n",
                "            print(\"read creds.json.\")\n",
                "            f.close()\n",
                "\n",
                "    secretmanager_client = boto3.client(\"secretsmanager\", \n",
                "                                    region_name = \"us-east-2\", \n",
                "                                    aws_access_key_id = creds[\"aws_client\"], \n",
                "                                    aws_secret_access_key = creds[\"aws_secret\"])\n",
                "    \n",
                "    df = spark.read.format(\"delta\").option(\"header\", True).load(\"s3a://reddit-streaming-stevenhurwitt/\" + subreddit + \"_clean\")\n",
                "\n",
                "    db_creds = ast.literal_eval(secretmanager_client.get_secret_value(SecretId=\"dev/reddit/postgres\")[\"SecretString\"])\n",
                "    connect_str = \"jdbc:postgresql://{}:5432/reddit\".format(db_creds[\"host\"])\n",
                "\n",
                "    try:\n",
                "        df.write.format(\"jdbc\") \\\n",
                "            .mode(\"overwrite\") \\\n",
                "            .option(\"url\", connect_str) \\\n",
                "            .option(\"dbtable\", \"reddit.{}\".format(subreddit)) \\\n",
                "            .option(\"user\", db_creds[\"username\"]) \\\n",
                "            .option(\"password\", db_creds[\"password\"]) \\\n",
                "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
                "            .save()\n",
                "\n",
                "        print(\"wrote df to postgresql table.\")\n",
                "\n",
                "    except Exception as e:\n",
                "        print(e)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {
                "azdata_cell_guid": "00fe96bc-dd9c-45ea-98f0-dc7c3a727a36",
                "language": "sql",
                "vscode": {
                    "languageId": "sql"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['technology',\n",
                            " 'ProgrammerHumor',\n",
                            " 'news',\n",
                            " 'worldnews',\n",
                            " 'BikiniBottomTwitter',\n",
                            " 'BlackPeopleTwitter',\n",
                            " 'WhitePeopleTwitter',\n",
                            " 'aws']"
                        ]
                    },
                    "execution_count": 38,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "config[\"subreddit\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {
                "azdata_cell_guid": "f231af5a-3c5e-40bf-9406-72f55261b7ad",
                "language": "sql",
                "vscode": {
                    "languageId": "sql"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "subreddit: technology\n",
                        "read creds.json.\n",
                        "23/02/17 05:16:50 WARN DeltaLog: Failed to parse s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log/_last_checkpoint. This may happen if there was an error during read operation, or a file appears to be partial. Sleeping and trying again.\n",
                        "java.nio.file.AccessDeniedException: s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log/_last_checkpoint: getFileStatus on s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log/_last_checkpoint: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 7EGNZAC6M0AT0Q40; S3 Extended Request ID: JQKBA2rHdpQdTfMrrXbsqDK3bNeYxjIqUnU1jksIHrVLKfD5gZGzigH15bdxDcSHgaLy7Ckcj+Y=; Proxy: null), S3 Extended Request ID: JQKBA2rHdpQdTfMrrXbsqDK3bNeYxjIqUnU1jksIHrVLKfD5gZGzigH15bdxDcSHgaLy7Ckcj+Y=:403 Forbidden\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:249)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:170)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3286)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:4903)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1200)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1178)\n",
                        "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
                        "\tat org.apache.spark.sql.delta.storage.HadoopFileSystemLogStore.read(HadoopFileSystemLogStore.scala:54)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:191)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.lastCheckpoint(Checkpoints.scala:184)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.lastCheckpoint$(Checkpoints.scala:183)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.lastCheckpoint(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:248)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:246)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:245)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:53)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:69)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:567)\n",
                        "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$2(DeltaLog.scala:567)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:437)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:114)\n",
                        "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:77)\n",
                        "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:67)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:437)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:113)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:98)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:437)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:566)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:577)\n",
                        "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n",
                        "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n",
                        "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n",
                        "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n",
                        "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n",
                        "\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n",
                        "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:577)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:584)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:487)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:78)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:78)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:107)\n",
                        "\tat scala.Option.getOrElse(Option.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:107)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:95)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:165)\n",
                        "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$4(DeltaDataSource.scala:187)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:50)\n",
                        "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:164)\n",
                        "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n",
                        "\tat scala.Option.getOrElse(Option.scala:189)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n",
                        "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
                        "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
                        "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
                        "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
                        "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
                        "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
                        "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
                        "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
                        "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
                        "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
                        "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
                        "\tat java.lang.Thread.run(Thread.java:750)\n",
                        "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 7EGNZAC6M0AT0Q40; S3 Extended Request ID: JQKBA2rHdpQdTfMrrXbsqDK3bNeYxjIqUnU1jksIHrVLKfD5gZGzigH15bdxDcSHgaLy7Ckcj+Y=; Proxy: null), S3 Extended Request ID: JQKBA2rHdpQdTfMrrXbsqDK3bNeYxjIqUnU1jksIHrVLKfD5gZGzigH15bdxDcSHgaLy7Ckcj+Y=\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1828)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1412)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1374)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n",
                        "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5227)\n",
                        "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5173)\n",
                        "\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1360)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$6(S3AFileSystem.java:2066)\n",
                        "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:412)\n",
                        "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:375)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2056)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2032)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3273)\n",
                        "\t... 82 more\n",
                        "23/02/17 05:16:52 WARN DeltaLog: Failed to parse s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log/_last_checkpoint. This may happen if there was an error during read operation, or a file appears to be partial. Sleeping and trying again.\n",
                        "java.nio.file.AccessDeniedException: s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log/_last_checkpoint: getFileStatus on s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log/_last_checkpoint: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 4QYW50ZK9VV7BZNH; S3 Extended Request ID: q+Px78Si29cG9CsM5V94b915TNPGZdte0bB9xovhm5QKp/QsF+Xsq2QAqADhXDoSzFLPs64zrdE=; Proxy: null), S3 Extended Request ID: q+Px78Si29cG9CsM5V94b915TNPGZdte0bB9xovhm5QKp/QsF+Xsq2QAqADhXDoSzFLPs64zrdE=:403 Forbidden\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:249)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:170)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3286)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:4903)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1200)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1178)\n",
                        "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
                        "\tat org.apache.spark.sql.delta.storage.HadoopFileSystemLogStore.read(HadoopFileSystemLogStore.scala:54)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:191)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:202)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.lastCheckpoint(Checkpoints.scala:184)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.lastCheckpoint$(Checkpoints.scala:183)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.lastCheckpoint(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:248)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:246)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:245)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:53)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:69)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:567)\n",
                        "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$2(DeltaLog.scala:567)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:437)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:114)\n",
                        "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:77)\n",
                        "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:67)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:437)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:113)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:98)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:437)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:566)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:577)\n",
                        "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n",
                        "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n",
                        "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n",
                        "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n",
                        "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n",
                        "\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n",
                        "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:577)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:584)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:487)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:78)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:78)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:107)\n",
                        "\tat scala.Option.getOrElse(Option.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:107)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:95)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:165)\n",
                        "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$4(DeltaDataSource.scala:187)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:50)\n",
                        "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:164)\n",
                        "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n",
                        "\tat scala.Option.getOrElse(Option.scala:189)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n",
                        "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
                        "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
                        "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
                        "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
                        "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
                        "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
                        "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
                        "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
                        "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
                        "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
                        "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
                        "\tat java.lang.Thread.run(Thread.java:750)\n",
                        "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 4QYW50ZK9VV7BZNH; S3 Extended Request ID: q+Px78Si29cG9CsM5V94b915TNPGZdte0bB9xovhm5QKp/QsF+Xsq2QAqADhXDoSzFLPs64zrdE=; Proxy: null), S3 Extended Request ID: q+Px78Si29cG9CsM5V94b915TNPGZdte0bB9xovhm5QKp/QsF+Xsq2QAqADhXDoSzFLPs64zrdE=\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1828)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1412)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1374)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n",
                        "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5227)\n",
                        "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5173)\n",
                        "\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1360)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$6(S3AFileSystem.java:2066)\n",
                        "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:412)\n",
                        "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:375)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2056)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2032)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3273)\n",
                        "\t... 91 more\n",
                        "23/02/17 05:16:53 WARN DeltaLog: Failed to parse s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log/_last_checkpoint. This may happen if there was an error during read operation, or a file appears to be partial. Sleeping and trying again.\n",
                        "java.nio.file.AccessDeniedException: s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log/_last_checkpoint: getFileStatus on s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log/_last_checkpoint: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: R4JEWBB0120H7ZBK; S3 Extended Request ID: BVHNBASC7EJOUXMF2ZFvw3b+AdlRuvzUE3vyiZw1qST7broa3HIaX7ciYuJ1L2XevmEz5vYDcNQ=; Proxy: null), S3 Extended Request ID: BVHNBASC7EJOUXMF2ZFvw3b+AdlRuvzUE3vyiZw1qST7broa3HIaX7ciYuJ1L2XevmEz5vYDcNQ=:403 Forbidden\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:249)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:170)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3286)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:4903)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1200)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1178)\n",
                        "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
                        "\tat org.apache.spark.sql.delta.storage.HadoopFileSystemLogStore.read(HadoopFileSystemLogStore.scala:54)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:191)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:202)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:202)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.lastCheckpoint(Checkpoints.scala:184)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.lastCheckpoint$(Checkpoints.scala:183)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.lastCheckpoint(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:248)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:246)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:245)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:53)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:69)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:567)\n",
                        "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$2(DeltaLog.scala:567)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:437)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:114)\n",
                        "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:77)\n",
                        "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:67)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:437)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:113)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:98)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:437)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:566)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:577)\n",
                        "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n",
                        "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n",
                        "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n",
                        "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n",
                        "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n",
                        "\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n",
                        "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:577)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:584)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:487)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:78)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:78)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:107)\n",
                        "\tat scala.Option.getOrElse(Option.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:107)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:95)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:165)\n",
                        "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$4(DeltaDataSource.scala:187)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:50)\n",
                        "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:164)\n",
                        "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n",
                        "\tat scala.Option.getOrElse(Option.scala:189)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n",
                        "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
                        "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
                        "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
                        "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
                        "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
                        "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
                        "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
                        "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
                        "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
                        "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
                        "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
                        "\tat java.lang.Thread.run(Thread.java:750)\n",
                        "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: R4JEWBB0120H7ZBK; S3 Extended Request ID: BVHNBASC7EJOUXMF2ZFvw3b+AdlRuvzUE3vyiZw1qST7broa3HIaX7ciYuJ1L2XevmEz5vYDcNQ=; Proxy: null), S3 Extended Request ID: BVHNBASC7EJOUXMF2ZFvw3b+AdlRuvzUE3vyiZw1qST7broa3HIaX7ciYuJ1L2XevmEz5vYDcNQ=\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1828)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1412)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1374)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n",
                        "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5227)\n",
                        "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5173)\n",
                        "\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1360)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$6(S3AFileSystem.java:2066)\n",
                        "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:412)\n",
                        "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:375)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2056)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2032)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3273)\n",
                        "\t... 100 more\n",
                        "23/02/17 05:16:54 WARN DeltaLog: s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log/_last_checkpoint is corrupted. Will search the checkpoint files directly\n",
                        "java.nio.file.AccessDeniedException: s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log/_last_checkpoint: getFileStatus on s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log/_last_checkpoint: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: BMW6J8R2224DWCAH; S3 Extended Request ID: n6wpS7GXJTZFCG0vMxMZY7fYSFg1Z2BOEZKDM3S60QYP6FhBFXii4eDJYMiNzmJkoLaVOgHWrIw=; Proxy: null), S3 Extended Request ID: n6wpS7GXJTZFCG0vMxMZY7fYSFg1Z2BOEZKDM3S60QYP6FhBFXii4eDJYMiNzmJkoLaVOgHWrIw=:403 Forbidden\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:249)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:170)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3286)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:4903)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1200)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1178)\n",
                        "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
                        "\tat org.apache.spark.sql.delta.storage.HadoopFileSystemLogStore.read(HadoopFileSystemLogStore.scala:54)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:191)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:202)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:202)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:202)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.lastCheckpoint(Checkpoints.scala:184)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.lastCheckpoint$(Checkpoints.scala:183)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.lastCheckpoint(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:248)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:246)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:245)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:53)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:69)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:567)\n",
                        "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$2(DeltaLog.scala:567)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:437)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:114)\n",
                        "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:77)\n",
                        "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:67)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:437)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:113)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:98)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:437)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:566)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:577)\n",
                        "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n",
                        "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n",
                        "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n",
                        "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n",
                        "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n",
                        "\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n",
                        "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:577)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:584)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:487)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:78)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:78)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:107)\n",
                        "\tat scala.Option.getOrElse(Option.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:107)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:95)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:165)\n",
                        "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$4(DeltaDataSource.scala:187)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:50)\n",
                        "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:164)\n",
                        "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n",
                        "\tat scala.Option.getOrElse(Option.scala:189)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n",
                        "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
                        "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
                        "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
                        "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
                        "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
                        "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
                        "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
                        "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
                        "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
                        "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
                        "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
                        "\tat java.lang.Thread.run(Thread.java:750)\n",
                        "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: BMW6J8R2224DWCAH; S3 Extended Request ID: n6wpS7GXJTZFCG0vMxMZY7fYSFg1Z2BOEZKDM3S60QYP6FhBFXii4eDJYMiNzmJkoLaVOgHWrIw=; Proxy: null), S3 Extended Request ID: n6wpS7GXJTZFCG0vMxMZY7fYSFg1Z2BOEZKDM3S60QYP6FhBFXii4eDJYMiNzmJkoLaVOgHWrIw=\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1828)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1412)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1374)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n",
                        "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5227)\n",
                        "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5173)\n",
                        "\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1360)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$6(S3AFileSystem.java:2066)\n",
                        "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:412)\n",
                        "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:375)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2056)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2032)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3273)\n",
                        "\t... 109 more\n",
                        "23/02/17 05:16:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
                    ]
                },
                {
                    "ename": "Py4JJavaError",
                    "evalue": "An error occurred while calling o98.load.\n: java.util.concurrent.ExecutionException: java.nio.file.AccessDeniedException: s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log: getFileStatus on s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: BMWFQHRCMTY0VW8V; S3 Extended Request ID: rTuwE5yq2JVxR0Edwhnkf70yX7qtwl4LvQX9YrBx1Sd6zqU7Hf8EoMrlmbBMv78zibahnVO4ZhI=; Proxy: null), S3 Extended Request ID: rTuwE5yq2JVxR0Edwhnkf70yX7qtwl4LvQX9YrBx1Sd6zqU7Hf8EoMrlmbBMv78zibahnVO4ZhI=:403 Forbidden\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)\n\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)\n\tat com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)\n\tat com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:577)\n\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:584)\n\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:487)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:78)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:78)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:107)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:107)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:95)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:165)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$4(DeltaDataSource.scala:187)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:50)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.nio.file.AccessDeniedException: s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log: getFileStatus on s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: BMWFQHRCMTY0VW8V; S3 Extended Request ID: rTuwE5yq2JVxR0Edwhnkf70yX7qtwl4LvQX9YrBx1Sd6zqU7Hf8EoMrlmbBMv78zibahnVO4ZhI=; Proxy: null), S3 Extended Request ID: rTuwE5yq2JVxR0Edwhnkf70yX7qtwl4LvQX9YrBx1Sd6zqU7Hf8EoMrlmbBMv78zibahnVO4ZhI=:403 Forbidden\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:249)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:170)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3286)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3053)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4263)\n\tat org.apache.spark.sql.delta.storage.S3SingleDriverLogStore.listFromInternal(S3SingleDriverLogStore.scala:120)\n\tat org.apache.spark.sql.delta.storage.S3SingleDriverLogStore.listFrom(S3SingleDriverLogStore.scala:140)\n\tat org.apache.spark.sql.delta.Checkpoints.findLastCompleteCheckpoint(Checkpoints.scala:233)\n\tat org.apache.spark.sql.delta.Checkpoints.findLastCompleteCheckpoint$(Checkpoints.scala:224)\n\tat org.apache.spark.sql.delta.DeltaLog.findLastCompleteCheckpoint(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:208)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:202)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:202)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:202)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n\tat org.apache.spark.sql.delta.Checkpoints.lastCheckpoint(Checkpoints.scala:184)\n\tat org.apache.spark.sql.delta.Checkpoints.lastCheckpoint$(Checkpoints.scala:183)\n\tat org.apache.spark.sql.delta.DeltaLog.lastCheckpoint(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:248)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:246)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:245)\n\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:53)\n\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:69)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:567)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$2(DeltaLog.scala:567)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:437)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:114)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:77)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:67)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:437)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:113)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:98)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:437)\n\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:566)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:577)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n\t... 37 more\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: BMWFQHRCMTY0VW8V; S3 Extended Request ID: rTuwE5yq2JVxR0Edwhnkf70yX7qtwl4LvQX9YrBx1Sd6zqU7Hf8EoMrlmbBMv78zibahnVO4ZhI=; Proxy: null), S3 Extended Request ID: rTuwE5yq2JVxR0Edwhnkf70yX7qtwl4LvQX9YrBx1Sd6zqU7Hf8EoMrlmbBMv78zibahnVO4ZhI=\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1828)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1412)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1374)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5227)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5173)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1360)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$6(S3AFileSystem.java:2066)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:412)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:375)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2056)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2032)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3273)\n\t... 112 more\n",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/ipykernel_105188/2193645216.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"subreddit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subreddit: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mwrite_spark_jdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[0;32m/tmp/ipykernel_105188/2994395969.py\u001b[0m in \u001b[0;36mwrite_spark_jdbc\u001b[0;34m(subreddit)\u001b[0m\n\u001b[1;32m     21\u001b[0m                                     aws_secret_access_key = creds[\"aws_secret\"])\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3a://reddit-streaming-stevenhurwitt/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msubreddit\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_clean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mdb_creds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecretmanager_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_secret_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSecretId\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dev/reddit/postgres\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SecretString\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
                        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o98.load.\n: java.util.concurrent.ExecutionException: java.nio.file.AccessDeniedException: s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log: getFileStatus on s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: BMWFQHRCMTY0VW8V; S3 Extended Request ID: rTuwE5yq2JVxR0Edwhnkf70yX7qtwl4LvQX9YrBx1Sd6zqU7Hf8EoMrlmbBMv78zibahnVO4ZhI=; Proxy: null), S3 Extended Request ID: rTuwE5yq2JVxR0Edwhnkf70yX7qtwl4LvQX9YrBx1Sd6zqU7Hf8EoMrlmbBMv78zibahnVO4ZhI=:403 Forbidden\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)\n\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)\n\tat com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)\n\tat com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:577)\n\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:584)\n\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:487)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:78)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:78)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:107)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:107)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:95)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:165)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$4(DeltaDataSource.scala:187)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:50)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.nio.file.AccessDeniedException: s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log: getFileStatus on s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: BMWFQHRCMTY0VW8V; S3 Extended Request ID: rTuwE5yq2JVxR0Edwhnkf70yX7qtwl4LvQX9YrBx1Sd6zqU7Hf8EoMrlmbBMv78zibahnVO4ZhI=; Proxy: null), S3 Extended Request ID: rTuwE5yq2JVxR0Edwhnkf70yX7qtwl4LvQX9YrBx1Sd6zqU7Hf8EoMrlmbBMv78zibahnVO4ZhI=:403 Forbidden\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:249)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:170)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3286)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3053)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4263)\n\tat org.apache.spark.sql.delta.storage.S3SingleDriverLogStore.listFromInternal(S3SingleDriverLogStore.scala:120)\n\tat org.apache.spark.sql.delta.storage.S3SingleDriverLogStore.listFrom(S3SingleDriverLogStore.scala:140)\n\tat org.apache.spark.sql.delta.Checkpoints.findLastCompleteCheckpoint(Checkpoints.scala:233)\n\tat org.apache.spark.sql.delta.Checkpoints.findLastCompleteCheckpoint$(Checkpoints.scala:224)\n\tat org.apache.spark.sql.delta.DeltaLog.findLastCompleteCheckpoint(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:208)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:202)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:202)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:202)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n\tat org.apache.spark.sql.delta.Checkpoints.lastCheckpoint(Checkpoints.scala:184)\n\tat org.apache.spark.sql.delta.Checkpoints.lastCheckpoint$(Checkpoints.scala:183)\n\tat org.apache.spark.sql.delta.DeltaLog.lastCheckpoint(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:248)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:246)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:245)\n\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:53)\n\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:69)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:567)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$2(DeltaLog.scala:567)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:437)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:114)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:77)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:67)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:437)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:113)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:98)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:437)\n\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:566)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:577)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n\t... 37 more\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: BMWFQHRCMTY0VW8V; S3 Extended Request ID: rTuwE5yq2JVxR0Edwhnkf70yX7qtwl4LvQX9YrBx1Sd6zqU7Hf8EoMrlmbBMv78zibahnVO4ZhI=; Proxy: null), S3 Extended Request ID: rTuwE5yq2JVxR0Edwhnkf70yX7qtwl4LvQX9YrBx1Sd6zqU7Hf8EoMrlmbBMv78zibahnVO4ZhI=\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1828)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1412)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1374)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5227)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5173)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1360)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$6(S3AFileSystem.java:2066)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:412)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:375)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2056)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2032)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3273)\n\t... 112 more\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "23/02/17 05:17:09 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:17:24 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 0:>                                                          (0 + 0) / 3]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "23/02/17 05:17:39 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:17:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:18:09 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:18:24 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 0:>                                                          (0 + 0) / 3]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "23/02/17 05:18:39 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:18:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:19:09 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:19:24 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 0:>                                                          (0 + 0) / 3]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "23/02/17 05:19:39 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:19:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:20:09 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:20:24 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 0:>                                                          (0 + 0) / 3]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "23/02/17 05:20:39 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:20:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:21:09 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:21:24 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 0:>                                                          (0 + 0) / 3]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "23/02/17 05:21:39 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:21:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:22:09 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:22:24 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 0:>                                                          (0 + 0) / 3]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "23/02/17 05:22:39 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:22:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:23:09 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:23:24 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 0:>                                                          (0 + 0) / 3]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "23/02/17 05:23:39 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:23:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:24:09 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:24:24 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 0:>                                                          (0 + 0) / 3]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "23/02/17 05:24:39 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:24:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:25:09 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
                    ]
                }
            ],
            "source": [
                "for s in config[\"subreddit\"]:\n",
                "    print(\"subreddit: {}\".format(s))\n",
                "    write_spark_jdbc(s)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "cbe1b93d-c62d-4e0e-bafa-661d4c14d578"
            },
            "source": [
                "## write to postgres table"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "64508a5f-71e3-4be9-8b43-4588d088e5c7"
            },
            "source": [
                "### create schema"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "8b8f8867-a8f6-41d3-b55e-1a9b6fec63c2",
                "language": "sql"
            },
            "outputs": [
                {
                    "ename": "OperationalError",
                    "evalue": "could not connect to server: Connection refused\n\tIs the server running on host \"xanaxprincess.asuscomm.com\" (174.170.113.213) and accepting\n\tTCP/IP connections on port 5432?\n",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/ipykernel_105188/1375140164.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsycopg2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"postgres_host\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"postgres_user\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"postgres_password\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"postgres\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mdsn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dsn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection_factory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconnection_factory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwasync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcursor_factory\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mOperationalError\u001b[0m: could not connect to server: Connection refused\n\tIs the server running on host \"xanaxprincess.asuscomm.com\" (174.170.113.213) and accepting\n\tTCP/IP connections on port 5432?\n"
                    ]
                }
            ],
            "source": [
                "conn = psycopg2.connect(host = config[\"postgres_host\"], user = config[\"postgres_user\"], password = config[\"postgres_password\"], database=\"postgres\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "5e39a567-d873-416a-a428-93dc1f132094",
                "language": "sql"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "name 'df' is not defined\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "23/02/17 05:09:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
                    ]
                }
            ],
            "source": [
                "connect_str = \"jdbc:postgresql://{}:5432/postgres\".format(config[\"postgres_host\"])\n",
                "\n",
                "try:\n",
                "    df.write.format(\"jdbc\") \\\n",
                "        .mode(\"overwrite\") \\\n",
                "        .option(\"url\", connect_str) \\\n",
                "        .option(\"dbtable\", \"public.{}\".format(subreddit)) \\\n",
                "        .option(\"user\", config[\"postgres_user\"]) \\\n",
                "        .option(\"password\", config[\"postgres_password\"]) \\\n",
                "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
                "        .save()\n",
                "\n",
                "    print(\"wrote df to postgresql table.\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(e)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "e54fedea-14a9-4052-b2b4-6d3b8f30821e"
            },
            "source": [
                "## stop spark"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {
                "azdata_cell_guid": "65e23599-1cd8-4de0-92a1-d728bcb5ac62",
                "language": "sql",
                "vscode": {
                    "languageId": "sql"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "SparkSession does not exist in the JVM\n"
                    ]
                }
            ],
            "source": [
                "try:\n",
                "    spark.stop()\n",
                "\n",
                "except Exception as e:\n",
                "    print(e)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "f0273f14-2d98-4b40-9e25-cbd3c2ef393d",
                "language": "sql",
                "vscode": {
                    "languageId": "sql"
                }
            },
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "reddit-env",
            "language": "python",
            "name": "reddit-env"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.3"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
