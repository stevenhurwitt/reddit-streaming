{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "ad00c8cb-3c7c-4b3c-9b7b-90d363a11400"
            },
            "source": [
                "# database upload"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "azdata_cell_guid": "ae72a8bf-4240-4823-8ecb-6adc23ce1ae9",
                "language": "sql"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "imported modules.\n"
                    ]
                }
            ],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.types import *\n",
                "from pyspark.sql.functions import *\n",
                "import psycopg2\n",
                "import datetime as dt\n",
                "from delta import *\n",
                "import boto3\n",
                "import pprint\n",
                "import yaml\n",
                "import time\n",
                "import json\n",
                "import sys\n",
                "import ast\n",
                "import os\n",
                "\n",
                "pp = pprint.PrettyPrinter(indent = 1)\n",
                "print(\"imported modules.\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "03ad6b6b-2eef-4d01-b56f-7cdcc3a204e9"
            },
            "source": [
                "# creds & config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "azdata_cell_guid": "c7b59afb-04cd-4487-bf28-3760a7198061",
                "language": "sql"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "read creds.json.\n",
                        "read config.yaml.\n",
                        "read creds & config.\n",
                        "read creds successfully.\n"
                    ]
                }
            ],
            "source": [
                "creds_path = os.path.join(\"/opt\", \"workspace\", \"redditStreaming\", \"src\", \"main\", \"python\", \"reddit\", \"creds.json\")\n",
                "config_path = os.path.join(\"/opt\", \"workspace\", \"redditStreaming\", \"src\", \"main\", \"python\", \"reddit\", \"config.yaml\")\n",
                "\n",
                "try:\n",
                "    with open(creds_path, \"r\") as f:\n",
                "        creds = json.load(f)\n",
                "        print(\"read creds.json.\")\n",
                "        f.close()\n",
                "\n",
                "    with open(config_path, \"r\") as g:\n",
                "        config = yaml.safe_load(g)\n",
                "        print(\"read config.yaml.\")\n",
                "        g.close()\n",
                "\n",
                "    print(\"read creds & config.\")\n",
                "\n",
                "except:\n",
                "    creds_path = \"/home/steven/Documents/reddit-streaming/redditStreaming/src/main/python/reddit/creds.json\"\n",
                "    with open(creds_path, \"r\") as f:\n",
                "        creds = json.load(f)\n",
                "        print(\"read creds.json.\")\n",
                "        f.close()\n",
                "\n",
                "print(\"read creds successfully.\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "31cc4147-4b27-4efc-9870-1861546c4ddd"
            },
            "source": [
                "# spark session"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "azdata_cell_guid": "930cf4ac-4f92-4995-a340-03a056e8e678",
                "language": "sql"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        ":: loading settings :: url = jar:file:/usr/local/lib/python3.7/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Ivy Default Cache set to: /root/.ivy2/cache\n",
                        "The jars for the packages stored in: /root/.ivy2/jars\n",
                        "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
                        "org.apache.hadoop#hadoop-common added as a dependency\n",
                        "org.apache.hadoop#hadoop-aws added as a dependency\n",
                        "org.apache.hadoop#hadoop-client added as a dependency\n",
                        "io.delta#delta-core_2.12 added as a dependency\n",
                        "org.postgresql#postgresql added as a dependency\n",
                        ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6d721c81-1caf-4f20-af1b-c2054e4fce23;1.0\n",
                        "\tconfs: [default]\n",
                        "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.1 in central\n",
                        "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.1 in central\n",
                        "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
                        "\tfound org.lz4#lz4-java;1.8.0 in central\n",
                        "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
                        "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
                        "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
                        "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
                        "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
                        "\tfound commons-logging#commons-logging;1.1.3 in central\n",
                        "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
                        "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
                        "\tfound org.apache.hadoop#hadoop-common;3.3.1 in central\n",
                        "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 in central\n",
                        "\tfound org.apache.hadoop#hadoop-annotations;3.3.1 in central\n",
                        "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
                        "\tfound com.google.guava#guava;27.0-jre in central\n",
                        "\tfound com.google.guava#failureaccess;1.0 in central\n",
                        "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
                        "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
                        "\tfound org.checkerframework#checker-qual;2.5.2 in central\n",
                        "\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n",
                        "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.17 in central\n",
                        "\tfound commons-cli#commons-cli;1.2 in central\n",
                        "\tfound org.apache.commons#commons-math3;3.1.1 in central\n",
                        "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
                        "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
                        "\tfound commons-codec#commons-codec;1.11 in central\n",
                        "\tfound commons-io#commons-io;2.8.0 in central\n",
                        "\tfound commons-net#commons-net;3.6 in central\n",
                        "\tfound commons-collections#commons-collections;3.2.2 in central\n",
                        "\tfound javax.servlet#javax.servlet-api;3.1.0 in central\n",
                        "\tfound org.eclipse.jetty#jetty-server;9.4.40.v20210413 in central\n",
                        "\tfound org.eclipse.jetty#jetty-http;9.4.40.v20210413 in central\n",
                        "\tfound org.eclipse.jetty#jetty-util;9.4.40.v20210413 in central\n",
                        "\tfound org.eclipse.jetty#jetty-io;9.4.40.v20210413 in central\n",
                        "\tfound org.eclipse.jetty#jetty-servlet;9.4.40.v20210413 in central\n",
                        "\tfound org.eclipse.jetty#jetty-security;9.4.40.v20210413 in central\n",
                        "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.40.v20210413 in central\n",
                        "\tfound org.eclipse.jetty#jetty-webapp;9.4.40.v20210413 in central\n",
                        "\tfound org.eclipse.jetty#jetty-xml;9.4.40.v20210413 in central\n",
                        "\tfound com.sun.jersey#jersey-core;1.19 in central\n",
                        "\tfound javax.ws.rs#jsr311-api;1.1.1 in central\n",
                        "\tfound com.sun.jersey#jersey-servlet;1.19 in central\n",
                        "\tfound com.sun.jersey#jersey-server;1.19 in central\n",
                        "\tfound com.sun.jersey#jersey-json;1.19 in central\n",
                        "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
                        "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in central\n",
                        "\tfound javax.xml.bind#jaxb-api;2.2.11 in central\n",
                        "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
                        "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
                        "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in central\n",
                        "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in central\n",
                        "\tfound log4j#log4j;1.2.17 in central\n",
                        "\tfound commons-beanutils#commons-beanutils;1.9.4 in central\n",
                        "\tfound org.apache.commons#commons-configuration2;2.1.1 in central\n",
                        "\tfound org.apache.commons#commons-lang3;3.7 in central\n",
                        "\tfound org.apache.commons#commons-text;1.4 in central\n",
                        "\tfound org.slf4j#slf4j-log4j12;1.7.30 in central\n",
                        "\tfound org.apache.avro#avro;1.7.7 in central\n",
                        "\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\n",
                        "\tfound org.apache.commons#commons-compress;1.19 in central\n",
                        "\tfound com.google.re2j#re2j;1.1 in central\n",
                        "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
                        "\tfound com.google.code.gson#gson;2.2.4 in central\n",
                        "\tfound org.apache.hadoop#hadoop-auth;3.3.1 in central\n",
                        "\tfound com.nimbusds#nimbus-jose-jwt;9.8.1 in central\n",
                        "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
                        "\tfound net.minidev#json-smart;2.4.2 in central\n",
                        "\tfound net.minidev#accessors-smart;2.4.2 in central\n",
                        "\tfound org.ow2.asm#asm;5.0.4 in central\n",
                        "\tfound org.apache.zookeeper#zookeeper;3.5.6 in central\n",
                        "\tfound org.apache.zookeeper#zookeeper-jute;3.5.6 in central\n",
                        "\tfound org.apache.yetus#audience-annotations;0.5.0 in central\n",
                        "\tfound org.apache.curator#curator-framework;4.2.0 in central\n",
                        "\tfound org.apache.curator#curator-client;4.2.0 in central\n",
                        "\tfound org.apache.kerby#kerb-simplekdc;1.0.1 in central\n",
                        "\tfound org.apache.kerby#kerb-client;1.0.1 in central\n",
                        "\tfound org.apache.kerby#kerby-config;1.0.1 in central\n",
                        "\tfound org.apache.kerby#kerb-core;1.0.1 in central\n",
                        "\tfound org.apache.kerby#kerby-pkix;1.0.1 in central\n",
                        "\tfound org.apache.kerby#kerby-asn1;1.0.1 in central\n",
                        "\tfound org.apache.kerby#kerby-util;1.0.1 in central\n",
                        "\tfound org.apache.kerby#kerb-common;1.0.1 in central\n",
                        "\tfound org.apache.kerby#kerb-crypto;1.0.1 in central\n",
                        "\tfound org.apache.kerby#kerb-util;1.0.1 in central\n",
                        "\tfound org.apache.kerby#token-provider;1.0.1 in central\n",
                        "\tfound org.apache.kerby#kerb-admin;1.0.1 in central\n",
                        "\tfound org.apache.kerby#kerb-server;1.0.1 in central\n",
                        "\tfound org.apache.kerby#kerb-identity;1.0.1 in central\n",
                        "\tfound org.apache.kerby#kerby-xdr;1.0.1 in central\n",
                        "\tfound com.jcraft#jsch;0.1.55 in central\n",
                        "\tfound org.apache.curator#curator-recipes;4.2.0 in central\n",
                        "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
                        "\tfound com.fasterxml.jackson.core#jackson-databind;2.10.5.1 in central\n",
                        "\tfound com.fasterxml.jackson.core#jackson-annotations;2.10.5 in central\n",
                        "\tfound com.fasterxml.jackson.core#jackson-core;2.10.5 in central\n",
                        "\tfound org.codehaus.woodstox#stax2-api;4.2.1 in central\n",
                        "\tfound com.fasterxml.woodstox#woodstox-core;5.3.0 in central\n",
                        "\tfound dnsjava#dnsjava;2.1.7 in central\n",
                        "\tfound jakarta.activation#jakarta.activation-api;1.2.1 in central\n",
                        "\tfound javax.servlet.jsp#jsp-api;2.1 in central\n",
                        "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
                        "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
                        "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
                        "\tfound org.apache.hadoop#hadoop-client;3.3.1 in central\n",
                        "\tfound org.apache.hadoop#hadoop-hdfs-client;3.3.1 in central\n",
                        "\tfound com.squareup.okhttp#okhttp;2.7.5 in central\n",
                        "\tfound com.squareup.okio#okio;1.6.0 in central\n",
                        "\tfound org.apache.hadoop#hadoop-yarn-api;3.3.1 in central\n",
                        "\tfound org.apache.hadoop#hadoop-yarn-client;3.3.1 in central\n",
                        "\tfound org.eclipse.jetty.websocket#websocket-client;9.4.40.v20210413 in central\n",
                        "\tfound org.eclipse.jetty#jetty-client;9.4.40.v20210413 in central\n",
                        "\tfound org.eclipse.jetty.websocket#websocket-common;9.4.40.v20210413 in central\n",
                        "\tfound org.eclipse.jetty.websocket#websocket-api;9.4.40.v20210413 in central\n",
                        "\tfound org.jline#jline;3.9.0 in central\n",
                        "\tfound org.apache.hadoop#hadoop-mapreduce-client-core;3.3.1 in central\n",
                        "\tfound org.apache.hadoop#hadoop-yarn-common;3.3.1 in central\n",
                        "\tfound com.sun.jersey#jersey-client;1.19 in central\n",
                        "\tfound com.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.10.5 in central\n",
                        "\tfound jakarta.xml.bind#jakarta.xml.bind-api;2.3.2 in central\n",
                        "\tfound com.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.10.5 in central\n",
                        "\tfound com.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.10.5 in central\n",
                        "\tfound org.apache.hadoop#hadoop-mapreduce-client-jobclient;3.3.1 in central\n",
                        "\tfound org.apache.hadoop#hadoop-mapreduce-client-common;3.3.1 in central\n",
                        "\tfound io.delta#delta-core_2.12;1.2.1 in central\n",
                        "\tfound io.delta#delta-storage;1.2.1 in central\n",
                        "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
                        "\tfound org.postgresql#postgresql;42.5.0 in central\n",
                        "\tfound org.checkerframework#checker-qual;3.5.0 in central\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.3.1/spark-sql-kafka-0-10_2.12-3.3.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.1!spark-sql-kafka-0-10_2.12.jar (350ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/3.3.1/hadoop-common-3.3.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-common;3.3.1!hadoop-common.jar (2809ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.1/hadoop-aws-3.3.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.1!hadoop-aws.jar (1238ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client/3.3.1/hadoop-client-3.3.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client;3.3.1!hadoop-client.jar (230ms)\n",
                        "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/1.2.1/delta-core_2.12-1.2.1.jar ...\n",
                        "\t[SUCCESSFUL ] io.delta#delta-core_2.12;1.2.1!delta-core_2.12.jar (5420ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.5.0/postgresql-42.5.0.jar ...\n",
                        "\t[SUCCESSFUL ] org.postgresql#postgresql;42.5.0!postgresql.jar (2285ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.3.1/spark-token-provider-kafka-0-10_2.12-3.3.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.1!spark-token-provider-kafka-0-10_2.12.jar (233ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.8.1/kafka-clients-2.8.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.8.1!kafka-clients.jar (7666ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (433ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
                        "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (250ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.2/hadoop-client-runtime-3.3.2.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.2!hadoop-client-runtime.jar (27209ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...\n",
                        "\t[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (479ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.4/snappy-java-1.1.8.4.jar ...\n",
                        "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.4!snappy-java.jar(bundle) (1308ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.32/slf4j-api-1.7.32.jar ...\n",
                        "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.32!slf4j-api.jar (167ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.2/hadoop-client-api-3.3.2.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.2!hadoop-client-api.jar (11810ms)\n",
                        "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
                        "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (92ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/hadoop/thirdparty/hadoop-shaded-protobuf_3_7/1.1.1/hadoop-shaded-protobuf_3_7-1.1.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1!hadoop-shaded-protobuf_3_7.jar (883ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-annotations/3.3.1/hadoop-annotations-3.3.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-annotations;3.3.1!hadoop-annotations.jar (50ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/hadoop/thirdparty/hadoop-shaded-guava/1.1.1/hadoop-shaded-guava-1.1.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1!hadoop-shaded-guava.jar (2187ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/google/guava/guava/27.0-jre/guava-27.0-jre.jar ...\n",
                        "\t[SUCCESSFUL ] com.google.guava#guava;27.0-jre!guava.jar(bundle) (1865ms)\n",
                        "downloading https://repo1.maven.org/maven2/commons-cli/commons-cli/1.2/commons-cli-1.2.jar ...\n",
                        "\t[SUCCESSFUL ] commons-cli#commons-cli;1.2!commons-cli.jar (81ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.commons#commons-math3;3.1.1!commons-math3.jar (1029ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpclient/4.5.13/httpclient-4.5.13.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.httpcomponents#httpclient;4.5.13!httpclient.jar (502ms)\n",
                        "downloading https://repo1.maven.org/maven2/commons-codec/commons-codec/1.11/commons-codec-1.11.jar ...\n",
                        "\t[SUCCESSFUL ] commons-codec#commons-codec;1.11!commons-codec.jar (345ms)\n",
                        "downloading https://repo1.maven.org/maven2/commons-io/commons-io/2.8.0/commons-io-2.8.0.jar ...\n",
                        "\t[SUCCESSFUL ] commons-io#commons-io;2.8.0!commons-io.jar (343ms)\n",
                        "downloading https://repo1.maven.org/maven2/commons-net/commons-net/3.6/commons-net-3.6.jar ...\n",
                        "\t[SUCCESSFUL ] commons-net#commons-net;3.6!commons-net.jar (324ms)\n",
                        "downloading https://repo1.maven.org/maven2/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar ...\n",
                        "\t[SUCCESSFUL ] commons-collections#commons-collections;3.2.2!commons-collections.jar (436ms)\n",
                        "downloading https://repo1.maven.org/maven2/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar ...\n",
                        "\t[SUCCESSFUL ] javax.servlet#javax.servlet-api;3.1.0!javax.servlet-api.jar (99ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-server/9.4.40.v20210413/jetty-server-9.4.40.v20210413.jar ...\n",
                        "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-server;9.4.40.v20210413!jetty-server.jar (428ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/9.4.40.v20210413/jetty-util-9.4.40.v20210413.jar ...\n",
                        "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-util;9.4.40.v20210413!jetty-util.jar (458ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-servlet/9.4.40.v20210413/jetty-servlet-9.4.40.v20210413.jar ...\n",
                        "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-servlet;9.4.40.v20210413!jetty-servlet.jar (166ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-webapp/9.4.40.v20210413/jetty-webapp-9.4.40.v20210413.jar ...\n",
                        "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-webapp;9.4.40.v20210413!jetty-webapp.jar (170ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-core/1.19/jersey-core-1.19.jar ...\n",
                        "\t[SUCCESSFUL ] com.sun.jersey#jersey-core;1.19!jersey-core.jar (364ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-servlet/1.19/jersey-servlet-1.19.jar ...\n",
                        "\t[SUCCESSFUL ] com.sun.jersey#jersey-servlet;1.19!jersey-servlet.jar (142ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-json/1.19/jersey-json-1.19.jar ...\n",
                        "\t[SUCCESSFUL ] com.sun.jersey#jersey-json;1.19!jersey-json.jar (163ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-server/1.19/jersey-server-1.19.jar ...\n",
                        "\t[SUCCESSFUL ] com.sun.jersey#jersey-server;1.19!jersey-server.jar (597ms)\n",
                        "downloading https://repo1.maven.org/maven2/log4j/log4j/1.2.17/log4j-1.2.17.jar ...\n",
                        "\t[SUCCESSFUL ] log4j#log4j;1.2.17!log4j.jar(bundle) (416ms)\n",
                        "downloading https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.9.4/commons-beanutils-1.9.4.jar ...\n",
                        "\t[SUCCESSFUL ] commons-beanutils#commons-beanutils;1.9.4!commons-beanutils.jar (257ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-configuration2/2.1.1/commons-configuration2-2.1.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.commons#commons-configuration2;2.1.1!commons-configuration2.jar (520ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.7/commons-lang3-3.7.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.commons#commons-lang3;3.7!commons-lang3.jar (396ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-text/1.4/commons-text-1.4.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.commons#commons-text;1.4!commons-text.jar (165ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.30/slf4j-log4j12-1.7.30.jar ...\n",
                        "\t[SUCCESSFUL ] org.slf4j#slf4j-log4j12;1.7.30!slf4j-log4j12.jar (43ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/avro/avro/1.7.7/avro-1.7.7.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.avro#avro;1.7.7!avro.jar(bundle) (292ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/google/re2j/re2j/1.1/re2j-1.1.jar ...\n",
                        "\t[SUCCESSFUL ] com.google.re2j#re2j;1.1!re2j.jar (118ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar ...\n",
                        "\t[SUCCESSFUL ] com.google.protobuf#protobuf-java;2.5.0!protobuf-java.jar(bundle) (327ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar ...\n",
                        "\t[SUCCESSFUL ] com.google.code.gson#gson;2.2.4!gson.jar (158ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-auth/3.3.1/hadoop-auth-3.3.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-auth;3.3.1!hadoop-auth.jar (101ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/jcraft/jsch/0.1.55/jsch-0.1.55.jar ...\n",
                        "\t[SUCCESSFUL ] com.jcraft#jsch;0.1.55!jsch.jar (216ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/curator/curator-client/4.2.0/curator-client-4.2.0.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.curator#curator-client;4.2.0!curator-client.jar(bundle) (1846ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/curator/curator-recipes/4.2.0/curator-recipes-4.2.0.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.curator#curator-recipes;4.2.0!curator-recipes.jar(bundle) (244ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar ...\n",
                        "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.2!jsr305.jar (62ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/htrace/htrace-core4/4.1.0-incubating/htrace-core4-4.1.0-incubating.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.htrace#htrace-core4;4.1.0-incubating!htrace-core4.jar (1015ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/zookeeper/zookeeper/3.5.6/zookeeper-3.5.6.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.zookeeper#zookeeper;3.5.6!zookeeper.jar (595ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-compress/1.19/commons-compress-1.19.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.commons#commons-compress;1.19!commons-compress.jar (410ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-core/1.0.1/kerb-core-1.0.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.kerby#kerb-core;1.0.1!kerb-core.jar (221ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.10.5.1/jackson-databind-2.10.5.1.jar ...\n",
                        "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-databind;2.10.5.1!jackson-databind.jar(bundle) (938ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/codehaus/woodstox/stax2-api/4.2.1/stax2-api-4.2.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.codehaus.woodstox#stax2-api;4.2.1!stax2-api.jar(bundle) (160ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/fasterxml/woodstox/woodstox-core/5.3.0/woodstox-core-5.3.0.jar ...\n",
                        "\t[SUCCESSFUL ] com.fasterxml.woodstox#woodstox-core;5.3.0!woodstox-core.jar(bundle) (916ms)\n",
                        "downloading https://repo1.maven.org/maven2/dnsjava/dnsjava/2.1.7/dnsjava-2.1.7.jar ...\n",
                        "\t[SUCCESSFUL ] dnsjava#dnsjava;2.1.7!dnsjava.jar (219ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/google/guava/failureaccess/1.0/failureaccess-1.0.jar ...\n",
                        "\t[SUCCESSFUL ] com.google.guava#failureaccess;1.0!failureaccess.jar (52ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/google/guava/listenablefuture/9999.0-empty-to-avoid-conflict-with-guava/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar ...\n",
                        "\t[SUCCESSFUL ] com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava!listenablefuture.jar (47ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/google/j2objc/j2objc-annotations/1.1/j2objc-annotations-1.1.jar ...\n",
                        "\t[SUCCESSFUL ] com.google.j2objc#j2objc-annotations;1.1!j2objc-annotations.jar (51ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/codehaus/mojo/animal-sniffer-annotations/1.17/animal-sniffer-annotations-1.17.jar ...\n",
                        "\t[SUCCESSFUL ] org.codehaus.mojo#animal-sniffer-annotations;1.17!animal-sniffer-annotations.jar (35ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcore/4.4.13/httpcore-4.4.13.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.httpcomponents#httpcore;4.4.13!httpcore.jar (238ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-http/9.4.40.v20210413/jetty-http-9.4.40.v20210413.jar ...\n",
                        "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-http;9.4.40.v20210413!jetty-http.jar (538ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-io/9.4.40.v20210413/jetty-io-9.4.40.v20210413.jar ...\n",
                        "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-io;9.4.40.v20210413!jetty-io.jar (133ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-security/9.4.40.v20210413/jetty-security-9.4.40.v20210413.jar ...\n",
                        "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-security;9.4.40.v20210413!jetty-security.jar (153ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util-ajax/9.4.40.v20210413/jetty-util-ajax-9.4.40.v20210413.jar ...\n",
                        "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-util-ajax;9.4.40.v20210413!jetty-util-ajax.jar (82ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-xml/9.4.40.v20210413/jetty-xml-9.4.40.v20210413.jar ...\n",
                        "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-xml;9.4.40.v20210413!jetty-xml.jar (94ms)\n",
                        "downloading https://repo1.maven.org/maven2/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1.jar ...\n",
                        "\t[SUCCESSFUL ] javax.ws.rs#jsr311-api;1.1.1!jsr311-api.jar (92ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.codehaus.jettison#jettison;1.1!jettison.jar(bundle) (185ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar ...\n",
                        "\t[SUCCESSFUL ] com.sun.xml.bind#jaxb-impl;2.2.3-1!jaxb-impl.jar (784ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar ...\n",
                        "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-core-asl;1.9.13!jackson-core-asl.jar (187ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar ...\n",
                        "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-mapper-asl;1.9.13!jackson-mapper-asl.jar (530ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar ...\n",
                        "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-jaxrs;1.9.13!jackson-jaxrs.jar (57ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar ...\n",
                        "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-xc;1.9.13!jackson-xc.jar (55ms)\n",
                        "downloading https://repo1.maven.org/maven2/javax/xml/bind/jaxb-api/2.2.11/jaxb-api-2.2.11.jar ...\n",
                        "\t[SUCCESSFUL ] javax.xml.bind#jaxb-api;2.2.11!jaxb-api.jar (92ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar ...\n",
                        "\t[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.3!paranamer.jar (54ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/nimbusds/nimbus-jose-jwt/9.8.1/nimbus-jose-jwt-9.8.1.jar ...\n",
                        "\t[SUCCESSFUL ] com.nimbusds#nimbus-jose-jwt;9.8.1!nimbus-jose-jwt.jar (316ms)\n",
                        "downloading https://repo1.maven.org/maven2/net/minidev/json-smart/2.4.2/json-smart-2.4.2.jar ...\n",
                        "\t[SUCCESSFUL ] net.minidev#json-smart;2.4.2!json-smart.jar(bundle) (173ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/curator/curator-framework/4.2.0/curator-framework-4.2.0.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.curator#curator-framework;4.2.0!curator-framework.jar(bundle) (240ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-simplekdc/1.0.1/kerb-simplekdc-1.0.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.kerby#kerb-simplekdc;1.0.1!kerb-simplekdc.jar (91ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar ...\n",
                        "\t[SUCCESSFUL ] com.github.stephenc.jcip#jcip-annotations;1.0-1!jcip-annotations.jar (38ms)\n",
                        "downloading https://repo1.maven.org/maven2/net/minidev/accessors-smart/2.4.2/accessors-smart-2.4.2.jar ...\n",
                        "\t[SUCCESSFUL ] net.minidev#accessors-smart;2.4.2!accessors-smart.jar(bundle) (58ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar ...\n",
                        "\t[SUCCESSFUL ] org.ow2.asm#asm;5.0.4!asm.jar (79ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/zookeeper/zookeeper-jute/3.5.6/zookeeper-jute-3.5.6.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.zookeeper#zookeeper-jute;3.5.6!zookeeper-jute.jar (203ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.yetus#audience-annotations;0.5.0!audience-annotations.jar (53ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-client/1.0.1/kerb-client-1.0.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.kerby#kerb-client;1.0.1!kerb-client.jar (106ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-admin/1.0.1/kerb-admin-1.0.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.kerby#kerb-admin;1.0.1!kerb-admin.jar (100ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerby-config/1.0.1/kerby-config-1.0.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.kerby#kerby-config;1.0.1!kerby-config.jar (57ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-common/1.0.1/kerb-common-1.0.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.kerby#kerb-common;1.0.1!kerb-common.jar (120ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-util/1.0.1/kerb-util-1.0.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.kerby#kerb-util;1.0.1!kerb-util.jar (70ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/kerby/token-provider/1.0.1/token-provider-1.0.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.kerby#token-provider;1.0.1!token-provider.jar (54ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerby-pkix/1.0.1/kerby-pkix-1.0.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.kerby#kerby-pkix;1.0.1!kerby-pkix.jar (174ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerby-asn1/1.0.1/kerby-asn1-1.0.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.kerby#kerby-asn1;1.0.1!kerby-asn1.jar (119ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerby-util/1.0.1/kerby-util-1.0.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.kerby#kerby-util;1.0.1!kerby-util.jar (64ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-crypto/1.0.1/kerb-crypto-1.0.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.kerby#kerb-crypto;1.0.1!kerb-crypto.jar (131ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-server/1.0.1/kerb-server-1.0.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.kerby#kerb-server;1.0.1!kerb-server.jar (149ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerby-xdr/1.0.1/kerby-xdr-1.0.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.kerby#kerby-xdr;1.0.1!kerby-xdr.jar (49ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-identity/1.0.1/kerb-identity-1.0.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.kerby#kerb-identity;1.0.1!kerb-identity.jar (68ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.10.5/jackson-annotations-2.10.5.jar ...\n",
                        "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-annotations;2.10.5!jackson-annotations.jar(bundle) (87ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.10.5/jackson-core-2.10.5.jar ...\n",
                        "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-core;2.10.5!jackson-core.jar(bundle) (289ms)\n",
                        "downloading https://repo1.maven.org/maven2/jakarta/activation/jakarta.activation-api/1.2.1/jakarta.activation-api-1.2.1.jar ...\n",
                        "\t[SUCCESSFUL ] jakarta.activation#jakarta.activation-api;1.2.1!jakarta.activation-api.jar (89ms)\n",
                        "downloading https://repo1.maven.org/maven2/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar ...\n",
                        "\t[SUCCESSFUL ] javax.servlet.jsp#jsp-api;2.1!jsp-api.jar (100ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.901/aws-java-sdk-bundle-1.11.901.jar ...\n",
                        "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.11.901!aws-java-sdk-bundle.jar (118226ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.0.7.Final/wildfly-openssl-1.0.7.Final.jar ...\n",
                        "\t[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.0.7.Final!wildfly-openssl.jar (311ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-hdfs-client/3.3.1/hadoop-hdfs-client-3.3.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-hdfs-client;3.3.1!hadoop-hdfs-client.jar (3359ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-api/3.3.1/hadoop-yarn-api-3.3.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-yarn-api;3.3.1!hadoop-yarn-api.jar (2144ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-client/3.3.1/hadoop-yarn-client-3.3.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-yarn-client;3.3.1!hadoop-yarn-client.jar (187ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-core/3.3.1/hadoop-mapreduce-client-core-3.3.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-mapreduce-client-core;3.3.1!hadoop-mapreduce-client-core.jar (950ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-jobclient/3.3.1/hadoop-mapreduce-client-jobclient-3.3.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-mapreduce-client-jobclient;3.3.1!hadoop-mapreduce-client-jobclient.jar (104ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/squareup/okhttp/okhttp/2.7.5/okhttp-2.7.5.jar ...\n",
                        "\t[SUCCESSFUL ] com.squareup.okhttp#okhttp;2.7.5!okhttp.jar (256ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/squareup/okio/okio/1.6.0/okio-1.6.0.jar ...\n",
                        "\t[SUCCESSFUL ] com.squareup.okio#okio;1.6.0!okio.jar (78ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/websocket/websocket-client/9.4.40.v20210413/websocket-client-9.4.40.v20210413.jar ...\n",
                        "\t[SUCCESSFUL ] org.eclipse.jetty.websocket#websocket-client;9.4.40.v20210413!websocket-client.jar (82ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/jline/jline/3.9.0/jline-3.9.0.jar ...\n",
                        "\t[SUCCESSFUL ] org.jline#jline;3.9.0!jline.jar (410ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-client/9.4.40.v20210413/jetty-client-9.4.40.v20210413.jar ...\n",
                        "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-client;9.4.40.v20210413!jetty-client.jar (249ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/websocket/websocket-common/9.4.40.v20210413/websocket-common-9.4.40.v20210413.jar ...\n",
                        "\t[SUCCESSFUL ] org.eclipse.jetty.websocket#websocket-common;9.4.40.v20210413!websocket-common.jar (259ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/websocket/websocket-api/9.4.40.v20210413/websocket-api-9.4.40.v20210413.jar ...\n",
                        "\t[SUCCESSFUL ] org.eclipse.jetty.websocket#websocket-api;9.4.40.v20210413!websocket-api.jar (86ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-common/3.3.1/hadoop-yarn-common-3.3.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-yarn-common;3.3.1!hadoop-yarn-common.jar (1869ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-client/1.19/jersey-client-1.19.jar ...\n",
                        "\t[SUCCESSFUL ] com.sun.jersey#jersey-client;1.19!jersey-client.jar (138ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/module/jackson-module-jaxb-annotations/2.10.5/jackson-module-jaxb-annotations-2.10.5.jar ...\n",
                        "\t[SUCCESSFUL ] com.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.10.5!jackson-module-jaxb-annotations.jar(bundle) (64ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/jaxrs/jackson-jaxrs-json-provider/2.10.5/jackson-jaxrs-json-provider-2.10.5.jar ...\n",
                        "\t[SUCCESSFUL ] com.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.10.5!jackson-jaxrs-json-provider.jar(bundle) (42ms)\n",
                        "downloading https://repo1.maven.org/maven2/jakarta/xml/bind/jakarta.xml.bind-api/2.3.2/jakarta.xml.bind-api-2.3.2.jar ...\n",
                        "\t[SUCCESSFUL ] jakarta.xml.bind#jakarta.xml.bind-api;2.3.2!jakarta.xml.bind-api.jar (96ms)\n",
                        "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/jaxrs/jackson-jaxrs-base/2.10.5/jackson-jaxrs-base-2.10.5.jar ...\n",
                        "\t[SUCCESSFUL ] com.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.10.5!jackson-jaxrs-base.jar(bundle) (62ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-mapreduce-client-common/3.3.1/hadoop-mapreduce-client-common-3.3.1.jar ...\n",
                        "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-mapreduce-client-common;3.3.1!hadoop-mapreduce-client-common.jar (502ms)\n",
                        "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/1.2.1/delta-storage-1.2.1.jar ...\n",
                        "\t[SUCCESSFUL ] io.delta#delta-storage;1.2.1!delta-storage.jar (78ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.8/antlr4-runtime-4.8.jar ...\n",
                        "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.8!antlr4-runtime.jar (248ms)\n",
                        "downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/3.5.0/checker-qual-3.5.0.jar ...\n",
                        "\t[SUCCESSFUL ] org.checkerframework#checker-qual;3.5.0!checker-qual.jar (158ms)\n",
                        ":: resolution report :: resolve 37070ms :: artifacts dl 218872ms\n",
                        "\t:: modules in use:\n",
                        "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
                        "\tcom.fasterxml.jackson.core#jackson-annotations;2.10.5 from central in [default]\n",
                        "\tcom.fasterxml.jackson.core#jackson-core;2.10.5 from central in [default]\n",
                        "\tcom.fasterxml.jackson.core#jackson-databind;2.10.5.1 from central in [default]\n",
                        "\tcom.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.10.5 from central in [default]\n",
                        "\tcom.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.10.5 from central in [default]\n",
                        "\tcom.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.10.5 from central in [default]\n",
                        "\tcom.fasterxml.woodstox#woodstox-core;5.3.0 from central in [default]\n",
                        "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
                        "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
                        "\tcom.google.code.gson#gson;2.2.4 from central in [default]\n",
                        "\tcom.google.guava#failureaccess;1.0 from central in [default]\n",
                        "\tcom.google.guava#guava;27.0-jre from central in [default]\n",
                        "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
                        "\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n",
                        "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
                        "\tcom.google.re2j#re2j;1.1 from central in [default]\n",
                        "\tcom.jcraft#jsch;0.1.55 from central in [default]\n",
                        "\tcom.nimbusds#nimbus-jose-jwt;9.8.1 from central in [default]\n",
                        "\tcom.squareup.okhttp#okhttp;2.7.5 from central in [default]\n",
                        "\tcom.squareup.okio#okio;1.6.0 from central in [default]\n",
                        "\tcom.sun.jersey#jersey-client;1.19 from central in [default]\n",
                        "\tcom.sun.jersey#jersey-core;1.19 from central in [default]\n",
                        "\tcom.sun.jersey#jersey-json;1.19 from central in [default]\n",
                        "\tcom.sun.jersey#jersey-server;1.19 from central in [default]\n",
                        "\tcom.sun.jersey#jersey-servlet;1.19 from central in [default]\n",
                        "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from central in [default]\n",
                        "\tcom.thoughtworks.paranamer#paranamer;2.3 from central in [default]\n",
                        "\tcommons-beanutils#commons-beanutils;1.9.4 from central in [default]\n",
                        "\tcommons-cli#commons-cli;1.2 from central in [default]\n",
                        "\tcommons-codec#commons-codec;1.11 from central in [default]\n",
                        "\tcommons-collections#commons-collections;3.2.2 from central in [default]\n",
                        "\tcommons-io#commons-io;2.8.0 from central in [default]\n",
                        "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
                        "\tcommons-net#commons-net;3.6 from central in [default]\n",
                        "\tdnsjava#dnsjava;2.1.7 from central in [default]\n",
                        "\tio.delta#delta-core_2.12;1.2.1 from central in [default]\n",
                        "\tio.delta#delta-storage;1.2.1 from central in [default]\n",
                        "\tjakarta.activation#jakarta.activation-api;1.2.1 from central in [default]\n",
                        "\tjakarta.xml.bind#jakarta.xml.bind-api;2.3.2 from central in [default]\n",
                        "\tjavax.servlet#javax.servlet-api;3.1.0 from central in [default]\n",
                        "\tjavax.servlet.jsp#jsp-api;2.1 from central in [default]\n",
                        "\tjavax.ws.rs#jsr311-api;1.1.1 from central in [default]\n",
                        "\tjavax.xml.bind#jaxb-api;2.2.11 from central in [default]\n",
                        "\tlog4j#log4j;1.2.17 from central in [default]\n",
                        "\tnet.minidev#accessors-smart;2.4.2 from central in [default]\n",
                        "\tnet.minidev#json-smart;2.4.2 from central in [default]\n",
                        "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
                        "\torg.apache.avro#avro;1.7.7 from central in [default]\n",
                        "\torg.apache.commons#commons-compress;1.19 from central in [default]\n",
                        "\torg.apache.commons#commons-configuration2;2.1.1 from central in [default]\n",
                        "\torg.apache.commons#commons-lang3;3.7 from central in [default]\n",
                        "\torg.apache.commons#commons-math3;3.1.1 from central in [default]\n",
                        "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
                        "\torg.apache.commons#commons-text;1.4 from central in [default]\n",
                        "\torg.apache.curator#curator-client;4.2.0 from central in [default]\n",
                        "\torg.apache.curator#curator-framework;4.2.0 from central in [default]\n",
                        "\torg.apache.curator#curator-recipes;4.2.0 from central in [default]\n",
                        "\torg.apache.hadoop#hadoop-annotations;3.3.1 from central in [default]\n",
                        "\torg.apache.hadoop#hadoop-auth;3.3.1 from central in [default]\n",
                        "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
                        "\torg.apache.hadoop#hadoop-client;3.3.1 from central in [default]\n",
                        "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
                        "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
                        "\torg.apache.hadoop#hadoop-common;3.3.1 from central in [default]\n",
                        "\torg.apache.hadoop#hadoop-hdfs-client;3.3.1 from central in [default]\n",
                        "\torg.apache.hadoop#hadoop-mapreduce-client-common;3.3.1 from central in [default]\n",
                        "\torg.apache.hadoop#hadoop-mapreduce-client-core;3.3.1 from central in [default]\n",
                        "\torg.apache.hadoop#hadoop-mapreduce-client-jobclient;3.3.1 from central in [default]\n",
                        "\torg.apache.hadoop#hadoop-yarn-api;3.3.1 from central in [default]\n",
                        "\torg.apache.hadoop#hadoop-yarn-client;3.3.1 from central in [default]\n",
                        "\torg.apache.hadoop#hadoop-yarn-common;3.3.1 from central in [default]\n",
                        "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
                        "\torg.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 from central in [default]\n",
                        "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
                        "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
                        "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
                        "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
                        "\torg.apache.kerby#kerb-admin;1.0.1 from central in [default]\n",
                        "\torg.apache.kerby#kerb-client;1.0.1 from central in [default]\n",
                        "\torg.apache.kerby#kerb-common;1.0.1 from central in [default]\n",
                        "\torg.apache.kerby#kerb-core;1.0.1 from central in [default]\n",
                        "\torg.apache.kerby#kerb-crypto;1.0.1 from central in [default]\n",
                        "\torg.apache.kerby#kerb-identity;1.0.1 from central in [default]\n",
                        "\torg.apache.kerby#kerb-server;1.0.1 from central in [default]\n",
                        "\torg.apache.kerby#kerb-simplekdc;1.0.1 from central in [default]\n",
                        "\torg.apache.kerby#kerb-util;1.0.1 from central in [default]\n",
                        "\torg.apache.kerby#kerby-asn1;1.0.1 from central in [default]\n",
                        "\torg.apache.kerby#kerby-config;1.0.1 from central in [default]\n",
                        "\torg.apache.kerby#kerby-pkix;1.0.1 from central in [default]\n",
                        "\torg.apache.kerby#kerby-util;1.0.1 from central in [default]\n",
                        "\torg.apache.kerby#kerby-xdr;1.0.1 from central in [default]\n",
                        "\torg.apache.kerby#token-provider;1.0.1 from central in [default]\n",
                        "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.1 from central in [default]\n",
                        "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.1 from central in [default]\n",
                        "\torg.apache.yetus#audience-annotations;0.5.0 from central in [default]\n",
                        "\torg.apache.zookeeper#zookeeper;3.5.6 from central in [default]\n",
                        "\torg.apache.zookeeper#zookeeper-jute;3.5.6 from central in [default]\n",
                        "\torg.checkerframework#checker-qual;3.5.0 from central in [default]\n",
                        "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
                        "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]\n",
                        "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
                        "\torg.codehaus.jackson#jackson-xc;1.9.13 from central in [default]\n",
                        "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
                        "\torg.codehaus.mojo#animal-sniffer-annotations;1.17 from central in [default]\n",
                        "\torg.codehaus.woodstox#stax2-api;4.2.1 from central in [default]\n",
                        "\torg.eclipse.jetty#jetty-client;9.4.40.v20210413 from central in [default]\n",
                        "\torg.eclipse.jetty#jetty-http;9.4.40.v20210413 from central in [default]\n",
                        "\torg.eclipse.jetty#jetty-io;9.4.40.v20210413 from central in [default]\n",
                        "\torg.eclipse.jetty#jetty-security;9.4.40.v20210413 from central in [default]\n",
                        "\torg.eclipse.jetty#jetty-server;9.4.40.v20210413 from central in [default]\n",
                        "\torg.eclipse.jetty#jetty-servlet;9.4.40.v20210413 from central in [default]\n",
                        "\torg.eclipse.jetty#jetty-util;9.4.40.v20210413 from central in [default]\n",
                        "\torg.eclipse.jetty#jetty-util-ajax;9.4.40.v20210413 from central in [default]\n",
                        "\torg.eclipse.jetty#jetty-webapp;9.4.40.v20210413 from central in [default]\n",
                        "\torg.eclipse.jetty#jetty-xml;9.4.40.v20210413 from central in [default]\n",
                        "\torg.eclipse.jetty.websocket#websocket-api;9.4.40.v20210413 from central in [default]\n",
                        "\torg.eclipse.jetty.websocket#websocket-client;9.4.40.v20210413 from central in [default]\n",
                        "\torg.eclipse.jetty.websocket#websocket-common;9.4.40.v20210413 from central in [default]\n",
                        "\torg.jline#jline;3.9.0 from central in [default]\n",
                        "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
                        "\torg.ow2.asm#asm;5.0.4 from central in [default]\n",
                        "\torg.postgresql#postgresql;42.5.0 from central in [default]\n",
                        "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
                        "\torg.slf4j#slf4j-log4j12;1.7.30 from central in [default]\n",
                        "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
                        "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
                        "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
                        "\t:: evicted modules:\n",
                        "\tcom.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.2] in [default]\n",
                        "\torg.checkerframework#checker-qual;2.5.2 by [org.checkerframework#checker-qual;3.5.0] in [default]\n",
                        "\torg.slf4j#slf4j-api;1.7.30 by [org.slf4j#slf4j-api;1.7.32] in [default]\n",
                        "\torg.xerial.snappy#snappy-java;1.1.8.2 by [org.xerial.snappy#snappy-java;1.1.8.4] in [default]\n",
                        "\t---------------------------------------------------------------------\n",
                        "\t|                  |            modules            ||   artifacts   |\n",
                        "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
                        "\t---------------------------------------------------------------------\n",
                        "\t|      default     |  132  |  130  |  130  |   4   ||  128  |  128  |\n",
                        "\t---------------------------------------------------------------------\n",
                        ":: retrieving :: org.apache.spark#spark-submit-parent-6d721c81-1caf-4f20-af1b-c2054e4fce23\n",
                        "\tconfs: [default]\n",
                        "\t128 artifacts copied, 0 already retrieved (303526kB/190ms)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "23/02/22 01:49:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Setting default log level to \"WARN\".\n",
                        "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "23/02/22 01:49:48 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
                        "created spark successfully\n"
                    ]
                }
            ],
            "source": [
                "spark_host = \"spark-master\"\n",
                "# spark_host = \"spark-master\"\n",
                "aws_client = creds[\"aws_client\"]\n",
                "aws_secret = creds[\"aws_secret\"]\n",
                "index = 0\n",
                "subreddit = \"technology\"\n",
                "\n",
                "# initialize spark session\n",
                "try:\n",
                "    spark = SparkSession.builder.appName(\"reddit_{}\".format(subreddit)) \\\n",
                "                .master(\"spark://{}:7077\".format(spark_host)) \\\n",
                "                .config(\"spark.scheduler.mode\", \"FAIR\") \\\n",
                "                .config(\"spark.scheduler.allocation.file\", \"file:///opt/workspace/redditStreaming/fairscheduler.xml\") \\\n",
                "                .config(\"spark.executor.memory\", \"4096m\") \\\n",
                "                .config(\"spark.executor.cores\", \"4\") \\\n",
                "                .config(\"spark.local.dir\", \"/opt/workspace/tmp/driver/{}/\".format(subreddit)) \\\n",
                "                .config(\"spark.worker.dir\", \"/opt/workspace/tmp/executor/{}/\".format(subreddit)) \\\n",
                "                .config(\"spark.eventLog.enabled\", \"true\") \\\n",
                "                .config(\"spark.eventLog.dir\", \"file:///opt/workspace/events/{}/\".format(subreddit)) \\\n",
                "                .config(\"spark.sql.debug.maxToStringFields\", 1000) \\\n",
                "                .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,org.apache.hadoop:hadoop-common:3.3.1,org.apache.hadoop:hadoop-aws:3.3.1,org.apache.hadoop:hadoop-client:3.3.1,io.delta:delta-core_2.12:1.2.1,org.postgresql:postgresql:42.5.0\") \\\n",
                "                .config(\"spark.hadoop.fs.s3a.access.key\", aws_client) \\\n",
                "                .config(\"spark.hadoop.fs.s3a.secret.key\", aws_secret) \\\n",
                "                .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
                "                .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
                "                .config('spark.hadoop.fs.s3a.buffer.dir', '/opt/workspace/tmp/blocks') \\\n",
                "                .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
                "                .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
                "                .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
                "                .enableHiveSupport() \\\n",
                "                .getOrCreate()\n",
                "\n",
                "    sc = spark.sparkContext\n",
                "    # .config('spark.hadoop.fs.s3a.fast.upload.buffer', 'bytebuffer') \\\n",
                "\n",
                "    sc.setLogLevel('WARN')\n",
                "    sc.setLocalProperty(\"spark.scheduler.pool\", \"pool{}\".format(str(index)))\n",
                "    # sc._jsc.hadoopConfiguration().set(\"fs.s3a.awsAccessKeyId\", aws_client)\n",
                "    # sc._jsc.hadoopConfiguration().set(\"fs.s3a.awsSecretAccessKey\", aws_secret)\n",
                "    # sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.us-east-2.amazonaws.com\")\n",
                "    print(\"created spark successfully\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(e)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# spark"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "            <div>\n",
                            "                <p><b>SparkSession - hive</b></p>\n",
                            "                \n",
                            "        <div>\n",
                            "            <p><b>SparkContext</b></p>\n",
                            "\n",
                            "            <p><a href=\"http://1579cbf8767c:4040\">Spark UI</a></p>\n",
                            "\n",
                            "            <dl>\n",
                            "              <dt>Version</dt>\n",
                            "                <dd><code>v3.3.2</code></dd>\n",
                            "              <dt>Master</dt>\n",
                            "                <dd><code>spark://spark-master:7077</code></dd>\n",
                            "              <dt>AppName</dt>\n",
                            "                <dd><code>reddit_technology</code></dd>\n",
                            "            </dl>\n",
                            "        </div>\n",
                            "        \n",
                            "            </div>\n",
                            "        "
                        ],
                        "text/plain": [
                            "<pyspark.sql.session.SparkSession at 0x7f64ed40a4e0>"
                        ]
                    },
                    "execution_count": 4,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "spark"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# read df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "s3a://reddit-streaming-stevenhurwitt-new/aws_clean\n",
                        "23/02/22 01:50:19 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                }
            ],
            "source": [
                "bucket = config[\"bucket\"]\n",
                "folder = \"aws_clean\"\n",
                "read_path = \"s3a://\" + bucket + \"/\" + folder\n",
                "print(read_path)\n",
                "df = spark.read.format(\"delta\").option(\"header\", True).load(read_path)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# show df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "                                                                                \r"
                    ]
                }
            ],
            "source": [
                "pandas_df = df.toPandas()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>approved_at_utc</th>\n",
                            "      <th>subreddit</th>\n",
                            "      <th>selftext</th>\n",
                            "      <th>author_fullname</th>\n",
                            "      <th>saved</th>\n",
                            "      <th>mod_reason_title</th>\n",
                            "      <th>gilded</th>\n",
                            "      <th>clicked</th>\n",
                            "      <th>title</th>\n",
                            "      <th>subreddit_name_prefixed</th>\n",
                            "      <th>...</th>\n",
                            "      <th>url</th>\n",
                            "      <th>subreddit_subscribers</th>\n",
                            "      <th>created_utc</th>\n",
                            "      <th>num_crossposts</th>\n",
                            "      <th>media</th>\n",
                            "      <th>is_video</th>\n",
                            "      <th>date</th>\n",
                            "      <th>year</th>\n",
                            "      <th>month</th>\n",
                            "      <th>day</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>NaT</td>\n",
                            "      <td>aws</td>\n",
                            "      <td>I have this PHP (Laravel) script to get all th...</td>\n",
                            "      <td>t2_6j0lp</td>\n",
                            "      <td>False</td>\n",
                            "      <td>None</td>\n",
                            "      <td>0</td>\n",
                            "      <td>False</td>\n",
                            "      <td>Weird behavior getting a list of S3 object keys</td>\n",
                            "      <td>r/aws</td>\n",
                            "      <td>...</td>\n",
                            "      <td>https://www.reddit.com/r/aws/comments/10t6cw6/...</td>\n",
                            "      <td>228560</td>\n",
                            "      <td>2023-02-04 05:48:13.401088</td>\n",
                            "      <td>0</td>\n",
                            "      <td>None</td>\n",
                            "      <td>False</td>\n",
                            "      <td>2023-02-04</td>\n",
                            "      <td>2023</td>\n",
                            "      <td>2</td>\n",
                            "      <td>4</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>NaT</td>\n",
                            "      <td>aws</td>\n",
                            "      <td>I'm not exactly on the up and up on some of th...</td>\n",
                            "      <td>t2_jsvj0raw</td>\n",
                            "      <td>False</td>\n",
                            "      <td>None</td>\n",
                            "      <td>0</td>\n",
                            "      <td>False</td>\n",
                            "      <td>Pattern for ingesting deltas and merging into ...</td>\n",
                            "      <td>r/aws</td>\n",
                            "      <td>...</td>\n",
                            "      <td>https://www.reddit.com/r/aws/comments/10pmcs3/...</td>\n",
                            "      <td>228234</td>\n",
                            "      <td>2023-01-31 03:00:16.421888</td>\n",
                            "      <td>0</td>\n",
                            "      <td>None</td>\n",
                            "      <td>False</td>\n",
                            "      <td>2023-01-31</td>\n",
                            "      <td>2023</td>\n",
                            "      <td>1</td>\n",
                            "      <td>31</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>NaT</td>\n",
                            "      <td>aws</td>\n",
                            "      <td>Hey there,\\n\\nIs anyone using the AWS Transfer...</td>\n",
                            "      <td>t2_4b72e</td>\n",
                            "      <td>False</td>\n",
                            "      <td>None</td>\n",
                            "      <td>0</td>\n",
                            "      <td>False</td>\n",
                            "      <td>AWS Transfer Family price question</td>\n",
                            "      <td>r/aws</td>\n",
                            "      <td>...</td>\n",
                            "      <td>https://www.reddit.com/r/aws/comments/10p0t6t/...</td>\n",
                            "      <td>228218</td>\n",
                            "      <td>2023-01-30 12:32:19.943424</td>\n",
                            "      <td>0</td>\n",
                            "      <td>None</td>\n",
                            "      <td>False</td>\n",
                            "      <td>2023-01-30</td>\n",
                            "      <td>2023</td>\n",
                            "      <td>1</td>\n",
                            "      <td>30</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>NaT</td>\n",
                            "      <td>aws</td>\n",
                            "      <td>I can't imagine the answer is \"it does scale\",...</td>\n",
                            "      <td>t2_52j9a</td>\n",
                            "      <td>False</td>\n",
                            "      <td>None</td>\n",
                            "      <td>0</td>\n",
                            "      <td>False</td>\n",
                            "      <td>How does (does?) API Keys / Usage Plans / WAF ...</td>\n",
                            "      <td>r/aws</td>\n",
                            "      <td>...</td>\n",
                            "      <td>https://www.reddit.com/r/aws/comments/10p3zl9/...</td>\n",
                            "      <td>228191</td>\n",
                            "      <td>2023-01-30 14:28:39.265280</td>\n",
                            "      <td>0</td>\n",
                            "      <td>None</td>\n",
                            "      <td>False</td>\n",
                            "      <td>2023-01-30</td>\n",
                            "      <td>2023</td>\n",
                            "      <td>1</td>\n",
                            "      <td>30</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>NaT</td>\n",
                            "      <td>aws</td>\n",
                            "      <td>Hi,\\n\\nI’m looking at using SES for email list...</td>\n",
                            "      <td>t2_3sde0</td>\n",
                            "      <td>False</td>\n",
                            "      <td>None</td>\n",
                            "      <td>0</td>\n",
                            "      <td>False</td>\n",
                            "      <td>SES Number of Contact Lists Per Account</td>\n",
                            "      <td>r/aws</td>\n",
                            "      <td>...</td>\n",
                            "      <td>https://www.reddit.com/r/aws/comments/10opryt/...</td>\n",
                            "      <td>228222</td>\n",
                            "      <td>2023-01-30 02:08:13.197312</td>\n",
                            "      <td>0</td>\n",
                            "      <td>None</td>\n",
                            "      <td>False</td>\n",
                            "      <td>2023-01-30</td>\n",
                            "      <td>2023</td>\n",
                            "      <td>1</td>\n",
                            "      <td>30</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>...</th>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "      <td>...</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>69</th>\n",
                            "      <td>NaT</td>\n",
                            "      <td>aws</td>\n",
                            "      <td>I have data in a DDB. I want to make visualiza...</td>\n",
                            "      <td>t2_uf837</td>\n",
                            "      <td>False</td>\n",
                            "      <td>None</td>\n",
                            "      <td>0</td>\n",
                            "      <td>False</td>\n",
                            "      <td>Optimal way to visualize your data from DDB</td>\n",
                            "      <td>r/aws</td>\n",
                            "      <td>...</td>\n",
                            "      <td>https://www.reddit.com/r/aws/comments/10s3qcg/...</td>\n",
                            "      <td>228478</td>\n",
                            "      <td>2023-02-02 22:58:15.340032</td>\n",
                            "      <td>0</td>\n",
                            "      <td>None</td>\n",
                            "      <td>False</td>\n",
                            "      <td>2023-02-02</td>\n",
                            "      <td>2023</td>\n",
                            "      <td>2</td>\n",
                            "      <td>2</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>70</th>\n",
                            "      <td>NaT</td>\n",
                            "      <td>aws</td>\n",
                            "      <td>So, I'm trying to learn how to use ECS to port...</td>\n",
                            "      <td>t2_6jr6y</td>\n",
                            "      <td>False</td>\n",
                            "      <td>None</td>\n",
                            "      <td>0</td>\n",
                            "      <td>False</td>\n",
                            "      <td>Cloudformation: Is it just really bad for ever...</td>\n",
                            "      <td>r/aws</td>\n",
                            "      <td>...</td>\n",
                            "      <td>https://www.reddit.com/r/aws/comments/10pjtk0/...</td>\n",
                            "      <td>228228</td>\n",
                            "      <td>2023-01-31 01:03:57.100032</td>\n",
                            "      <td>0</td>\n",
                            "      <td>None</td>\n",
                            "      <td>False</td>\n",
                            "      <td>2023-01-31</td>\n",
                            "      <td>2023</td>\n",
                            "      <td>1</td>\n",
                            "      <td>31</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>71</th>\n",
                            "      <td>NaT</td>\n",
                            "      <td>aws</td>\n",
                            "      <td>I created a [public hosted zone](https://i.img...</td>\n",
                            "      <td>t2_e8so8</td>\n",
                            "      <td>False</td>\n",
                            "      <td>None</td>\n",
                            "      <td>0</td>\n",
                            "      <td>False</td>\n",
                            "      <td>Why can't I see my domain's nameservers hosted...</td>\n",
                            "      <td>r/aws</td>\n",
                            "      <td>...</td>\n",
                            "      <td>https://www.reddit.com/r/aws/comments/10t7vbt/...</td>\n",
                            "      <td>228564</td>\n",
                            "      <td>2023-02-04 07:19:56.327936</td>\n",
                            "      <td>0</td>\n",
                            "      <td>None</td>\n",
                            "      <td>False</td>\n",
                            "      <td>2023-02-04</td>\n",
                            "      <td>2023</td>\n",
                            "      <td>2</td>\n",
                            "      <td>4</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>72</th>\n",
                            "      <td>NaT</td>\n",
                            "      <td>aws</td>\n",
                            "      <td>I have a project to add ocr and search functio...</td>\n",
                            "      <td>t2_ia3tn</td>\n",
                            "      <td>False</td>\n",
                            "      <td>None</td>\n",
                            "      <td>0</td>\n",
                            "      <td>False</td>\n",
                            "      <td>search document text</td>\n",
                            "      <td>r/aws</td>\n",
                            "      <td>...</td>\n",
                            "      <td>https://www.reddit.com/r/aws/comments/10t349w/...</td>\n",
                            "      <td>228554</td>\n",
                            "      <td>2023-02-04 02:53:44.418304</td>\n",
                            "      <td>0</td>\n",
                            "      <td>None</td>\n",
                            "      <td>False</td>\n",
                            "      <td>2023-02-04</td>\n",
                            "      <td>2023</td>\n",
                            "      <td>2</td>\n",
                            "      <td>4</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>73</th>\n",
                            "      <td>NaT</td>\n",
                            "      <td>aws</td>\n",
                            "      <td>Has anyone read the entire AWS Well-Architecte...</td>\n",
                            "      <td>t2_142kw9ex</td>\n",
                            "      <td>False</td>\n",
                            "      <td>None</td>\n",
                            "      <td>0</td>\n",
                            "      <td>False</td>\n",
                            "      <td>Well Architected Framework Read Through</td>\n",
                            "      <td>r/aws</td>\n",
                            "      <td>...</td>\n",
                            "      <td>https://www.reddit.com/r/aws/comments/10rxlgy/...</td>\n",
                            "      <td>228454</td>\n",
                            "      <td>2023-02-02 18:54:25.607680</td>\n",
                            "      <td>0</td>\n",
                            "      <td>None</td>\n",
                            "      <td>False</td>\n",
                            "      <td>2023-02-02</td>\n",
                            "      <td>2023</td>\n",
                            "      <td>2</td>\n",
                            "      <td>2</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "<p>74 rows × 103 columns</p>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "   approved_at_utc subreddit  \\\n",
                            "0              NaT       aws   \n",
                            "1              NaT       aws   \n",
                            "2              NaT       aws   \n",
                            "3              NaT       aws   \n",
                            "4              NaT       aws   \n",
                            "..             ...       ...   \n",
                            "69             NaT       aws   \n",
                            "70             NaT       aws   \n",
                            "71             NaT       aws   \n",
                            "72             NaT       aws   \n",
                            "73             NaT       aws   \n",
                            "\n",
                            "                                             selftext author_fullname  saved  \\\n",
                            "0   I have this PHP (Laravel) script to get all th...        t2_6j0lp  False   \n",
                            "1   I'm not exactly on the up and up on some of th...     t2_jsvj0raw  False   \n",
                            "2   Hey there,\\n\\nIs anyone using the AWS Transfer...        t2_4b72e  False   \n",
                            "3   I can't imagine the answer is \"it does scale\",...        t2_52j9a  False   \n",
                            "4   Hi,\\n\\nI’m looking at using SES for email list...        t2_3sde0  False   \n",
                            "..                                                ...             ...    ...   \n",
                            "69  I have data in a DDB. I want to make visualiza...        t2_uf837  False   \n",
                            "70  So, I'm trying to learn how to use ECS to port...        t2_6jr6y  False   \n",
                            "71  I created a [public hosted zone](https://i.img...        t2_e8so8  False   \n",
                            "72  I have a project to add ocr and search functio...        t2_ia3tn  False   \n",
                            "73  Has anyone read the entire AWS Well-Architecte...     t2_142kw9ex  False   \n",
                            "\n",
                            "   mod_reason_title  gilded  clicked  \\\n",
                            "0              None       0    False   \n",
                            "1              None       0    False   \n",
                            "2              None       0    False   \n",
                            "3              None       0    False   \n",
                            "4              None       0    False   \n",
                            "..              ...     ...      ...   \n",
                            "69             None       0    False   \n",
                            "70             None       0    False   \n",
                            "71             None       0    False   \n",
                            "72             None       0    False   \n",
                            "73             None       0    False   \n",
                            "\n",
                            "                                                title subreddit_name_prefixed  \\\n",
                            "0     Weird behavior getting a list of S3 object keys                   r/aws   \n",
                            "1   Pattern for ingesting deltas and merging into ...                   r/aws   \n",
                            "2                  AWS Transfer Family price question                   r/aws   \n",
                            "3   How does (does?) API Keys / Usage Plans / WAF ...                   r/aws   \n",
                            "4             SES Number of Contact Lists Per Account                   r/aws   \n",
                            "..                                                ...                     ...   \n",
                            "69        Optimal way to visualize your data from DDB                   r/aws   \n",
                            "70  Cloudformation: Is it just really bad for ever...                   r/aws   \n",
                            "71  Why can't I see my domain's nameservers hosted...                   r/aws   \n",
                            "72                               search document text                   r/aws   \n",
                            "73            Well Architected Framework Read Through                   r/aws   \n",
                            "\n",
                            "    ...                                                url  \\\n",
                            "0   ...  https://www.reddit.com/r/aws/comments/10t6cw6/...   \n",
                            "1   ...  https://www.reddit.com/r/aws/comments/10pmcs3/...   \n",
                            "2   ...  https://www.reddit.com/r/aws/comments/10p0t6t/...   \n",
                            "3   ...  https://www.reddit.com/r/aws/comments/10p3zl9/...   \n",
                            "4   ...  https://www.reddit.com/r/aws/comments/10opryt/...   \n",
                            "..  ...                                                ...   \n",
                            "69  ...  https://www.reddit.com/r/aws/comments/10s3qcg/...   \n",
                            "70  ...  https://www.reddit.com/r/aws/comments/10pjtk0/...   \n",
                            "71  ...  https://www.reddit.com/r/aws/comments/10t7vbt/...   \n",
                            "72  ...  https://www.reddit.com/r/aws/comments/10t349w/...   \n",
                            "73  ...  https://www.reddit.com/r/aws/comments/10rxlgy/...   \n",
                            "\n",
                            "    subreddit_subscribers                created_utc  num_crossposts  media  \\\n",
                            "0                  228560 2023-02-04 05:48:13.401088               0   None   \n",
                            "1                  228234 2023-01-31 03:00:16.421888               0   None   \n",
                            "2                  228218 2023-01-30 12:32:19.943424               0   None   \n",
                            "3                  228191 2023-01-30 14:28:39.265280               0   None   \n",
                            "4                  228222 2023-01-30 02:08:13.197312               0   None   \n",
                            "..                    ...                        ...             ...    ...   \n",
                            "69                 228478 2023-02-02 22:58:15.340032               0   None   \n",
                            "70                 228228 2023-01-31 01:03:57.100032               0   None   \n",
                            "71                 228564 2023-02-04 07:19:56.327936               0   None   \n",
                            "72                 228554 2023-02-04 02:53:44.418304               0   None   \n",
                            "73                 228454 2023-02-02 18:54:25.607680               0   None   \n",
                            "\n",
                            "   is_video        date  year  month day  \n",
                            "0     False  2023-02-04  2023      2   4  \n",
                            "1     False  2023-01-31  2023      1  31  \n",
                            "2     False  2023-01-30  2023      1  30  \n",
                            "3     False  2023-01-30  2023      1  30  \n",
                            "4     False  2023-01-30  2023      1  30  \n",
                            "..      ...         ...   ...    ...  ..  \n",
                            "69    False  2023-02-02  2023      2   2  \n",
                            "70    False  2023-01-31  2023      1  31  \n",
                            "71    False  2023-02-04  2023      2   4  \n",
                            "72    False  2023-02-04  2023      2   4  \n",
                            "73    False  2023-02-02  2023      2   2  \n",
                            "\n",
                            "[74 rows x 103 columns]"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "pandas_df"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# write df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "s3a://reddit-streaming-stevenhurwitt-newfinal_aws_clean/\n"
                    ]
                }
            ],
            "source": [
                "write_path = \"s3a://\" + bucket + \"final_aws_clean/\"\n",
                "print(write_path)\n",
                "# df.write.format(\"delta\").option(\"header\", True).save(write_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "6a371b96-ad9a-4a0a-970b-12c66eb86777",
                "language": "sql"
            },
            "outputs": [],
            "source": [
                "# spark.stop()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "47b2576d-2495-49fd-9c96-b40258870631"
            },
            "source": [
                "## read clean df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "azdata_cell_guid": "f4a21fcf-2512-4c62-99f5-395d13ceacd6",
                "language": "sql",
                "vscode": {
                    "languageId": "sql"
                }
            },
            "outputs": [],
            "source": [
                "def write_spark_jdbc(subreddit):\n",
                "\n",
                "    creds_path = os.path.join(\"/opt\", \"workspace\", \"redditStreaming\", \"creds.json\")\n",
                "\n",
                "    try:\n",
                "        with open(creds_path, \"r\") as f:\n",
                "            creds = json.load(f)\n",
                "            print(\"read creds.json.\")\n",
                "            f.close()\n",
                "\n",
                "    except:\n",
                "        creds_path = \"/home/steven/Documents/reddit-streaming/redditStreaming/creds.json\"\n",
                "        with open(creds_path, \"r\") as f:\n",
                "            creds = json.load(f)\n",
                "            print(\"read creds.json.\")\n",
                "            f.close()\n",
                "\n",
                "    secretmanager_client = boto3.client(\"secretsmanager\", \n",
                "                                    region_name = \"us-east-2\", \n",
                "                                    aws_access_key_id = creds[\"aws_client\"], \n",
                "                                    aws_secret_access_key = creds[\"aws_secret\"])\n",
                "    \n",
                "    df = spark.read.format(\"delta\").option(\"header\", True).load(\"s3a://reddit-streaming-stevenhurwitt/\" + subreddit + \"_clean\")\n",
                "\n",
                "    db_creds = ast.literal_eval(secretmanager_client.get_secret_value(SecretId=\"dev/reddit/postgres\")[\"SecretString\"])\n",
                "    connect_str = \"jdbc:postgresql://{}:5432/reddit\".format(db_creds[\"host\"])\n",
                "\n",
                "    try:\n",
                "        df.write.format(\"jdbc\") \\\n",
                "            .mode(\"overwrite\") \\\n",
                "            .option(\"url\", connect_str) \\\n",
                "            .option(\"dbtable\", \"reddit.{}\".format(subreddit)) \\\n",
                "            .option(\"user\", db_creds[\"username\"]) \\\n",
                "            .option(\"password\", db_creds[\"password\"]) \\\n",
                "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
                "            .save()\n",
                "\n",
                "        print(\"wrote df to postgresql table.\")\n",
                "\n",
                "    except Exception as e:\n",
                "        print(e)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {
                "azdata_cell_guid": "00fe96bc-dd9c-45ea-98f0-dc7c3a727a36",
                "language": "sql",
                "vscode": {
                    "languageId": "sql"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['technology',\n",
                            " 'ProgrammerHumor',\n",
                            " 'news',\n",
                            " 'worldnews',\n",
                            " 'BikiniBottomTwitter',\n",
                            " 'BlackPeopleTwitter',\n",
                            " 'WhitePeopleTwitter',\n",
                            " 'aws']"
                        ]
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "config[\"subreddit\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {
                "azdata_cell_guid": "f231af5a-3c5e-40bf-9406-72f55261b7ad",
                "language": "sql",
                "vscode": {
                    "languageId": "sql"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "subreddit: technology\n",
                        "read creds.json.\n",
                        "23/02/17 05:16:50 WARN DeltaLog: Failed to parse s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log/_last_checkpoint. This may happen if there was an error during read operation, or a file appears to be partial. Sleeping and trying again.\n",
                        "java.nio.file.AccessDeniedException: s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log/_last_checkpoint: getFileStatus on s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log/_last_checkpoint: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 7EGNZAC6M0AT0Q40; S3 Extended Request ID: JQKBA2rHdpQdTfMrrXbsqDK3bNeYxjIqUnU1jksIHrVLKfD5gZGzigH15bdxDcSHgaLy7Ckcj+Y=; Proxy: null), S3 Extended Request ID: JQKBA2rHdpQdTfMrrXbsqDK3bNeYxjIqUnU1jksIHrVLKfD5gZGzigH15bdxDcSHgaLy7Ckcj+Y=:403 Forbidden\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:249)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:170)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3286)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:4903)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1200)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1178)\n",
                        "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
                        "\tat org.apache.spark.sql.delta.storage.HadoopFileSystemLogStore.read(HadoopFileSystemLogStore.scala:54)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:191)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.lastCheckpoint(Checkpoints.scala:184)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.lastCheckpoint$(Checkpoints.scala:183)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.lastCheckpoint(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:248)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:246)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:245)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:53)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:69)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:567)\n",
                        "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$2(DeltaLog.scala:567)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:437)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:114)\n",
                        "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:77)\n",
                        "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:67)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:437)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:113)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:98)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:437)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:566)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:577)\n",
                        "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n",
                        "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n",
                        "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n",
                        "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n",
                        "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n",
                        "\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n",
                        "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:577)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:584)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:487)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:78)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:78)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:107)\n",
                        "\tat scala.Option.getOrElse(Option.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:107)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:95)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:165)\n",
                        "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$4(DeltaDataSource.scala:187)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:50)\n",
                        "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:164)\n",
                        "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n",
                        "\tat scala.Option.getOrElse(Option.scala:189)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n",
                        "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
                        "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
                        "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
                        "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
                        "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
                        "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
                        "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
                        "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
                        "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
                        "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
                        "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
                        "\tat java.lang.Thread.run(Thread.java:750)\n",
                        "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 7EGNZAC6M0AT0Q40; S3 Extended Request ID: JQKBA2rHdpQdTfMrrXbsqDK3bNeYxjIqUnU1jksIHrVLKfD5gZGzigH15bdxDcSHgaLy7Ckcj+Y=; Proxy: null), S3 Extended Request ID: JQKBA2rHdpQdTfMrrXbsqDK3bNeYxjIqUnU1jksIHrVLKfD5gZGzigH15bdxDcSHgaLy7Ckcj+Y=\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1828)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1412)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1374)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n",
                        "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5227)\n",
                        "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5173)\n",
                        "\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1360)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$6(S3AFileSystem.java:2066)\n",
                        "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:412)\n",
                        "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:375)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2056)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2032)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3273)\n",
                        "\t... 82 more\n",
                        "23/02/17 05:16:52 WARN DeltaLog: Failed to parse s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log/_last_checkpoint. This may happen if there was an error during read operation, or a file appears to be partial. Sleeping and trying again.\n",
                        "java.nio.file.AccessDeniedException: s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log/_last_checkpoint: getFileStatus on s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log/_last_checkpoint: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 4QYW50ZK9VV7BZNH; S3 Extended Request ID: q+Px78Si29cG9CsM5V94b915TNPGZdte0bB9xovhm5QKp/QsF+Xsq2QAqADhXDoSzFLPs64zrdE=; Proxy: null), S3 Extended Request ID: q+Px78Si29cG9CsM5V94b915TNPGZdte0bB9xovhm5QKp/QsF+Xsq2QAqADhXDoSzFLPs64zrdE=:403 Forbidden\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:249)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:170)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3286)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:4903)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1200)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1178)\n",
                        "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
                        "\tat org.apache.spark.sql.delta.storage.HadoopFileSystemLogStore.read(HadoopFileSystemLogStore.scala:54)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:191)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:202)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.lastCheckpoint(Checkpoints.scala:184)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.lastCheckpoint$(Checkpoints.scala:183)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.lastCheckpoint(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:248)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:246)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:245)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:53)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:69)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:567)\n",
                        "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$2(DeltaLog.scala:567)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:437)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:114)\n",
                        "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:77)\n",
                        "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:67)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:437)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:113)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:98)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:437)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:566)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:577)\n",
                        "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n",
                        "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n",
                        "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n",
                        "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n",
                        "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n",
                        "\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n",
                        "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:577)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:584)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:487)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:78)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:78)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:107)\n",
                        "\tat scala.Option.getOrElse(Option.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:107)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:95)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:165)\n",
                        "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$4(DeltaDataSource.scala:187)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:50)\n",
                        "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:164)\n",
                        "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n",
                        "\tat scala.Option.getOrElse(Option.scala:189)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n",
                        "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
                        "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
                        "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
                        "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
                        "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
                        "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
                        "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
                        "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
                        "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
                        "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
                        "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
                        "\tat java.lang.Thread.run(Thread.java:750)\n",
                        "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 4QYW50ZK9VV7BZNH; S3 Extended Request ID: q+Px78Si29cG9CsM5V94b915TNPGZdte0bB9xovhm5QKp/QsF+Xsq2QAqADhXDoSzFLPs64zrdE=; Proxy: null), S3 Extended Request ID: q+Px78Si29cG9CsM5V94b915TNPGZdte0bB9xovhm5QKp/QsF+Xsq2QAqADhXDoSzFLPs64zrdE=\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1828)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1412)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1374)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n",
                        "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5227)\n",
                        "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5173)\n",
                        "\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1360)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$6(S3AFileSystem.java:2066)\n",
                        "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:412)\n",
                        "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:375)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2056)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2032)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3273)\n",
                        "\t... 91 more\n",
                        "23/02/17 05:16:53 WARN DeltaLog: Failed to parse s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log/_last_checkpoint. This may happen if there was an error during read operation, or a file appears to be partial. Sleeping and trying again.\n",
                        "java.nio.file.AccessDeniedException: s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log/_last_checkpoint: getFileStatus on s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log/_last_checkpoint: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: R4JEWBB0120H7ZBK; S3 Extended Request ID: BVHNBASC7EJOUXMF2ZFvw3b+AdlRuvzUE3vyiZw1qST7broa3HIaX7ciYuJ1L2XevmEz5vYDcNQ=; Proxy: null), S3 Extended Request ID: BVHNBASC7EJOUXMF2ZFvw3b+AdlRuvzUE3vyiZw1qST7broa3HIaX7ciYuJ1L2XevmEz5vYDcNQ=:403 Forbidden\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:249)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:170)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3286)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:4903)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1200)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1178)\n",
                        "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
                        "\tat org.apache.spark.sql.delta.storage.HadoopFileSystemLogStore.read(HadoopFileSystemLogStore.scala:54)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:191)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:202)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:202)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.lastCheckpoint(Checkpoints.scala:184)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.lastCheckpoint$(Checkpoints.scala:183)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.lastCheckpoint(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:248)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:246)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:245)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:53)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:69)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:567)\n",
                        "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$2(DeltaLog.scala:567)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:437)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:114)\n",
                        "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:77)\n",
                        "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:67)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:437)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:113)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:98)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:437)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:566)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:577)\n",
                        "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n",
                        "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n",
                        "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n",
                        "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n",
                        "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n",
                        "\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n",
                        "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:577)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:584)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:487)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:78)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:78)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:107)\n",
                        "\tat scala.Option.getOrElse(Option.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:107)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:95)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:165)\n",
                        "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$4(DeltaDataSource.scala:187)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:50)\n",
                        "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:164)\n",
                        "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n",
                        "\tat scala.Option.getOrElse(Option.scala:189)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n",
                        "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
                        "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
                        "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
                        "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
                        "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
                        "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
                        "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
                        "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
                        "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
                        "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
                        "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
                        "\tat java.lang.Thread.run(Thread.java:750)\n",
                        "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: R4JEWBB0120H7ZBK; S3 Extended Request ID: BVHNBASC7EJOUXMF2ZFvw3b+AdlRuvzUE3vyiZw1qST7broa3HIaX7ciYuJ1L2XevmEz5vYDcNQ=; Proxy: null), S3 Extended Request ID: BVHNBASC7EJOUXMF2ZFvw3b+AdlRuvzUE3vyiZw1qST7broa3HIaX7ciYuJ1L2XevmEz5vYDcNQ=\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1828)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1412)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1374)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n",
                        "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5227)\n",
                        "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5173)\n",
                        "\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1360)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$6(S3AFileSystem.java:2066)\n",
                        "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:412)\n",
                        "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:375)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2056)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2032)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3273)\n",
                        "\t... 100 more\n",
                        "23/02/17 05:16:54 WARN DeltaLog: s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log/_last_checkpoint is corrupted. Will search the checkpoint files directly\n",
                        "java.nio.file.AccessDeniedException: s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log/_last_checkpoint: getFileStatus on s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log/_last_checkpoint: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: BMW6J8R2224DWCAH; S3 Extended Request ID: n6wpS7GXJTZFCG0vMxMZY7fYSFg1Z2BOEZKDM3S60QYP6FhBFXii4eDJYMiNzmJkoLaVOgHWrIw=; Proxy: null), S3 Extended Request ID: n6wpS7GXJTZFCG0vMxMZY7fYSFg1Z2BOEZKDM3S60QYP6FhBFXii4eDJYMiNzmJkoLaVOgHWrIw=:403 Forbidden\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:249)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:170)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3286)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:4903)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1200)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1178)\n",
                        "\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n",
                        "\tat org.apache.spark.sql.delta.storage.HadoopFileSystemLogStore.read(HadoopFileSystemLogStore.scala:54)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:191)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:202)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:202)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:202)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.lastCheckpoint(Checkpoints.scala:184)\n",
                        "\tat org.apache.spark.sql.delta.Checkpoints.lastCheckpoint$(Checkpoints.scala:183)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.lastCheckpoint(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:248)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:246)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:245)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:64)\n",
                        "\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:53)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:69)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:567)\n",
                        "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$2(DeltaLog.scala:567)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:437)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:114)\n",
                        "\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:77)\n",
                        "\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:67)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:437)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:113)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:98)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:437)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:566)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:577)\n",
                        "\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n",
                        "\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n",
                        "\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n",
                        "\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n",
                        "\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n",
                        "\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n",
                        "\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:577)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:584)\n",
                        "\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:487)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:78)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:78)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:107)\n",
                        "\tat scala.Option.getOrElse(Option.scala:189)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:107)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:95)\n",
                        "\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:165)\n",
                        "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$4(DeltaDataSource.scala:187)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n",
                        "\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n",
                        "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:50)\n",
                        "\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:164)\n",
                        "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n",
                        "\tat scala.Option.getOrElse(Option.scala:189)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n",
                        "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n",
                        "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
                        "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
                        "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
                        "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
                        "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
                        "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
                        "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
                        "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
                        "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
                        "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
                        "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
                        "\tat java.lang.Thread.run(Thread.java:750)\n",
                        "Caused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: BMW6J8R2224DWCAH; S3 Extended Request ID: n6wpS7GXJTZFCG0vMxMZY7fYSFg1Z2BOEZKDM3S60QYP6FhBFXii4eDJYMiNzmJkoLaVOgHWrIw=; Proxy: null), S3 Extended Request ID: n6wpS7GXJTZFCG0vMxMZY7fYSFg1Z2BOEZKDM3S60QYP6FhBFXii4eDJYMiNzmJkoLaVOgHWrIw=\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1828)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1412)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1374)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n",
                        "\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n",
                        "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5227)\n",
                        "\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5173)\n",
                        "\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1360)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$6(S3AFileSystem.java:2066)\n",
                        "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:412)\n",
                        "\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:375)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2056)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2032)\n",
                        "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3273)\n",
                        "\t... 109 more\n",
                        "23/02/17 05:16:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
                    ]
                },
                {
                    "ename": "Py4JJavaError",
                    "evalue": "An error occurred while calling o98.load.\n: java.util.concurrent.ExecutionException: java.nio.file.AccessDeniedException: s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log: getFileStatus on s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: BMWFQHRCMTY0VW8V; S3 Extended Request ID: rTuwE5yq2JVxR0Edwhnkf70yX7qtwl4LvQX9YrBx1Sd6zqU7Hf8EoMrlmbBMv78zibahnVO4ZhI=; Proxy: null), S3 Extended Request ID: rTuwE5yq2JVxR0Edwhnkf70yX7qtwl4LvQX9YrBx1Sd6zqU7Hf8EoMrlmbBMv78zibahnVO4ZhI=:403 Forbidden\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)\n\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)\n\tat com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)\n\tat com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:577)\n\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:584)\n\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:487)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:78)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:78)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:107)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:107)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:95)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:165)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$4(DeltaDataSource.scala:187)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:50)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.nio.file.AccessDeniedException: s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log: getFileStatus on s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: BMWFQHRCMTY0VW8V; S3 Extended Request ID: rTuwE5yq2JVxR0Edwhnkf70yX7qtwl4LvQX9YrBx1Sd6zqU7Hf8EoMrlmbBMv78zibahnVO4ZhI=; Proxy: null), S3 Extended Request ID: rTuwE5yq2JVxR0Edwhnkf70yX7qtwl4LvQX9YrBx1Sd6zqU7Hf8EoMrlmbBMv78zibahnVO4ZhI=:403 Forbidden\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:249)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:170)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3286)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3053)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4263)\n\tat org.apache.spark.sql.delta.storage.S3SingleDriverLogStore.listFromInternal(S3SingleDriverLogStore.scala:120)\n\tat org.apache.spark.sql.delta.storage.S3SingleDriverLogStore.listFrom(S3SingleDriverLogStore.scala:140)\n\tat org.apache.spark.sql.delta.Checkpoints.findLastCompleteCheckpoint(Checkpoints.scala:233)\n\tat org.apache.spark.sql.delta.Checkpoints.findLastCompleteCheckpoint$(Checkpoints.scala:224)\n\tat org.apache.spark.sql.delta.DeltaLog.findLastCompleteCheckpoint(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:208)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:202)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:202)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:202)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n\tat org.apache.spark.sql.delta.Checkpoints.lastCheckpoint(Checkpoints.scala:184)\n\tat org.apache.spark.sql.delta.Checkpoints.lastCheckpoint$(Checkpoints.scala:183)\n\tat org.apache.spark.sql.delta.DeltaLog.lastCheckpoint(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:248)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:246)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:245)\n\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:53)\n\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:69)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:567)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$2(DeltaLog.scala:567)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:437)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:114)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:77)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:67)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:437)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:113)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:98)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:437)\n\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:566)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:577)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n\t... 37 more\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: BMWFQHRCMTY0VW8V; S3 Extended Request ID: rTuwE5yq2JVxR0Edwhnkf70yX7qtwl4LvQX9YrBx1Sd6zqU7Hf8EoMrlmbBMv78zibahnVO4ZhI=; Proxy: null), S3 Extended Request ID: rTuwE5yq2JVxR0Edwhnkf70yX7qtwl4LvQX9YrBx1Sd6zqU7Hf8EoMrlmbBMv78zibahnVO4ZhI=\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1828)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1412)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1374)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5227)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5173)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1360)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$6(S3AFileSystem.java:2066)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:412)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:375)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2056)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2032)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3273)\n\t... 112 more\n",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/ipykernel_105188/2193645216.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"subreddit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subreddit: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mwrite_spark_jdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[0;32m/tmp/ipykernel_105188/2994395969.py\u001b[0m in \u001b[0;36mwrite_spark_jdbc\u001b[0;34m(subreddit)\u001b[0m\n\u001b[1;32m     21\u001b[0m                                     aws_secret_access_key = creds[\"aws_secret\"])\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3a://reddit-streaming-stevenhurwitt/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msubreddit\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_clean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mdb_creds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecretmanager_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_secret_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSecretId\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dev/reddit/postgres\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SecretString\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
                        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o98.load.\n: java.util.concurrent.ExecutionException: java.nio.file.AccessDeniedException: s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log: getFileStatus on s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: BMWFQHRCMTY0VW8V; S3 Extended Request ID: rTuwE5yq2JVxR0Edwhnkf70yX7qtwl4LvQX9YrBx1Sd6zqU7Hf8EoMrlmbBMv78zibahnVO4ZhI=; Proxy: null), S3 Extended Request ID: rTuwE5yq2JVxR0Edwhnkf70yX7qtwl4LvQX9YrBx1Sd6zqU7Hf8EoMrlmbBMv78zibahnVO4ZhI=:403 Forbidden\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)\n\tat com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)\n\tat com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)\n\tat com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)\n\tat com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)\n\tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)\n\tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)\n\tat com.google.common.cache.LocalCache.get(LocalCache.java:4000)\n\tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)\n\tat org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:577)\n\tat org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:584)\n\tat org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:487)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:78)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:78)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:107)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:107)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:95)\n\tat org.apache.spark.sql.delta.catalog.DeltaTableV2.toBaseRelation(DeltaTableV2.scala:165)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.$anonfun$createRelation$4(DeltaDataSource.scala:187)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.recordFrameProfile(DeltaDataSource.scala:50)\n\tat org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:185)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.nio.file.AccessDeniedException: s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log: getFileStatus on s3a://reddit-streaming-stevenhurwitt/technology_clean/_delta_log: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: BMWFQHRCMTY0VW8V; S3 Extended Request ID: rTuwE5yq2JVxR0Edwhnkf70yX7qtwl4LvQX9YrBx1Sd6zqU7Hf8EoMrlmbBMv78zibahnVO4ZhI=; Proxy: null), S3 Extended Request ID: rTuwE5yq2JVxR0Edwhnkf70yX7qtwl4LvQX9YrBx1Sd6zqU7Hf8EoMrlmbBMv78zibahnVO4ZhI=:403 Forbidden\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:249)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:170)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3286)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3053)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4263)\n\tat org.apache.spark.sql.delta.storage.S3SingleDriverLogStore.listFromInternal(S3SingleDriverLogStore.scala:120)\n\tat org.apache.spark.sql.delta.storage.S3SingleDriverLogStore.listFrom(S3SingleDriverLogStore.scala:140)\n\tat org.apache.spark.sql.delta.Checkpoints.findLastCompleteCheckpoint(Checkpoints.scala:233)\n\tat org.apache.spark.sql.delta.Checkpoints.findLastCompleteCheckpoint$(Checkpoints.scala:224)\n\tat org.apache.spark.sql.delta.DeltaLog.findLastCompleteCheckpoint(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:208)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:202)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:202)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:202)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:190)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:124)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:123)\n\tat org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:189)\n\tat org.apache.spark.sql.delta.Checkpoints.lastCheckpoint(Checkpoints.scala:184)\n\tat org.apache.spark.sql.delta.Checkpoints.lastCheckpoint$(Checkpoints.scala:183)\n\tat org.apache.spark.sql.delta.DeltaLog.lastCheckpoint(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:248)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:246)\n\tat org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:245)\n\tat org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:64)\n\tat org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:53)\n\tat org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:69)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:567)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$2(DeltaLog.scala:567)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:120)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:118)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:437)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:114)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:77)\n\tat com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:67)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:437)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:113)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:98)\n\tat org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:437)\n\tat org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:566)\n\tat org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:577)\n\tat com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)\n\tat com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)\n\tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)\n\t... 37 more\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: BMWFQHRCMTY0VW8V; S3 Extended Request ID: rTuwE5yq2JVxR0Edwhnkf70yX7qtwl4LvQX9YrBx1Sd6zqU7Hf8EoMrlmbBMv78zibahnVO4ZhI=; Proxy: null), S3 Extended Request ID: rTuwE5yq2JVxR0Edwhnkf70yX7qtwl4LvQX9YrBx1Sd6zqU7Hf8EoMrlmbBMv78zibahnVO4ZhI=\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1828)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1412)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1374)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5227)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5173)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1360)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$6(S3AFileSystem.java:2066)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:412)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:375)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2056)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2032)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3273)\n\t... 112 more\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "23/02/17 05:17:09 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:17:24 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 0:>                                                          (0 + 0) / 3]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "23/02/17 05:17:39 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:17:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:18:09 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:18:24 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 0:>                                                          (0 + 0) / 3]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "23/02/17 05:18:39 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:18:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:19:09 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:19:24 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 0:>                                                          (0 + 0) / 3]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "23/02/17 05:19:39 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:19:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:20:09 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:20:24 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 0:>                                                          (0 + 0) / 3]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "23/02/17 05:20:39 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:20:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:21:09 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:21:24 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 0:>                                                          (0 + 0) / 3]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "23/02/17 05:21:39 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:21:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:22:09 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:22:24 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 0:>                                                          (0 + 0) / 3]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "23/02/17 05:22:39 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:22:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:23:09 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:23:24 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 0:>                                                          (0 + 0) / 3]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "23/02/17 05:23:39 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:23:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:24:09 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:24:24 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[Stage 0:>                                                          (0 + 0) / 3]\r"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "23/02/17 05:24:39 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:24:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
                        "23/02/17 05:25:09 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
                    ]
                }
            ],
            "source": [
                "for s in config[\"subreddit\"]:\n",
                "    print(\"subreddit: {}\".format(s))\n",
                "    write_spark_jdbc(s)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "cbe1b93d-c62d-4e0e-bafa-661d4c14d578"
            },
            "source": [
                "## write to postgres table"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "64508a5f-71e3-4be9-8b43-4588d088e5c7"
            },
            "source": [
                "### create schema"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "8b8f8867-a8f6-41d3-b55e-1a9b6fec63c2",
                "language": "sql"
            },
            "outputs": [
                {
                    "ename": "OperationalError",
                    "evalue": "could not connect to server: Connection refused\n\tIs the server running on host \"xanaxprincess.asuscomm.com\" (174.170.113.213) and accepting\n\tTCP/IP connections on port 5432?\n",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
                        "\u001b[0;32m/tmp/ipykernel_105188/1375140164.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsycopg2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"postgres_host\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"postgres_user\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"postgres_password\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"postgres\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
                        "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mdsn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dsn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection_factory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconnection_factory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwasync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcursor_factory\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mOperationalError\u001b[0m: could not connect to server: Connection refused\n\tIs the server running on host \"xanaxprincess.asuscomm.com\" (174.170.113.213) and accepting\n\tTCP/IP connections on port 5432?\n"
                    ]
                }
            ],
            "source": [
                "conn = psycopg2.connect(host = config[\"postgres_host\"], user = config[\"postgres_user\"], password = config[\"postgres_password\"], database=\"postgres\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "5e39a567-d873-416a-a428-93dc1f132094",
                "language": "sql"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "name 'df' is not defined\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "23/02/17 05:09:54 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
                    ]
                }
            ],
            "source": [
                "connect_str = \"jdbc:postgresql://{}:5432/postgres\".format(config[\"postgres_host\"])\n",
                "\n",
                "try:\n",
                "    df.write.format(\"jdbc\") \\\n",
                "        .mode(\"overwrite\") \\\n",
                "        .option(\"url\", connect_str) \\\n",
                "        .option(\"dbtable\", \"public.{}\".format(subreddit)) \\\n",
                "        .option(\"user\", config[\"postgres_user\"]) \\\n",
                "        .option(\"password\", config[\"postgres_password\"]) \\\n",
                "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
                "        .save()\n",
                "\n",
                "    print(\"wrote df to postgresql table.\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(e)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "e54fedea-14a9-4052-b2b4-6d3b8f30821e"
            },
            "source": [
                "## stop spark"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {
                "azdata_cell_guid": "65e23599-1cd8-4de0-92a1-d728bcb5ac62",
                "language": "sql",
                "vscode": {
                    "languageId": "sql"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "SparkSession does not exist in the JVM\n"
                    ]
                }
            ],
            "source": [
                "try:\n",
                "    spark.stop()\n",
                "\n",
                "except Exception as e:\n",
                "    print(e)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "f0273f14-2d98-4b40-9e25-cbd3c2ef393d",
                "language": "sql",
                "vscode": {
                    "languageId": "sql"
                }
            },
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "reddit-env",
            "language": "python",
            "name": "reddit-env"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.3"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
