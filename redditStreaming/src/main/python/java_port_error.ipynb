{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Java Port Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Set Java environment variables\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-11-openjdk-amd64'\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['SPARK_LOCAL_IP'] = 'localhost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import ast\n",
    "import yaml\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import yaml\n",
    "import datetime as dt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# sc = SparkContext()\n",
    "# sc.setLogLevel('INFO')\n",
    "logger = logging.getLogger('reddit_streaming')\n",
    "\n",
    "spark_host = \"spark-master\" \n",
    "kafka_host = \"kafka\" \n",
    "subreddit = \"aws\"\n",
    "spark_version = \"3.4.0\"\n",
    "hadoop_version = \"3.3.4\"\n",
    "delta_version = \"1.2.1\"\n",
    "postgres_version = \"9.4.1212\"\n",
    "# aws_client = ast.literal_eval(secretmanager_client.get_secret_value(SecretId=\"AWS_ACCESS_KEY_ID\")[\"SecretString\"])[\"AWS_ACCESS_KEY_ID\"]\n",
    "# aws_secret = ast.literal_eval(secretmanager_client.get_secret_value(SecretId=\"AWS_SECRET_ACCESS_KEY\")[\"SecretString\"])[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "extra_jar_list = f\"org.apache.spark:spark-sql-kafka-0-10_2.12:{spark_version},org.apache.hadoop:hadoop-common:{hadoop_version},org.apache.hadoop:hadoop-aws:{hadoop_version},org.apache.hadoop:hadoop-client:{hadoop_version},io.delta:delta-core_2.12:{delta_version},org.postgresql:postgresql:{postgres_version}\"\n",
    "bucket = \"reddit-streaming-stevenhurwitt-2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_files():\n",
    "    \"\"\"\n",
    "    initializes spark session using config.yaml and creds.json files.\n",
    "    \"\"\"\n",
    "\n",
    "    base = os.getcwd()\n",
    "    creds_path_container = os.path.join(base, \"creds.json\")\n",
    "\n",
    "    creds_dir = \"/\".join(base.split(\"/\")[:-3])\n",
    "    creds_path = os.path.join(base, \"creds.json\")\n",
    "\n",
    "    try:\n",
    "        with open(creds_path, \"r\") as f:\n",
    "            creds = json.load(f)\n",
    "            f.close()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        # print(\"couldn't find: {}.\".format(creds_path))\n",
    "        try:\n",
    "            with open(creds_path_container, \"r\") as f:\n",
    "                creds = json.load(f)\n",
    "                f.close()\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            with open(\"/opt/workspace//redditStreaming/creds.json\", \"r\") as f:\n",
    "                creds = json.load(f)\n",
    "                f.close()\n",
    "\n",
    "    except:\n",
    "        print(\"failed to find creds.json.\")\n",
    "        sys.exit()\n",
    "\n",
    "    try:\n",
    "        with open(\"config.yaml\", \"r\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "            # print(\"read config file.\")\n",
    "            f.close()\n",
    "\n",
    "    except:\n",
    "        print(\"failed to find config.yaml, exiting now.\")\n",
    "        sys.exit()\n",
    "\n",
    "    return(creds, config)\n",
    "def init_spark(subreddit, index):\n",
    "    \"\"\"\n",
    "    initialize spark given config and credential's files\n",
    "\n",
    "    returns: spark, sparkContext (sc)\n",
    "    raises: Exception if Spark session creation fails\n",
    "    \"\"\"\n",
    "    creds, config = read_files()\n",
    "    spark_host = config[\"spark_host\"]\n",
    "    extra_jar_list = config[\"extra_jar_list\"]\n",
    "\n",
    "    # Set Java specific configurations\n",
    "    # os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "    # os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "    \n",
    "    \n",
    "    # initialize spark session\n",
    "    try:\n",
    "        spark = SparkSession.builder.appName(f\"reddit_{subreddit}\") \\\n",
    "                    .master(\"local[*]\") \\\n",
    "                    .config(\"spark.driver.host\", \"localhost\") \\\n",
    "                    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "                    .config(\"spark.scheduler.mode\", \"FAIR\") \\\n",
    "                    .config(\"spark.scheduler.allocation.file\", \"file:///opt/workspace/redditStreaming/fairscheduler.xml\") \\\n",
    "                    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "                    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "                    .config(\"spark.executor.cores\", \"2\") \\\n",
    "                    .config(\"spark.streaming.concurrentJobs\", \"8\") \\\n",
    "                    .config(\"spark.local.dir\", \"/opt/workspace/tmp/spark\") \\\n",
    "                    .config(\"spark.worker.dir\", \"/opt/workspace/tmp/executor/{}/\".format(subreddit)) \\\n",
    "                    .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "                    .config(\"spark.eventLog.dir\", \"file:///opt/workspace/events\") \\\n",
    "                    .config(\"spark.sql.debug.maxToStringFields\", 1000) \\\n",
    "                    .config(\"spark.jars.packages\", extra_jar_list) \\\n",
    "                    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "                    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "                    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "        sc = spark.sparkContext\n",
    "        sc.setLogLevel('WARN')\n",
    "        sc.setLocalProperty(\"spark.scheduler.pool\", \"pool{}\".format(str(index)))\n",
    "        \n",
    "        print(\"Created Spark session successfully\")\n",
    "        return spark, sc\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create Spark session: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "def read_kafka_stream(spark, sc, subreddit):\n",
    "    \"\"\"\n",
    "    reads streaming data from kafka producer\n",
    "\n",
    "    params: spark, sc\n",
    "    returns: df\n",
    "    \"\"\"\n",
    "    creds, config = read_files()\n",
    "    kafka_host = config[\"kafka_host\"]\n",
    "\n",
    "    # define schema for payload data\n",
    "    payload_schema = StructType([\n",
    "        StructField(\"approved_at_utc\", FloatType(), True),\n",
    "        StructField(\"subreddit\", StringType(), False),\n",
    "        StructField(\"selftext\", StringType(), False),\n",
    "        StructField(\"author_fullname\", StringType(), False),\n",
    "        StructField(\"saved\", BooleanType(), False),\n",
    "        StructField(\"mod_reason_title\", StringType(), True),\n",
    "        StructField(\"gilded\", IntegerType(), False),\n",
    "        StructField(\"clicked\", BooleanType(), False),\n",
    "        StructField(\"title\", StringType(), False),\n",
    "        StructField(\"subreddit_name_prefixed\", StringType(), False),\n",
    "        StructField(\"hidden\", BooleanType(), False),\n",
    "        StructField(\"pwls\", IntegerType(), False),\n",
    "        StructField(\"link_flair_css_class\", StringType(), False),\n",
    "        StructField(\"downs\", IntegerType(), False),\n",
    "        StructField(\"thumbnail_height\", IntegerType(), True),\n",
    "        StructField(\"top_awarded_type\", StringType(), True),\n",
    "        StructField(\"hide_score\", BooleanType(), False),\n",
    "        StructField(\"name\", StringType(), False),\n",
    "        StructField(\"quarantine\", BooleanType(), False),\n",
    "        StructField(\"link_flair_text_color\", StringType(), True),\n",
    "        StructField(\"upvote_ratio\", FloatType(), False),\n",
    "        StructField(\"author_flair_background_color\", StringType(), True),\n",
    "        StructField(\"ups\", IntegerType(), False),\n",
    "        StructField(\"total_awards_received\", IntegerType(), False),\n",
    "        StructField(\"thumbnail_width\", IntegerType(), True),\n",
    "        StructField(\"author_flair_template_id\", StringType(), True),\n",
    "        StructField(\"is_original_content\", BooleanType(), False),\n",
    "        StructField(\"secure_media\", StringType(), True),\n",
    "        StructField(\"is_reddit_media_domain\", BooleanType(), False),\n",
    "        StructField(\"is_meta\", BooleanType(), False),\n",
    "        StructField(\"category\", StringType(), True),\n",
    "        StructField(\"link_flair_text\", StringType(), True),\n",
    "        StructField(\"can_mod_post\", BooleanType(), False),\n",
    "        StructField(\"score\", IntegerType(), False),\n",
    "        StructField(\"approved_by\", StringType(), True),\n",
    "        StructField(\"is_created_from_ads_ui\", BooleanType(), False),\n",
    "        StructField(\"author_premium\", BooleanType(), False),\n",
    "        StructField(\"thumbnail\", StringType(), True),\n",
    "        StructField(\"edited\", BooleanType(), False),\n",
    "        StructField(\"author_flair_css_class\", StringType(), True),\n",
    "        StructField(\"post_hint\", StringType(), False),\n",
    "        StructField(\"content_categories\", StringType(), True),\n",
    "        StructField(\"is_self\", BooleanType(), False),\n",
    "        StructField(\"subreddit_type\", StringType(), False),\n",
    "        StructField(\"created\", FloatType(), False),\n",
    "        StructField(\"link_flair_type\", StringType(), True),\n",
    "        StructField(\"wls\", IntegerType(), False),\n",
    "        StructField(\"removed_by_category\", StringType(), True),\n",
    "        StructField(\"banned_by\", StringType(), True),\n",
    "        StructField(\"author_flair_type\", StringType(), True),\n",
    "        StructField(\"domain\", StringType(), True),\n",
    "        StructField(\"allow_live_comments\", BooleanType(), False),\n",
    "        StructField(\"selftext_html\", StringType(), True),\n",
    "        StructField(\"likes\", IntegerType(), True),\n",
    "        StructField(\"suggested_sort\", StringType(), True),\n",
    "        StructField(\"banned_at_utc\", FloatType(), True),\n",
    "        StructField(\"url_overridden_by_dest\", StringType(), True),\n",
    "        StructField(\"view_count\", IntegerType(), True),\n",
    "        StructField(\"archived\", BooleanType(), False),\n",
    "        StructField(\"no_follow\", BooleanType(), False),\n",
    "        StructField(\"is_crosspostable\", BooleanType(), False),\n",
    "        StructField(\"pinned\", BooleanType(), False),\n",
    "        StructField(\"over_18\", BooleanType(), False),\n",
    "        StructField(\"media_only\", BooleanType(), False),\n",
    "        StructField(\"link_flair_template_id\", StringType(), True),\n",
    "        StructField(\"can_gild\", BooleanType(), False),\n",
    "        StructField(\"spoiler\", BooleanType(), False),\n",
    "        StructField(\"locked\", BooleanType(), False),\n",
    "        StructField(\"author_flair_text\", StringType(), True),\n",
    "        StructField(\"visited\", BooleanType(), False),\n",
    "        StructField(\"removed_by\", StringType(), True),\n",
    "        StructField(\"mod_note\", StringType(), True),\n",
    "        StructField(\"distinguished\", StringType(), True),\n",
    "        StructField(\"subreddit_id\", StringType(), False),\n",
    "        StructField(\"author_is_blocked\", BooleanType(), False),\n",
    "        StructField(\"mod_reason_by\", StringType(), True),\n",
    "        StructField(\"num_reports\", IntegerType(), True),\n",
    "        StructField(\"removal_reason\", StringType(), True),\n",
    "        StructField(\"link_flair_background_color\", StringType(), True),\n",
    "        StructField(\"id\", StringType(), False),\n",
    "        StructField(\"is_robot_indexable\", BooleanType(), False),\n",
    "        StructField(\"report_reasons\", StringType(), True),\n",
    "        StructField(\"author\", StringType(), False),\n",
    "        StructField(\"discussion_type\", StringType(), True),\n",
    "        StructField(\"num_comments\", IntegerType(), False),\n",
    "        StructField(\"send_replies\", BooleanType(), False),\n",
    "        StructField(\"whitelist_status\", StringType(), False),\n",
    "        StructField(\"contest_mode\", BooleanType(), False),\n",
    "        StructField(\"author_patreon_flair\", BooleanType(), False),\n",
    "        StructField(\"author_flair_text_color\", StringType(), True),\n",
    "        StructField(\"permalink\", StringType(), False),\n",
    "        StructField(\"parent_whitelist_status\", StringType(), False),\n",
    "        StructField(\"stickied\", BooleanType(), False),\n",
    "        StructField(\"url\", StringType(), False),\n",
    "        StructField(\"subreddit_subscribers\", IntegerType(), False),\n",
    "        StructField(\"created_utc\", FloatType(), False),\n",
    "        StructField(\"num_crossposts\", IntegerType(), False),\n",
    "        StructField(\"media\", StringType(), True),\n",
    "        StructField(\"is_video\", BooleanType(), False),\n",
    "    ])\n",
    "\n",
    "    # read json from kafka and select all columns\n",
    "    df = spark \\\n",
    "            .readStream \\\n",
    "                .format(\"kafka\") \\\n",
    "                .option(\"kafka.bootstrap.servers\", \"{}:9092\".format(kafka_host)) \\\n",
    "                .option(\"subscribe\", \"reddit_\" + subreddit) \\\n",
    "                .option(\"startingOffsets\", \"latest\") \\\n",
    "                .option(\"failOnDataLoss\", \"false\") \\\n",
    "                .load() \\\n",
    "                .selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "                .select(from_json(col(\"json\"), payload_schema).alias(\"data\")) \\\n",
    "                .select(\"data.*\") \n",
    "\n",
    "    return(df)\n",
    "\n",
    "def write_stream(df, subreddit):\n",
    "    \"\"\"\n",
    "    writes streaming data to s3 data lake\n",
    "\n",
    "    params: df\n",
    "    \"\"\"\n",
    "\n",
    "    creds, config = read_files()\n",
    "\n",
    "    bucket = config[\"bucket\"]\n",
    "    # logger.info(\"bucket: {}\".format(bucket))\n",
    "    logger.info(\"subreddit: {}\".format(subreddit))\n",
    "    write_path = f\"s3a://{bucket}/{subreddit}\"\n",
    "    logger.info(\"write path: {}\".format(write_path))\n",
    "\n",
    "    # write subset of df to console\n",
    "    df.withColumn(\"created_utc\", col(\"created_utc\").cast(\"timestamp\")) \\\n",
    "        .select(\"subreddit\", \"title\", \"score\", \"created_utc\") \\\n",
    "        .writeStream \\\n",
    "        .trigger(processingTime='180 seconds') \\\n",
    "        .option(\"truncate\", \"true\") \\\n",
    "        .option(\"checkpointLocation\", \"file:///opt/workspace/checkpoints/{}_console\".format(subreddit)) \\\n",
    "        .outputMode(\"update\") \\\n",
    "        .format(\"console\") \\\n",
    "        .queryName(subreddit + \"_console\") \\\n",
    "        .start()\n",
    "\n",
    "    # write to s3 delta\n",
    "    df.writeStream \\\n",
    "        .trigger(processingTime=\"180 seconds\") \\\n",
    "        .format(\"delta\") \\\n",
    "        .option(\"path\", write_path) \\\n",
    "        .option(\"checkpointLocation\", \"file:///opt/workspace/checkpoints/{}\".format(subreddit)) \\\n",
    "        .option(\"header\", True) \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .queryName(subreddit + \"_delta\") \\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Spark for subreddit: aws\n",
      "Error processing subreddit aws: Java gateway process exited before sending its port number\n",
      "Initializing Spark for subreddit: bikinibottomtwitter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 71: /usr/lib/jvm/java-11-openjdk-amd64/bin/java: No such file or directory\n",
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 96: CMD: bad array subscript\n",
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 71: /usr/lib/jvm/java-11-openjdk-amd64/bin/java: No such file or directory\n",
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 96: CMD: bad array subscript\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing subreddit bikinibottomtwitter: Java gateway process exited before sending its port number\n",
      "Initializing Spark for subreddit: BlackPeopleTwitter\n",
      "Error processing subreddit BlackPeopleTwitter: Java gateway process exited before sending its port number\n",
      "Initializing Spark for subreddit: WhitePeopleTwitter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 71: /usr/lib/jvm/java-11-openjdk-amd64/bin/java: No such file or directory\n",
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 96: CMD: bad array subscript\n",
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 71: /usr/lib/jvm/java-11-openjdk-amd64/bin/java: No such file or directory\n",
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 96: CMD: bad array subscript\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing subreddit WhitePeopleTwitter: Java gateway process exited before sending its port number\n",
      "Initializing Spark for subreddit: news\n",
      "Error processing subreddit news: Java gateway process exited before sending its port number\n",
      "Initializing Spark for subreddit: ProgrammerHumor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 71: /usr/lib/jvm/java-11-openjdk-amd64/bin/java: No such file or directory\n",
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 96: CMD: bad array subscript\n",
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 71: /usr/lib/jvm/java-11-openjdk-amd64/bin/java: No such file or directory\n",
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 96: CMD: bad array subscript\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing subreddit ProgrammerHumor: Java gateway process exited before sending its port number\n",
      "Initializing Spark for subreddit: technology\n",
      "Error processing subreddit technology: Java gateway process exited before sending its port number\n",
      "Initializing Spark for subreddit: worldsnews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 71: /usr/lib/jvm/java-11-openjdk-amd64/bin/java: No such file or directory\n",
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 96: CMD: bad array subscript\n",
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 71: /usr/lib/jvm/java-11-openjdk-amd64/bin/java: No such file or directory\n",
      "/usr/local/lib/python3.7/dist-packages/pyspark/bin/spark-class: line 96: CMD: bad array subscript\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing subreddit worldsnews: Java gateway process exited before sending its port number\n",
      "Fatal error: No streams were successfully created\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'stop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32002/2376184590.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstreams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No streams were successfully created\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: No streams were successfully created",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32002/2376184590.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Fatal error: {str(e)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'spark'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'stop'"
     ]
    }
   ],
   "source": [
    "spark = None\n",
    "streams = []\n",
    "\n",
    "try:\n",
    "    creds, config = read_files()\n",
    "    subreddit_list = config[\"subreddit\"]\n",
    "\n",
    "    # Validate configuration\n",
    "    if not subreddit_list or not isinstance(subreddit_list, list):\n",
    "        raise ValueError(\"Invalid subreddit configuration\")\n",
    "        \n",
    "    for i, s in enumerate(subreddit_list):\n",
    "        try:\n",
    "            print(f\"Initializing Spark for subreddit: {s}\")\n",
    "            spark, sc = init_spark(s, i)\n",
    "            stage_df = read_kafka_stream(spark, sc, s)\n",
    "            write_stream(stage_df, s)\n",
    "            streams.append(stream)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing subreddit {s}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    if not streams:\n",
    "        raise Exception(\"No streams were successfully created\")\n",
    "\n",
    "    print(\"All streams initialized, awaiting termination...\")\n",
    "    spark.streams.awaitAnyTermination()\n",
    "\n",
    "    # Only await termination if at least one stream was created\n",
    "    if 'spark' in locals():\n",
    "        spark.streams.awaitAnyTermination()\n",
    "    else:\n",
    "        print(\"No streams were successfully created\")\n",
    "        sys.exit(1)\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nShutting down gracefully...\")\n",
    "    for stream in streams:\n",
    "        stream.stop()\n",
    "    if 'spark' in locals():\n",
    "        spark.stop()\n",
    "    sys.exit(0)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Fatal error: {str(e)}\")\n",
    "    if 'spark' in locals():\n",
    "        spark.stop()\n",
    "    sys.exit(1)\n",
    "\n",
    "finally:\n",
    "    for stream in streams:\n",
    "        try:\n",
    "            stream.stop()\n",
    "        except:\n",
    "            pass\n",
    "    if spark:\n",
    "        try:\n",
    "            spark.stop()\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
