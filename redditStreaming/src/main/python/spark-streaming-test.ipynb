{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark streaming test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported modules\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from reddit.reddit_streaming import *\n",
    "\n",
    "print(\"imported modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.7/dist-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-common added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-cd00b656-7211-4456-b577-984dcc4de86b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-common;3.2.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.2.0 in central\n",
      "\tfound com.google.guava#guava;11.0.2 in central\n",
      "\tfound commons-cli#commons-cli;1.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.1.1 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.2 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.4 in central\n",
      "\tfound commons-codec#commons-codec;1.11 in central\n",
      "\tfound commons-io#commons-io;2.5 in central\n",
      "\tfound commons-net#commons-net;3.6 in central\n",
      "\tfound commons-collections#commons-collections;3.2.2 in central\n",
      "\tfound javax.servlet#javax.servlet-api;3.1.0 in central\n",
      "\tfound org.eclipse.jetty#jetty-server;9.3.24.v20180605 in central\n",
      "\tfound org.eclipse.jetty#jetty-http;9.3.24.v20180605 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.3.24.v20180605 in central\n",
      "\tfound org.eclipse.jetty#jetty-io;9.3.24.v20180605 in central\n",
      "\tfound org.eclipse.jetty#jetty-servlet;9.3.24.v20180605 in central\n",
      "\tfound org.eclipse.jetty#jetty-security;9.3.24.v20180605 in central\n",
      "\tfound org.eclipse.jetty#jetty-webapp;9.3.24.v20180605 in central\n",
      "\tfound org.eclipse.jetty#jetty-xml;9.3.24.v20180605 in central\n",
      "\tfound com.sun.jersey#jersey-core;1.19 in central\n",
      "\tfound javax.ws.rs#jsr311-api;1.1.1 in central\n",
      "\tfound com.sun.jersey#jersey-servlet;1.19 in central\n",
      "\tfound com.sun.jersey#jersey-server;1.19 in central\n",
      "\tfound com.sun.jersey#jersey-json;1.19 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.11 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in central\n",
      "\tfound log4j#log4j;1.2.17 in central\n",
      "\tfound commons-beanutils#commons-beanutils;1.9.3 in central\n",
      "\tfound org.apache.commons#commons-configuration2;2.1.1 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.7 in central\n",
      "\tfound org.apache.commons#commons-text;1.4 in central\n",
      "\tfound org.slf4j#slf4j-log4j12;1.7.25 in central\n",
      "\tfound org.apache.avro#avro;1.7.7 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\n",
      "\tfound org.apache.commons#commons-compress;1.4.1 in central\n",
      "\tfound org.tukaani#xz;1.0 in central\n",
      "\tfound com.google.re2j#re2j;1.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound com.google.code.gson#gson;2.2.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-auth;3.2.0 in central\n",
      "\tfound com.nimbusds#nimbus-jose-jwt;4.41.1 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound net.minidev#json-smart;2.3 in central\n",
      "\tfound net.minidev#accessors-smart;1.2 in central\n",
      "\tfound org.ow2.asm#asm;5.0.4 in central\n",
      "\tfound org.apache.zookeeper#zookeeper;3.4.13 in central\n",
      "\tfound org.apache.yetus#audience-annotations;0.5.0 in central\n",
      "\tfound io.netty#netty;3.10.5.Final in central\n",
      "\tfound org.apache.curator#curator-framework;2.12.0 in central\n",
      "\tfound org.apache.curator#curator-client;2.12.0 in central\n",
      "\tfound org.apache.kerby#kerb-simplekdc;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-client;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-config;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-core;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-pkix;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-asn1;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-common;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-crypto;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#token-provider;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-admin;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-server;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-identity;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-xdr;1.0.1 in central\n",
      "\tfound com.jcraft#jsch;0.1.54 in central\n",
      "\tfound org.apache.curator#curator-recipes;2.12.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.9.5 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.9.5 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.9.5 in central\n",
      "\tfound org.codehaus.woodstox#stax2-api;3.1.4 in central\n",
      "\tfound com.fasterxml.woodstox#woodstox-core;5.0.3 in central\n",
      "\tfound dnsjava#dnsjava;2.1.7 in central\n",
      "\tfound javax.servlet.jsp#jsp-api;2.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.0 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.375 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/3.2.0/hadoop-common-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-common;3.2.0!hadoop-common.jar (273ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-annotations/3.2.0/hadoop-annotations-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-annotations;3.2.0!hadoop-annotations.jar (33ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/guava/guava/11.0.2/guava-11.0.2.jar ...\n",
      "\t[SUCCESSFUL ] com.google.guava#guava;11.0.2!guava.jar (88ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-cli/commons-cli/1.2/commons-cli-1.2.jar ...\n",
      "\t[SUCCESSFUL ] commons-cli#commons-cli;1.2!commons-cli.jar (48ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-math3;3.1.1!commons-math3.jar (85ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpclient/4.5.2/httpclient-4.5.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.httpcomponents#httpclient;4.5.2!httpclient.jar (47ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-codec/commons-codec/1.11/commons-codec-1.11.jar ...\n",
      "\t[SUCCESSFUL ] commons-codec#commons-codec;1.11!commons-codec.jar (36ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-io/commons-io/2.5/commons-io-2.5.jar ...\n",
      "\t[SUCCESSFUL ] commons-io#commons-io;2.5!commons-io.jar (32ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-net/commons-net/3.6/commons-net-3.6.jar ...\n",
      "\t[SUCCESSFUL ] commons-net#commons-net;3.6!commons-net.jar (35ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar ...\n",
      "\t[SUCCESSFUL ] commons-collections#commons-collections;3.2.2!commons-collections.jar (38ms)\n",
      "downloading https://repo1.maven.org/maven2/javax/servlet/javax.servlet-api/3.1.0/javax.servlet-api-3.1.0.jar ...\n",
      "\t[SUCCESSFUL ] javax.servlet#javax.servlet-api;3.1.0!javax.servlet-api.jar (30ms)\n",
      "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-server/9.3.24.v20180605/jetty-server-9.3.24.v20180605.jar ...\n",
      "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-server;9.3.24.v20180605!jetty-server.jar (41ms)\n",
      "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/9.3.24.v20180605/jetty-util-9.3.24.v20180605.jar ...\n",
      "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-util;9.3.24.v20180605!jetty-util.jar (34ms)\n",
      "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-servlet/9.3.24.v20180605/jetty-servlet-9.3.24.v20180605.jar ...\n",
      "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-servlet;9.3.24.v20180605!jetty-servlet.jar (26ms)\n",
      "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-webapp/9.3.24.v20180605/jetty-webapp-9.3.24.v20180605.jar ...\n",
      "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-webapp;9.3.24.v20180605!jetty-webapp.jar (25ms)\n",
      "downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-core/1.19/jersey-core-1.19.jar ...\n",
      "\t[SUCCESSFUL ] com.sun.jersey#jersey-core;1.19!jersey-core.jar (33ms)\n",
      "downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-servlet/1.19/jersey-servlet-1.19.jar ...\n",
      "\t[SUCCESSFUL ] com.sun.jersey#jersey-servlet;1.19!jersey-servlet.jar (29ms)\n",
      "downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-json/1.19/jersey-json-1.19.jar ...\n",
      "\t[SUCCESSFUL ] com.sun.jersey#jersey-json;1.19!jersey-json.jar (31ms)\n",
      "downloading https://repo1.maven.org/maven2/com/sun/jersey/jersey-server/1.19/jersey-server-1.19.jar ...\n",
      "\t[SUCCESSFUL ] com.sun.jersey#jersey-server;1.19!jersey-server.jar (50ms)\n",
      "downloading https://repo1.maven.org/maven2/log4j/log4j/1.2.17/log4j-1.2.17.jar ...\n",
      "\t[SUCCESSFUL ] log4j#log4j;1.2.17!log4j.jar(bundle) (33ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar ...\n",
      "\t[SUCCESSFUL ] commons-beanutils#commons-beanutils;1.9.3!commons-beanutils.jar (32ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-configuration2/2.1.1/commons-configuration2-2.1.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-configuration2;2.1.1!commons-configuration2.jar (127ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-lang3/3.7/commons-lang3-3.7.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-lang3;3.7!commons-lang3.jar (36ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-text/1.4/commons-text-1.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-text;1.4!commons-text.jar (24ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.25/slf4j-log4j12-1.7.25.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-log4j12;1.7.25!slf4j-log4j12.jar (23ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/avro/avro/1.7.7/avro-1.7.7.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.avro#avro;1.7.7!avro.jar(bundle) (33ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/re2j/re2j/1.1/re2j-1.1.jar ...\n",
      "\t[SUCCESSFUL ] com.google.re2j#re2j;1.1!re2j.jar (29ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.protobuf#protobuf-java;2.5.0!protobuf-java.jar(bundle) (40ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/gson/gson/2.2.4/gson-2.2.4.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.gson#gson;2.2.4!gson.jar (56ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-auth/3.2.0/hadoop-auth-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-auth;3.2.0!hadoop-auth.jar (34ms)\n",
      "downloading https://repo1.maven.org/maven2/com/jcraft/jsch/0.1.54/jsch-0.1.54.jar ...\n",
      "\t[SUCCESSFUL ] com.jcraft#jsch;0.1.54!jsch.jar (38ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/curator/curator-client/2.12.0/curator-client-2.12.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.curator#curator-client;2.12.0!curator-client.jar(bundle) (97ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/curator/curator-recipes/2.12.0/curator-recipes-2.12.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.curator#curator-recipes;2.12.0!curator-recipes.jar(bundle) (36ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/zookeeper/zookeeper/3.4.13/zookeeper-3.4.13.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.zookeeper#zookeeper;3.4.13!zookeeper.jar (46ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-compress;1.4.1!commons-compress.jar (31ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-simplekdc/1.0.1/kerb-simplekdc-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerb-simplekdc;1.0.1!kerb-simplekdc.jar (28ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.9.5/jackson-databind-2.9.5.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-databind;2.9.5!jackson-databind.jar(bundle) (57ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/woodstox/stax2-api/3.1.4/stax2-api-3.1.4.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.woodstox#stax2-api;3.1.4!stax2-api.jar(bundle) (30ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/woodstox/woodstox-core/5.0.3/woodstox-core-5.0.3.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.woodstox#woodstox-core;5.0.3!woodstox-core.jar(bundle) (38ms)\n",
      "downloading https://repo1.maven.org/maven2/dnsjava/dnsjava/2.1.7/dnsjava-2.1.7.jar ...\n",
      "\t[SUCCESSFUL ] dnsjava#dnsjava;2.1.7!dnsjava.jar (38ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcore/4.4.4/httpcore-4.4.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.httpcomponents#httpcore;4.4.4!httpcore.jar (34ms)\n",
      "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-http/9.3.24.v20180605/jetty-http-9.3.24.v20180605.jar ...\n",
      "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-http;9.3.24.v20180605!jetty-http.jar (37ms)\n",
      "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-io/9.3.24.v20180605/jetty-io-9.3.24.v20180605.jar ...\n",
      "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-io;9.3.24.v20180605!jetty-io.jar (31ms)\n",
      "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-security/9.3.24.v20180605/jetty-security-9.3.24.v20180605.jar ...\n",
      "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-security;9.3.24.v20180605!jetty-security.jar (33ms)\n",
      "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-xml/9.3.24.v20180605/jetty-xml-9.3.24.v20180605.jar ...\n",
      "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-xml;9.3.24.v20180605!jetty-xml.jar (31ms)\n",
      "downloading https://repo1.maven.org/maven2/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1.jar ...\n",
      "\t[SUCCESSFUL ] javax.ws.rs#jsr311-api;1.1.1!jsr311-api.jar (26ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.jettison#jettison;1.1!jettison.jar(bundle) (24ms)\n",
      "downloading https://repo1.maven.org/maven2/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar ...\n",
      "\t[SUCCESSFUL ] com.sun.xml.bind#jaxb-impl;2.2.3-1!jaxb-impl.jar (45ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-core-asl;1.9.13!jackson-core-asl.jar (42ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-mapper-asl;1.9.13!jackson-mapper-asl.jar (42ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-jaxrs/1.9.13/jackson-jaxrs-1.9.13.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-jaxrs;1.9.13!jackson-jaxrs.jar (23ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/jackson/jackson-xc/1.9.13/jackson-xc-1.9.13.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.jackson#jackson-xc;1.9.13!jackson-xc.jar (25ms)\n",
      "downloading https://repo1.maven.org/maven2/javax/xml/bind/jaxb-api/2.2.11/jaxb-api-2.2.11.jar ...\n",
      "\t[SUCCESSFUL ] javax.xml.bind#jaxb-api;2.2.11!jaxb-api.jar (35ms)\n",
      "downloading https://repo1.maven.org/maven2/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar ...\n",
      "\t[SUCCESSFUL ] com.thoughtworks.paranamer#paranamer;2.3!paranamer.jar (49ms)\n",
      "downloading https://repo1.maven.org/maven2/org/tukaani/xz/1.0/xz-1.0.jar ...\n",
      "\t[SUCCESSFUL ] org.tukaani#xz;1.0!xz.jar (26ms)\n",
      "downloading https://repo1.maven.org/maven2/com/nimbusds/nimbus-jose-jwt/4.41.1/nimbus-jose-jwt-4.41.1.jar ...\n",
      "\t[SUCCESSFUL ] com.nimbusds#nimbus-jose-jwt;4.41.1!nimbus-jose-jwt.jar (32ms)\n",
      "downloading https://repo1.maven.org/maven2/net/minidev/json-smart/2.3/json-smart-2.3.jar ...\n",
      "\t[SUCCESSFUL ] net.minidev#json-smart;2.3!json-smart.jar(bundle) (31ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/curator/curator-framework/2.12.0/curator-framework-2.12.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.curator#curator-framework;2.12.0!curator-framework.jar(bundle) (27ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar ...\n",
      "\t[SUCCESSFUL ] com.github.stephenc.jcip#jcip-annotations;1.0-1!jcip-annotations.jar (30ms)\n",
      "downloading https://repo1.maven.org/maven2/net/minidev/accessors-smart/1.2/accessors-smart-1.2.jar ...\n",
      "\t[SUCCESSFUL ] net.minidev#accessors-smart;1.2!accessors-smart.jar(bundle) (24ms)\n",
      "downloading https://repo1.maven.org/maven2/org/ow2/asm/asm/5.0.4/asm-5.0.4.jar ...\n",
      "\t[SUCCESSFUL ] org.ow2.asm#asm;5.0.4!asm.jar (30ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/yetus/audience-annotations/0.5.0/audience-annotations-0.5.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.yetus#audience-annotations;0.5.0!audience-annotations.jar (26ms)\n",
      "downloading https://repo1.maven.org/maven2/io/netty/netty/3.10.5.Final/netty-3.10.5.Final.jar ...\n",
      "\t[SUCCESSFUL ] io.netty#netty;3.10.5.Final!netty.jar(bundle) (61ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-client/1.0.1/kerb-client-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerb-client;1.0.1!kerb-client.jar (32ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-admin/1.0.1/kerb-admin-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerb-admin;1.0.1!kerb-admin.jar (29ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerby-config/1.0.1/kerby-config-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerby-config;1.0.1!kerby-config.jar (24ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-core/1.0.1/kerb-core-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerb-core;1.0.1!kerb-core.jar (32ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-common/1.0.1/kerb-common-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerb-common;1.0.1!kerb-common.jar (27ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-util/1.0.1/kerb-util-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerb-util;1.0.1!kerb-util.jar (26ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/token-provider/1.0.1/token-provider-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#token-provider;1.0.1!token-provider.jar (25ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerby-pkix/1.0.1/kerby-pkix-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerby-pkix;1.0.1!kerby-pkix.jar (49ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerby-asn1/1.0.1/kerby-asn1-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerby-asn1;1.0.1!kerby-asn1.jar (28ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerby-util/1.0.1/kerby-util-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerby-util;1.0.1!kerby-util.jar (24ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-crypto/1.0.1/kerb-crypto-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerb-crypto;1.0.1!kerb-crypto.jar (25ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-server/1.0.1/kerb-server-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerb-server;1.0.1!kerb-server.jar (23ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerby-xdr/1.0.1/kerby-xdr-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerby-xdr;1.0.1!kerby-xdr.jar (25ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kerby/kerb-identity/1.0.1/kerb-identity-1.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kerby#kerb-identity;1.0.1!kerb-identity.jar (39ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.9.5/jackson-annotations-2.9.5.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-annotations;2.9.5!jackson-annotations.jar(bundle) (30ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.9.5/jackson-core-2.9.5.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-core;2.9.5!jackson-core.jar(bundle) (33ms)\n",
      "downloading https://repo1.maven.org/maven2/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar ...\n",
      "\t[SUCCESSFUL ] javax.servlet.jsp#jsp-api;2.1!jsp-api.jar (28ms)\n",
      ":: resolution report :: resolve 25855ms :: artifacts dl 3229ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.375 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.9.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.9.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.9.5 from central in [default]\n",
      "\tcom.fasterxml.woodstox#woodstox-core;5.0.3 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.2.4 from central in [default]\n",
      "\tcom.google.guava#guava;11.0.2 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.1 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.54 from central in [default]\n",
      "\tcom.nimbusds#nimbus-jose-jwt;4.41.1 from central in [default]\n",
      "\tcom.sun.jersey#jersey-core;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-json;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-server;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-servlet;1.19 from central in [default]\n",
      "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.3 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.9.3 from central in [default]\n",
      "\tcommons-cli#commons-cli;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.11 from central in [default]\n",
      "\tcommons-collections#commons-collections;3.2.2 from central in [default]\n",
      "\tcommons-io#commons-io;2.5 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tcommons-net#commons-net;3.6 from central in [default]\n",
      "\tdnsjava#dnsjava;2.1.7 from central in [default]\n",
      "\tio.netty#netty;3.10.5.Final from central in [default]\n",
      "\tjavax.servlet#javax.servlet-api;3.1.0 from central in [default]\n",
      "\tjavax.servlet.jsp#jsp-api;2.1 from central in [default]\n",
      "\tjavax.ws.rs#jsr311-api;1.1.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.11 from central in [default]\n",
      "\tlog4j#log4j;1.2.17 from central in [default]\n",
      "\tnet.minidev#accessors-smart;1.2 from central in [default]\n",
      "\tnet.minidev#json-smart;2.3 from central in [default]\n",
      "\torg.apache.avro#avro;1.7.7 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.4.1 from central in [default]\n",
      "\torg.apache.commons#commons-configuration2;2.1.1 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.7 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.1.1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.commons#commons-text;1.4 from central in [default]\n",
      "\torg.apache.curator#curator-client;2.12.0 from central in [default]\n",
      "\torg.apache.curator#curator-framework;2.12.0 from central in [default]\n",
      "\torg.apache.curator#curator-recipes;2.12.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-auth;3.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-common;3.2.0 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.2 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.kerby#kerb-admin;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-client;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-common;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-core;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-crypto;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-identity;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-server;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-simplekdc;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-asn1;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-config;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-pkix;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-xdr;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#token-provider;1.0.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.yetus#audience-annotations;0.5.0 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper;3.4.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-xc;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.codehaus.woodstox#stax2-api;3.1.4 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-http;9.3.24.v20180605 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-io;9.3.24.v20180605 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-security;9.3.24.v20180605 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-server;9.3.24.v20180605 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-servlet;9.3.24.v20180605 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.3.24.v20180605 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-webapp;9.3.24.v20180605 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-xml;9.3.24.v20180605 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.ow2.asm#asm;5.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.slf4j#slf4j-log4j12;1.7.25 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.tukaani#xz;1.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.slf4j#slf4j-api;1.7.25 by [org.slf4j#slf4j-api;1.7.30] in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.0.5 by [org.xerial.snappy#snappy-java;1.1.8.4] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   97  |   80  |   80  |   2   ||   95  |   80  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-cd00b656-7211-4456-b577-984dcc4de86b\n",
      "\tconfs: [default]\n",
      "\t80 artifacts copied, 15 already retrieved (28389kB/91ms)\n",
      "22/05/11 15:25:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created spark successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/11 15:25:23 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "22/05/11 15:25:24 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "22/05/11 15:25:28 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "22/05/11 15:25:34 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (172.19.0.7 executor 1): java.lang.NoSuchMethodError: org.apache.hadoop.util.SemaphoredDelegatingExecutor.<init>(Lcom/google/common/util/concurrent/ListeningExecutorService;IZ)V\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:772)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:329)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:482)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:420)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:409)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:150)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:290)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "22/05/11 15:25:35 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job\n",
      "22/05/11 15:25:35 ERROR FileFormatWriter: Aborting job 408180e3-4a8b-4d62-ad4c-88875d160729.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (172.19.0.7 executor 1): java.lang.NoSuchMethodError: org.apache.hadoop.util.SemaphoredDelegatingExecutor.<init>(Lcom/google/common/util/concurrent/ListeningExecutorService;IZ)V\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:772)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:329)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:482)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:420)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:409)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:150)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:290)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:218)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink.addBatch(FileStreamSink.scala:181)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:600)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:228)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\n",
      "Caused by: java.lang.NoSuchMethodError: org.apache.hadoop.util.SemaphoredDelegatingExecutor.<init>(Lcom/google/common/util/concurrent/ListeningExecutorService;IZ)V\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:772)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:329)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:482)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:420)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:409)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:150)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:290)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/05/11 15:25:35 ERROR MicroBatchExecution: Query [id = 76304bdf-2973-479e-9ae8-ca6cd89c304c, runId = d2e64491-d364-4ca0-ab8a-d2b8c1b79796] terminated with error\n",
      "org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:251)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink.addBatch(FileStreamSink.scala:181)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:600)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:228)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (172.19.0.7 executor 1): java.lang.NoSuchMethodError: org.apache.hadoop.util.SemaphoredDelegatingExecutor.<init>(Lcom/google/common/util/concurrent/ListeningExecutorService;IZ)V\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:772)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:329)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:482)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:420)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:409)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:150)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:290)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:218)\n",
      "\t... 25 more\n",
      "Caused by: java.lang.NoSuchMethodError: org.apache.hadoop.util.SemaphoredDelegatingExecutor.<init>(Lcom/google/common/util/concurrent/ListeningExecutorService;IZ)V\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:772)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:329)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:482)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:420)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:409)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:150)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:290)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$16(FileFormatWriter.scala:229)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    },
    {
     "ename": "StreamingQueryException",
     "evalue": "Job aborted.\n=== Streaming Query ===\nIdentifier: [id = 76304bdf-2973-479e-9ae8-ca6cd89c304c, runId = d2e64491-d364-4ca0-ab8a-d2b8c1b79796]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {KafkaV2[Subscribe[reddit_technology]]: {\"reddit_technology\":{\"0\":3}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nProject [d#23.approved_at_utc AS approved_at_utc#25, d#23.subreddit AS subreddit#26, d#23.selftext AS selftext#27, d#23.author_fullname AS author_fullname#28, d#23.saved AS saved#29, d#23.mod_reason_title AS mod_reason_title#30, d#23.gilded AS gilded#31, d#23.clicked AS clicked#32, d#23.title AS title#33, d#23.subreddit_name_prefixed AS subreddit_name_prefixed#34, d#23.hidden AS hidden#35, d#23.pwls AS pwls#36, d#23.link_flair_css_class AS link_flair_css_class#37, d#23.downs AS downs#38, d#23.thumbnail_height AS thumbnail_height#39, d#23.top_awarded_type AS top_awarded_type#40, d#23.hide_score AS hide_score#41, d#23.name AS name#42, d#23.quarantine AS quarantine#43, d#23.link_flair_text_color AS link_flair_text_color#44, d#23.upvote_ratio AS upvote_ratio#45, d#23.author_flair_background_color AS author_flair_background_color#46, d#23.ups AS ups#47, d#23.total_awards_received AS total_awards_received#48, ... 77 more fields]\n+- Project [from_json(StructField(approved_at_utc,FloatType,true), StructField(subreddit,StringType,false), StructField(selftext,StringType,false), StructField(author_fullname,StringType,false), StructField(saved,BooleanType,false), StructField(mod_reason_title,StringType,true), StructField(gilded,IntegerType,false), StructField(clicked,BooleanType,false), StructField(title,StringType,false), StructField(subreddit_name_prefixed,StringType,false), StructField(hidden,BooleanType,false), StructField(pwls,IntegerType,false), StructField(link_flair_css_class,StringType,false), StructField(downs,IntegerType,false), StructField(thumbnail_height,IntegerType,true), StructField(top_awarded_type,StringType,true), StructField(hide_score,BooleanType,false), StructField(name,StringType,false), StructField(quarantine,BooleanType,false), StructField(link_flair_text_color,StringType,true), StructField(upvote_ratio,FloatType,false), StructField(author_flair_background_color,StringType,true), StructField(ups,IntegerType,false), StructField(total_awards_received,IntegerType,false), ... 79 more fields) AS d#23]\n   +- Project [cast(value#8 as string) AS json#21]\n      +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@1b17223c, KafkaV2[Subscribe[reddit_technology]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12190/3704608347.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stopped spark streaming.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/workspace/redditStreaming/src/main/python/reddit/reddit_streaming.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"s3a://reddit-stevenhurwitt/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msubreddit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"checkpointLocation\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"file:///opt/workspace/checkpoints\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"append\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1310\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: Job aborted.\n=== Streaming Query ===\nIdentifier: [id = 76304bdf-2973-479e-9ae8-ca6cd89c304c, runId = d2e64491-d364-4ca0-ab8a-d2b8c1b79796]\nCurrent Committed Offsets: {}\nCurrent Available Offsets: {KafkaV2[Subscribe[reddit_technology]]: {\"reddit_technology\":{\"0\":3}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nProject [d#23.approved_at_utc AS approved_at_utc#25, d#23.subreddit AS subreddit#26, d#23.selftext AS selftext#27, d#23.author_fullname AS author_fullname#28, d#23.saved AS saved#29, d#23.mod_reason_title AS mod_reason_title#30, d#23.gilded AS gilded#31, d#23.clicked AS clicked#32, d#23.title AS title#33, d#23.subreddit_name_prefixed AS subreddit_name_prefixed#34, d#23.hidden AS hidden#35, d#23.pwls AS pwls#36, d#23.link_flair_css_class AS link_flair_css_class#37, d#23.downs AS downs#38, d#23.thumbnail_height AS thumbnail_height#39, d#23.top_awarded_type AS top_awarded_type#40, d#23.hide_score AS hide_score#41, d#23.name AS name#42, d#23.quarantine AS quarantine#43, d#23.link_flair_text_color AS link_flair_text_color#44, d#23.upvote_ratio AS upvote_ratio#45, d#23.author_flair_background_color AS author_flair_background_color#46, d#23.ups AS ups#47, d#23.total_awards_received AS total_awards_received#48, ... 77 more fields]\n+- Project [from_json(StructField(approved_at_utc,FloatType,true), StructField(subreddit,StringType,false), StructField(selftext,StringType,false), StructField(author_fullname,StringType,false), StructField(saved,BooleanType,false), StructField(mod_reason_title,StringType,true), StructField(gilded,IntegerType,false), StructField(clicked,BooleanType,false), StructField(title,StringType,false), StructField(subreddit_name_prefixed,StringType,false), StructField(hidden,BooleanType,false), StructField(pwls,IntegerType,false), StructField(link_flair_css_class,StringType,false), StructField(downs,IntegerType,false), StructField(thumbnail_height,IntegerType,true), StructField(top_awarded_type,StringType,true), StructField(hide_score,BooleanType,false), StructField(name,StringType,false), StructField(quarantine,BooleanType,false), StructField(link_flair_text_color,StringType,true), StructField(upvote_ratio,FloatType,false), StructField(author_flair_background_color,StringType,true), StructField(ups,IntegerType,false), StructField(total_awards_received,IntegerType,false), ... 79 more fields) AS d#23]\n   +- Project [cast(value#8 as string) AS json#21]\n      +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@1b17223c, KafkaV2[Subscribe[reddit_technology]]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    main()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"stopped spark streaming.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.7/dist-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-adf9a9ca-bb72-42a8-a115-dd58bba6219c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.0 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.375 in central\n",
      ":: resolution report :: resolve 438ms :: artifacts dl 242ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.375 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   15  |   0   |   0   |   0   ||   15  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-adf9a9ca-bb72-42a8-a115-dd58bba6219c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 15 already retrieved (0kB/11ms)\n",
      "22/05/11 13:31:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created spark successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark = SparkSession.builder.appName(\"reddit_\" + subreddit) \\\n",
    "                .master(\"spark://{}:7077\".format(spark_host)) \\\n",
    "                .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "                .config(\"spark.eventLog.dir\", \"file:///opt/workspace/events\") \\\n",
    "                .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0,org.apache.hadoop:hadoop-aws:3.2.0\") \\\n",
    "                .getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    sc.setLogLevel('WARN')\n",
    "    sc.hadoopConfiguration.set(\"fs.s3a.access.key\", \"awsaccesskey value\")\n",
    "    sc.hadoopConfiguration.set(\"fs.s3a.secret.key\", \"aws secretkey value\")\n",
    "    sc.hadoopConfiguration.set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "    print(\"created spark successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload_schema = StructType([\n",
    "    StructField(\"approved_at_utc\", FloatType(), True),\n",
    "    StructField(\"subreddit\", StringType(), False),\n",
    "    StructField(\"selftext\", StringType(), False),\n",
    "    StructField(\"author_fullname\", StringType(), False),\n",
    "    StructField(\"saved\", BooleanType(), False),\n",
    "    StructField(\"mod_reason_title\", StringType(), True),\n",
    "    StructField(\"gilded\", IntegerType(), False),\n",
    "    StructField(\"clicked\", BooleanType(), False),\n",
    "    StructField(\"title\", StringType(), False),\n",
    "    StructField(\"subreddit_name_prefixed\", StringType(), False),\n",
    "    StructField(\"hidden\", BooleanType(), False),\n",
    "    StructField(\"pwls\", IntegerType(), False),\n",
    "    StructField(\"link_flair_css_class\", StringType(), False),\n",
    "    StructField(\"downs\", IntegerType(), False),\n",
    "    StructField(\"thumbnail_height\", IntegerType(), True),\n",
    "    StructField(\"top_awarded_type\", StringType(), True),\n",
    "    StructField(\"hide_score\", BooleanType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"quarantine\", BooleanType(), False),\n",
    "    StructField(\"link_flair_text_color\", StringType(), True),\n",
    "    StructField(\"upvote_ratio\", FloatType(), False),\n",
    "    StructField(\"author_flair_background_color\", StringType(), True),\n",
    "    StructField(\"ups\", IntegerType(), False),\n",
    "    StructField(\"total_awards_received\", IntegerType(), False),\n",
    "    StructField(\"thumbnail_width\", IntegerType(), True),\n",
    "    StructField(\"author_flair_template_id\", StringType(), True),\n",
    "    StructField(\"is_original_content\", BooleanType(), False),\n",
    "    StructField(\"secure_media\", StringType(), True),\n",
    "    StructField(\"is_reddit_media_domain\", BooleanType(), False),\n",
    "    StructField(\"is_meta\", BooleanType(), False),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"link_flair_text\", StringType(), True),\n",
    "    StructField(\"can_mod_post\", BooleanType(), False),\n",
    "    StructField(\"score\", IntegerType(), False),\n",
    "    StructField(\"approved_by\", StringType(), True),\n",
    "    StructField(\"is_created_from_ads_ui\", BooleanType(), False),\n",
    "    StructField(\"author_premium\", BooleanType(), False),\n",
    "    StructField(\"thumbnail\", StringType(), True),\n",
    "    StructField(\"edited\", BooleanType(), False),\n",
    "    StructField(\"author_flair_css_class\", StringType(), True),\n",
    "    StructField(\"post_hint\", StringType(), False),\n",
    "    StructField(\"content_categories\", StringType(), True),\n",
    "    StructField(\"is_self\", BooleanType(), False),\n",
    "    StructField(\"subreddit_type\", StringType(), False),\n",
    "    StructField(\"created\", FloatType(), False),\n",
    "    StructField(\"link_flair_type\", StringType(), True),\n",
    "    StructField(\"wls\", IntegerType(), False),\n",
    "    StructField(\"removed_by_category\", StringType(), True),\n",
    "    StructField(\"banned_by\", StringType(), True),\n",
    "    StructField(\"author_flair_type\", StringType(), True),\n",
    "    StructField(\"domain\", StringType(), True),\n",
    "    StructField(\"allow_live_comments\", BooleanType(), False),\n",
    "    StructField(\"link_flair_css_class\", StringType(), False),\n",
    "    StructField(\"selftext_html\", StringType(), True),\n",
    "    StructField(\"likes\", IntegerType(), True),\n",
    "    StructField(\"suggested_sort\", StringType(), True),\n",
    "    StructField(\"banned_at_utc\", FloatType(), True),\n",
    "    StructField(\"url_overridden_by_dest\", StringType(), True),\n",
    "    StructField(\"view_count\", IntegerType(), True),\n",
    "    StructField(\"archived\", BooleanType(), False),\n",
    "    StructField(\"no_follow\", BooleanType(), False),\n",
    "    StructField(\"is_crosspostable\", BooleanType(), False),\n",
    "    StructField(\"pinned\", BooleanType(), False),\n",
    "    StructField(\"over_18\", BooleanType(), False),\n",
    "    StructField(\"media_only\", BooleanType(), False),\n",
    "    StructField(\"link_flair_template_id\", StringType(), True),\n",
    "    StructField(\"can_gild\", BooleanType(), False),\n",
    "    StructField(\"spoiler\", BooleanType(), False),\n",
    "    StructField(\"locked\", BooleanType(), False),\n",
    "    StructField(\"author_flair_text\", StringType(), True),\n",
    "    StructField(\"visited\", BooleanType(), False),\n",
    "    StructField(\"removed_by\", StringType(), True),\n",
    "    StructField(\"mod_note\", StringType(), True),\n",
    "    StructField(\"distinguished\", StringType(), True),\n",
    "    StructField(\"subreddit_id\", StringType(), False),\n",
    "    StructField(\"author_is_blocked\", BooleanType(), False),\n",
    "    StructField(\"mod_reason_by\", StringType(), True),\n",
    "    StructField(\"num_reports\", IntegerType(), True),\n",
    "    StructField(\"removal_reason\", StringType(), True),\n",
    "    StructField(\"link_flair_background_color\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), False),\n",
    "    StructField(\"is_robot_indexable\", BooleanType(), False),\n",
    "    StructField(\"report_reasons\", StringType(), True),\n",
    "    StructField(\"author\", StringType(), False),\n",
    "    StructField(\"discussion_type\", StringType(), True),\n",
    "    StructField(\"num_comments\", IntegerType(), False),\n",
    "    StructField(\"send_replies\", BooleanType(), False),\n",
    "    StructField(\"whitelist_status\", StringType(), False),\n",
    "    StructField(\"contest_mode\", BooleanType(), False),\n",
    "    StructField(\"author_patreon_flair\", BooleanType(), False),\n",
    "    StructField(\"author_flair_text_color\", StringType(), True),\n",
    "    StructField(\"permalink\", StringType(), False),\n",
    "    StructField(\"parent_whitelist_status\", StringType(), False),\n",
    "    StructField(\"stickied\", BooleanType(), False),\n",
    "    StructField(\"url\", StringType(), False),\n",
    "    StructField(\"subreddit_subscribers\", IntegerType(), False),\n",
    "    StructField(\"pinned\", BooleanType(), False),\n",
    "    StructField(\"created_utc\", FloatType(), False),\n",
    "    StructField(\"num_crossposts\", IntegerType(), False),\n",
    "    StructField(\"media\", StringType(), True),\n",
    "    StructField(\"is_video\", BooleanType(), False),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_df = spark \\\n",
    "            .readStream \\\n",
    "                .format(\"kafka\") \\\n",
    "                .option(\"kafka.bootstrap.servers\", \"{}:9092\".format(kafka_host)) \\\n",
    "                .option(\"subscribe\", \"reddit_\" + subreddit) \\\n",
    "                .option(\"startingOffsets\", \"latest\") \\\n",
    "                .load() \\\n",
    "                .selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "                .select(from_json(col(\"json\"), payload_schema).alias(\"d\")) \\\n",
    "                .select(\"d.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/11 13:11:55 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-cec5ac48-4ff8-4602-9b1a-41ed56914521. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/05/11 13:11:56 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------------+---------+--------+---------------+-----+----------------+------+-------+-----+-----------------------+------+----+--------------------+-----+----------------+----------------+----------+----+----------+---------------------+------------+-----------------------------+---+---------------------+---------------+------------------------+-------------------+------------+----------------------+-------+--------+---------------+------------+-----+-----------+----------------------+--------------+---------+------+----------------------+---------+------------------+-------+--------------+-------+---------------+---+-------------------+---------+-----------------+------+-------------------+--------------------+-------------+-----+--------------+-------------+----------------------+----------+--------+---------+----------------+------+-------+----------+----------------------+--------+-------+------+-----------------+-------+----------+--------+-------------+------------+-----------------+-------------+-----------+--------------+---------------------------+---+------------------+--------------+------+---------------+------------+------------+----------------+------------+--------------------+-----------------------+---------+-----------------------+--------+---+---------------------+------+-----------+--------------+-----+--------+\n",
      "|approved_at_utc|subreddit|selftext|author_fullname|saved|mod_reason_title|gilded|clicked|title|subreddit_name_prefixed|hidden|pwls|link_flair_css_class|downs|thumbnail_height|top_awarded_type|hide_score|name|quarantine|link_flair_text_color|upvote_ratio|author_flair_background_color|ups|total_awards_received|thumbnail_width|author_flair_template_id|is_original_content|secure_media|is_reddit_media_domain|is_meta|category|link_flair_text|can_mod_post|score|approved_by|is_created_from_ads_ui|author_premium|thumbnail|edited|author_flair_css_class|post_hint|content_categories|is_self|subreddit_type|created|link_flair_type|wls|removed_by_category|banned_by|author_flair_type|domain|allow_live_comments|link_flair_css_class|selftext_html|likes|suggested_sort|banned_at_utc|url_overridden_by_dest|view_count|archived|no_follow|is_crosspostable|pinned|over_18|media_only|link_flair_template_id|can_gild|spoiler|locked|author_flair_text|visited|removed_by|mod_note|distinguished|subreddit_id|author_is_blocked|mod_reason_by|num_reports|removal_reason|link_flair_background_color| id|is_robot_indexable|report_reasons|author|discussion_type|num_comments|send_replies|whitelist_status|contest_mode|author_patreon_flair|author_flair_text_color|permalink|parent_whitelist_status|stickied|url|subreddit_subscribers|pinned|created_utc|num_crossposts|media|is_video|\n",
      "+---------------+---------+--------+---------------+-----+----------------+------+-------+-----+-----------------------+------+----+--------------------+-----+----------------+----------------+----------+----+----------+---------------------+------------+-----------------------------+---+---------------------+---------------+------------------------+-------------------+------------+----------------------+-------+--------+---------------+------------+-----+-----------+----------------------+--------------+---------+------+----------------------+---------+------------------+-------+--------------+-------+---------------+---+-------------------+---------+-----------------+------+-------------------+--------------------+-------------+-----+--------------+-------------+----------------------+----------+--------+---------+----------------+------+-------+----------+----------------------+--------+-------+------+-----------------+-------+----------+--------+-------------+------------+-----------------+-------------+-----------+--------------+---------------------------+---+------------------+--------------+------+---------------+------------+------------+----------------+------------+--------------------+-----------------------+---------+-----------------------+--------+---+---------------------+------+-----------+--------------+-----+--------+\n",
      "+---------------+---------+--------+---------------+-----+----------------+------+-------+-----+-----------------------+------+----+--------------------+-----+----------------+----------------+----------+----+----------+---------------------+------------+-----------------------------+---+---------------------+---------------+------------------------+-------------------+------------+----------------------+-------+--------+---------------+------------+-----+-----------+----------------------+--------------+---------+------+----------------------+---------+------------------+-------+--------------+-------+---------------+---+-------------------+---------+-----------------+------+-------------------+--------------------+-------------+-----+--------------+-------------+----------------------+----------+--------+---------+----------------+------+-------+----------+----------------------+--------+-------+------+-----------------+-------+----------+--------+-------------+------------+-----------------+-------------+-----------+--------------+---------------------------+---+------------------+--------------+------+---------------+------------+------------+----------------+------------+--------------------+-----------------------+---------+-----------------------+--------+---+---------------------+------+-----------+--------------+-----+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/11 13:12:07 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 6000 milliseconds, but spent 10196 milliseconds\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+---------------+----------+--------+---------------+-----+----------------+------+-------+--------------------+-----------------------+------+----+--------------------+-----+----------------+----------------+----------+---------+----------+---------------------+------------+-----------------------------+---+---------------------+---------------+------------------------+-------------------+------------+----------------------+-------+--------+------------------+------------+-----+-----------+----------------------+--------------+--------------------+------+----------------------+---------+------------------+-------+--------------+------------+---------------+---+-------------------+---------+-----------------+------------+-------------------+--------------------+-------------+-----+--------------+-------------+----------------------+----------+--------+---------+----------------+------+-------+----------+----------------------+--------+-------+------+-----------------+-------+----------+--------+-------------+------------+-----------------+-------------+-----------+--------------+---------------------------+------+------------------+--------------+-------+---------------+------------+------------+----------------+------------+--------------------+-----------------------+--------------------+-----------------------+--------+--------------------+---------------------+------+------------+--------------+-----+--------+\n",
      "|approved_at_utc| subreddit|selftext|author_fullname|saved|mod_reason_title|gilded|clicked|               title|subreddit_name_prefixed|hidden|pwls|link_flair_css_class|downs|thumbnail_height|top_awarded_type|hide_score|     name|quarantine|link_flair_text_color|upvote_ratio|author_flair_background_color|ups|total_awards_received|thumbnail_width|author_flair_template_id|is_original_content|secure_media|is_reddit_media_domain|is_meta|category|   link_flair_text|can_mod_post|score|approved_by|is_created_from_ads_ui|author_premium|           thumbnail|edited|author_flair_css_class|post_hint|content_categories|is_self|subreddit_type|     created|link_flair_type|wls|removed_by_category|banned_by|author_flair_type|      domain|allow_live_comments|link_flair_css_class|selftext_html|likes|suggested_sort|banned_at_utc|url_overridden_by_dest|view_count|archived|no_follow|is_crosspostable|pinned|over_18|media_only|link_flair_template_id|can_gild|spoiler|locked|author_flair_text|visited|removed_by|mod_note|distinguished|subreddit_id|author_is_blocked|mod_reason_by|num_reports|removal_reason|link_flair_background_color|    id|is_robot_indexable|report_reasons| author|discussion_type|num_comments|send_replies|whitelist_status|contest_mode|author_patreon_flair|author_flair_text_color|           permalink|parent_whitelist_status|stickied|                 url|subreddit_subscribers|pinned| created_utc|num_crossposts|media|is_video|\n",
      "+---------------+----------+--------+---------------+-----+----------------+------+-------+--------------------+-----------------------+------+----+--------------------+-----+----------------+----------------+----------+---------+----------+---------------------+------------+-----------------------------+---+---------------------+---------------+------------------------+-------------------+------------+----------------------+-------+--------+------------------+------------+-----+-----------+----------------------+--------------+--------------------+------+----------------------+---------+------------------+-------+--------------+------------+---------------+---+-------------------+---------+-----------------+------------+-------------------+--------------------+-------------+-----+--------------+-------------+----------------------+----------+--------+---------+----------------+------+-------+----------+----------------------+--------+-------+------+-----------------+-------+----------+--------+-------------+------------+-----------------+-------------+-----------+--------------+---------------------------+------+------------------+--------------+-------+---------------+------------+------------+----------------+------------+--------------------+-----------------------+--------------------+-----------------------+--------+--------------------+---------------------+------+------------+--------------+-----+--------+\n",
      "|           null|technology|        |    t2_4e88rhze|false|            null|     0|  false|AT&amp;T will use...|           r/technology| false|   6|             general|    0|              92|            null|      true|t3_un8sox|     false|                 dark|         0.8|                         null|  3|                    0|            140|                    null|              false|        null|                 false|  false|    null|Networking/Telecom|       false|    3|       null|                 false|          true|https://b.thumbs....| false|                  null|     link|              null|  false|        public|1.65227238E9|           text|  6|               null|     null|             text|engadget.com|              false|             general|         null| null|          null|         null|  https://www.engad...|      null|   false|    false|            true| false|  false|     false|  69c9e74a-a816-11e...|    true|  false| false|             null|  false|      null|    null|         null|    t5_2qh16|            false|         null|       null|          null|                           |un8sox|              true|          null|Sorin61|           null|           0|        true|         all_ads|       false|               false|                   null|/r/technology/com...|                all_ads|   false|https://www.engad...|             11958334| false|1.65227238E9|             0| null|   false|\n",
      "+---------------+----------+--------+---------------+-----+----------------+------+-------+--------------------+-----------------------+------+----+--------------------+-----+----------------+----------------+----------+---------+----------+---------------------+------------+-----------------------------+---+---------------------+---------------+------------------------+-------------------+------------+----------------------+-------+--------+------------------+------------+-----+-----------+----------------------+--------------+--------------------+------+----------------------+---------+------------------+-------+--------------+------------+---------------+---+-------------------+---------+-----------------+------------+-------------------+--------------------+-------------+-----+--------------+-------------+----------------------+----------+--------+---------+----------------+------+-------+----------+----------------------+--------+-------+------+-----------------+-------+----------+--------+-------------+------------+-----------------+-------------+-----------+--------------+---------------------------+------+------------------+--------------+-------+---------------+------------+------------+----------------+------------+--------------------+-----------------------+--------------------+-----------------------+--------+--------------------+---------------------+------+------------+--------------+-----+--------+\n",
      "\n",
      "stopped streaming.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    stage_df.writeStream \\\n",
    "        .trigger(processingTime='6 seconds') \\\n",
    "        .outputMode(\"update\") \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", \"true\") \\\n",
    "        .start() \\\n",
    "        .awaitTermination()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"stopped streaming.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped spark successfully.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"stopped spark successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
