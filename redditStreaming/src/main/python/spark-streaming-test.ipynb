{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark streaming test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couldn't find: /Users/stevenhurwitt/Documents/reddit-streaming/redditStreaming/creds.json.\n",
      "imported modules\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from reddit.reddit_streaming import *\n",
    "\n",
    "creds, config = read_files()\n",
    "subreddit = config[\"subreddit\"]\n",
    "kafka_host = config[\"kafka_host\"]\n",
    "spark_host = config[\"spark_host\"]\n",
    "aws_client = creds[\"aws-client\"]\n",
    "aws_secret = creds[\"aws-secret\"]\n",
    "\n",
    "print(\"imported modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couldn't find: /Users/stevenhurwitt/Documents/reddit-streaming/redditStreaming/creds.json.\n",
      "created spark successfully\n",
      "couldn't find: /Users/stevenhurwitt/Documents/reddit-streaming/redditStreaming/creds.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/09 16:31:24 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-2f0bd224-08fd-46ba-8ff6-13b226e9b146--862296223-driver-0-1, groupId=spark-kafka-source-2f0bd224-08fd-46ba-8ff6-13b226e9b146--862296223-driver-0] Connection to node -1 (kafka/172.20.0.5:9092) could not be established. Broker may not be available.\n",
      "22/06/09 16:31:24 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-2f0bd224-08fd-46ba-8ff6-13b226e9b146--862296223-driver-0-1, groupId=spark-kafka-source-2f0bd224-08fd-46ba-8ff6-13b226e9b146--862296223-driver-0] Bootstrap broker kafka:9092 (id: -1 rack: null) disconnected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped spark streaming.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/09 16:31:27 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-2f0bd224-08fd-46ba-8ff6-13b226e9b146--862296223-driver-0-1, groupId=spark-kafka-source-2f0bd224-08fd-46ba-8ff6-13b226e9b146--862296223-driver-0] Connection to node -1 (kafka/172.20.0.5:9092) could not be established. Broker may not be available.\n",
      "22/06/09 16:31:27 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-2f0bd224-08fd-46ba-8ff6-13b226e9b146--862296223-driver-0-1, groupId=spark-kafka-source-2f0bd224-08fd-46ba-8ff6-13b226e9b146--862296223-driver-0] Bootstrap broker kafka:9092 (id: -1 rack: null) disconnected\n",
      "22/06/09 16:31:30 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-2f0bd224-08fd-46ba-8ff6-13b226e9b146--862296223-driver-0-1, groupId=spark-kafka-source-2f0bd224-08fd-46ba-8ff6-13b226e9b146--862296223-driver-0] Connection to node -1 (kafka/172.20.0.5:9092) could not be established. Broker may not be available.\n",
      "22/06/09 16:31:30 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-2f0bd224-08fd-46ba-8ff6-13b226e9b146--862296223-driver-0-1, groupId=spark-kafka-source-2f0bd224-08fd-46ba-8ff6-13b226e9b146--862296223-driver-0] Bootstrap broker kafka:9092 (id: -1 rack: null) disconnected\n",
      "22/06/09 16:31:33 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-2f0bd224-08fd-46ba-8ff6-13b226e9b146--862296223-driver-0-1, groupId=spark-kafka-source-2f0bd224-08fd-46ba-8ff6-13b226e9b146--862296223-driver-0] Connection to node -1 (kafka/172.20.0.5:9092) could not be established. Broker may not be available.\n",
      "22/06/09 16:31:33 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-2f0bd224-08fd-46ba-8ff6-13b226e9b146--862296223-driver-0-1, groupId=spark-kafka-source-2f0bd224-08fd-46ba-8ff6-13b226e9b146--862296223-driver-0] Bootstrap broker kafka:9092 (id: -1 rack: null) disconnected\n",
      "22/06/09 16:31:34 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-2f0bd224-08fd-46ba-8ff6-13b226e9b146--862296223-driver-0-1, groupId=spark-kafka-source-2f0bd224-08fd-46ba-8ff6-13b226e9b146--862296223-driver-0] Error connecting to node kafka:9092 (id: -1 rack: null)\n",
      "java.net.UnknownHostException: kafka: No address associated with hostname\n",
      "\tat java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)\n",
      "\tat java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:929)\n",
      "\tat java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1324)\n",
      "\tat java.net.InetAddress.getAllByName0(InetAddress.java:1277)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1193)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1127)\n",
      "\tat org.apache.kafka.clients.DefaultHostResolver.resolve(DefaultHostResolver.java:27)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:111)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:512)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:466)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:172)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:985)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$600(NetworkClient.java:73)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1158)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1046)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:559)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:265)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:236)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:227)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:164)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:257)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:480)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1261)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1232)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1165)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReaderConsumer.scala:549)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReaderConsumer.scala:594)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.withRetriesWithoutInterrupt(KafkaOffsetReaderConsumer.scala:593)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReaderConsumer.scala:547)\n",
      "\tat org.apache.spark.util.UninterruptibleThreadRunner.runUninterruptibly(UninterruptibleThreadRunner.scala:48)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.partitionsAssignedToConsumer(KafkaOffsetReaderConsumer.scala:547)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.fetchLatestOffsets(KafkaOffsetReaderConsumer.scala:344)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:233)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:228)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$5(MicroBatchExecution.scala:394)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$3(MicroBatchExecution.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:387)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:384)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:627)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:380)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:210)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\n",
      "22/06/09 16:31:34 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-2f0bd224-08fd-46ba-8ff6-13b226e9b146--862296223-driver-0-1, groupId=spark-kafka-source-2f0bd224-08fd-46ba-8ff6-13b226e9b146--862296223-driver-0] Bootstrap broker kafka:9092 (id: -1 rack: null) disconnected\n",
      "22/06/09 16:31:35 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-2f0bd224-08fd-46ba-8ff6-13b226e9b146--862296223-driver-0-1, groupId=spark-kafka-source-2f0bd224-08fd-46ba-8ff6-13b226e9b146--862296223-driver-0] Error connecting to node kafka:9092 (id: -1 rack: null)\n",
      "java.net.UnknownHostException: kafka\n",
      "\tat java.net.InetAddress.getAllByName0(InetAddress.java:1281)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1193)\n",
      "\tat java.net.InetAddress.getAllByName(InetAddress.java:1127)\n",
      "\tat org.apache.kafka.clients.DefaultHostResolver.resolve(DefaultHostResolver.java:27)\n",
      "\tat org.apache.kafka.clients.ClientUtils.resolve(ClientUtils.java:111)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.currentAddress(ClusterConnectionStates.java:512)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates$NodeConnectionState.access$200(ClusterConnectionStates.java:466)\n",
      "\tat org.apache.kafka.clients.ClusterConnectionStates.currentAddress(ClusterConnectionStates.java:172)\n",
      "\tat org.apache.kafka.clients.NetworkClient.initiateConnect(NetworkClient.java:985)\n",
      "\tat org.apache.kafka.clients.NetworkClient.access$600(NetworkClient.java:73)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1158)\n",
      "\tat org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.maybeUpdate(NetworkClient.java:1046)\n",
      "\tat org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:559)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:265)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:236)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:227)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.awaitMetadataUpdate(ConsumerNetworkClient.java:164)\n",
      "\tat org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureCoordinatorReady(AbstractCoordinator.java:257)\n",
      "\tat org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:480)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1261)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1232)\n",
      "\tat org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1165)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.$anonfun$partitionsAssignedToConsumer$2(KafkaOffsetReaderConsumer.scala:549)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.$anonfun$withRetriesWithoutInterrupt$1(KafkaOffsetReaderConsumer.scala:594)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.withRetriesWithoutInterrupt(KafkaOffsetReaderConsumer.scala:593)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.$anonfun$partitionsAssignedToConsumer$1(KafkaOffsetReaderConsumer.scala:547)\n",
      "\tat org.apache.spark.util.UninterruptibleThreadRunner.runUninterruptibly(UninterruptibleThreadRunner.scala:48)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.partitionsAssignedToConsumer(KafkaOffsetReaderConsumer.scala:547)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderConsumer.fetchLatestOffsets(KafkaOffsetReaderConsumer.scala:344)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.$anonfun$getOrCreateInitialPartitionOffsets$1(KafkaMicroBatchStream.scala:233)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.getOrCreateInitialPartitionOffsets(KafkaMicroBatchStream.scala:228)\n",
      "\tat org.apache.spark.sql.kafka010.KafkaMicroBatchStream.initialOffset(KafkaMicroBatchStream.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$5(MicroBatchExecution.scala:394)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$3(MicroBatchExecution.scala:394)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$2(MicroBatchExecution.scala:387)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$constructNextBatch$1(MicroBatchExecution.scala:384)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.withProgressLocked(MicroBatchExecution.scala:627)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.constructNextBatch(MicroBatchExecution.scala:380)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:210)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209)\n",
      "22/06/09 16:31:35 WARN NetworkClient: [Consumer clientId=consumer-spark-kafka-source-2f0bd224-08fd-46ba-8ff6-13b226e9b146--862296223-driver-0-1, groupId=spark-kafka-source-2f0bd224-08fd-46ba-8ff6-13b226e9b146--862296223-driver-0] Bootstrap broker kafka:9092 (id: -1 rack: null) disconnected\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    main()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"stopped spark streaming.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/spark-3.2.0-bin-hadoop3.2/conf/spark-env.sh: line 45: export: `=': not a valid identifier\n",
      "/usr/bin/spark-3.2.0-bin-hadoop3.2/conf/spark-env.sh: line 45: export: `8080': not a valid identifier\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/bin/spark-3.2.0-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-common added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.apache.hadoop#hadoop-client added as a dependency\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7a1fb7b2-b262-4342-bbe8-b25313bd3470;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 in local-m2-cache\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 in local-m2-cache\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in local-m2-cache\n",
      "\tfound org.lz4#lz4-java;1.7.1 in local-m2-cache\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in local-m2-cache\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in local-m2-cache\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in local-m2-cache\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in local-m2-cache\n",
      "\tfound commons-logging#commons-logging;1.1.3 in local-m2-cache\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in local-m2-cache\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-common;3.3.1 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.3.1 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound com.google.guava#guava;27.0-jre in local-m2-cache\n",
      "\tfound com.google.guava#failureaccess;1.0 in local-m2-cache\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in local-m2-cache\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in local-m2-cache\n",
      "\tfound org.checkerframework#checker-qual;2.5.2 in local-m2-cache\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in local-m2-cache\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.17 in local-m2-cache\n",
      "\tfound commons-cli#commons-cli;1.2 in local-m2-cache\n",
      "\tfound org.apache.commons#commons-math3;3.1.1 in local-m2-cache\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in local-m2-cache\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in local-m2-cache\n",
      "\tfound commons-codec#commons-codec;1.11 in local-m2-cache\n",
      "\tfound commons-io#commons-io;2.8.0 in local-m2-cache\n",
      "\tfound commons-net#commons-net;3.6 in local-m2-cache\n",
      "\tfound commons-collections#commons-collections;3.2.2 in local-m2-cache\n",
      "\tfound javax.servlet#javax.servlet-api;3.1.0 in local-m2-cache\n",
      "\tfound org.eclipse.jetty#jetty-server;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-http;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-io;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-servlet;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-security;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-webapp;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-xml;9.4.40.v20210413 in central\n",
      "\tfound com.sun.jersey#jersey-core;1.19 in local-m2-cache\n",
      "\tfound javax.ws.rs#jsr311-api;1.1.1 in local-m2-cache\n",
      "\tfound com.sun.jersey#jersey-servlet;1.19 in local-m2-cache\n",
      "\tfound com.sun.jersey#jersey-server;1.19 in local-m2-cache\n",
      "\tfound com.sun.jersey#jersey-json;1.19 in local-m2-cache\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in local-m2-cache\n",
      "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in local-m2-cache\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.11 in local-m2-cache\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in local-m2-cache\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in local-m2-cache\n",
      "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in central\n",
      "\tfound log4j#log4j;1.2.17 in local-m2-cache\n",
      "\tfound commons-beanutils#commons-beanutils;1.9.4 in local-m2-cache\n",
      "\tfound org.apache.commons#commons-configuration2;2.1.1 in local-m2-cache\n",
      "\tfound org.apache.commons#commons-lang3;3.7 in local-m2-cache\n",
      "\tfound org.apache.commons#commons-text;1.4 in local-m2-cache\n",
      "\tfound org.slf4j#slf4j-log4j12;1.7.30 in local-m2-cache\n",
      "\tfound org.apache.avro#avro;1.7.7 in local-m2-cache\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.3 in local-m2-cache\n",
      "\tfound org.apache.commons#commons-compress;1.19 in local-m2-cache\n",
      "\tfound com.google.re2j#re2j;1.1 in local-m2-cache\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in local-m2-cache\n",
      "\tfound com.google.code.gson#gson;2.2.4 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-auth;3.3.1 in central\n",
      "\tfound com.nimbusds#nimbus-jose-jwt;9.8.1 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in local-m2-cache\n",
      "\tfound net.minidev#json-smart;2.4.2 in central\n",
      "\tfound net.minidev#accessors-smart;2.4.2 in central\n",
      "\tfound org.ow2.asm#asm;5.0.4 in local-m2-cache\n",
      "\tfound org.apache.zookeeper#zookeeper;3.5.6 in local-m2-cache\n",
      "\tfound org.apache.zookeeper#zookeeper-jute;3.5.6 in local-m2-cache\n",
      "\tfound org.apache.yetus#audience-annotations;0.5.0 in local-m2-cache\n",
      "\tfound org.apache.curator#curator-framework;4.2.0 in local-m2-cache\n",
      "\tfound org.apache.curator#curator-client;4.2.0 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-simplekdc;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-client;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerby-config;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-core;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerby-pkix;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerby-asn1;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerby-util;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-common;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-crypto;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-util;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#token-provider;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-admin;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-server;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerb-identity;1.0.1 in local-m2-cache\n",
      "\tfound org.apache.kerby#kerby-xdr;1.0.1 in local-m2-cache\n",
      "\tfound com.jcraft#jsch;0.1.55 in local-m2-cache\n",
      "\tfound org.apache.curator#curator-recipes;4.2.0 in local-m2-cache\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.10.5.1 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.10.5 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.10.5 in central\n",
      "\tfound org.codehaus.woodstox#stax2-api;4.2.1 in central\n",
      "\tfound com.fasterxml.woodstox#woodstox-core;5.3.0 in central\n",
      "\tfound dnsjava#dnsjava;2.1.7 in local-m2-cache\n",
      "\tfound jakarta.activation#jakarta.activation-api;1.2.1 in local-m2-cache\n",
      "\tfound javax.servlet.jsp#jsp-api;2.1 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-client;3.3.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-hdfs-client;3.3.1 in central\n",
      "\tfound com.squareup.okhttp#okhttp;2.7.5 in local-m2-cache\n",
      "\tfound com.squareup.okio#okio;1.6.0 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-yarn-api;3.3.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-client;3.3.1 in central\n",
      "\tfound org.eclipse.jetty.websocket#websocket-client;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-client;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty.websocket#websocket-common;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty.websocket#websocket-api;9.4.40.v20210413 in central\n",
      "\tfound org.jline#jline;3.9.0 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-mapreduce-client-core;3.3.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-common;3.3.1 in central\n",
      "\tfound com.sun.jersey#jersey-client;1.19 in local-m2-cache\n",
      "\tfound com.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.10.5 in central\n",
      "\tfound jakarta.xml.bind#jakarta.xml.bind-api;2.3.2 in local-m2-cache\n",
      "\tfound com.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.10.5 in central\n",
      "\tfound com.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.10.5 in central\n",
      "\tfound org.apache.hadoop#hadoop-mapreduce-client-jobclient;3.3.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-mapreduce-client-common;3.3.1 in central\n",
      "\tfound io.delta#delta-core_2.12;1.2.1 in central\n",
      "\tfound io.delta#delta-storage;1.2.1 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in local-m2-cache\n",
      ":: resolution report :: resolve 3895ms :: artifacts dl 148ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.10.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.10.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.10.5.1 from central in [default]\n",
      "\tcom.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.10.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.10.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.10.5 from central in [default]\n",
      "\tcom.fasterxml.woodstox#woodstox-core;5.3.0 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from local-m2-cache in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from local-m2-cache in [default]\n",
      "\tcom.google.code.gson#gson;2.2.4 from local-m2-cache in [default]\n",
      "\tcom.google.guava#failureaccess;1.0 from local-m2-cache in [default]\n",
      "\tcom.google.guava#guava;27.0-jre from local-m2-cache in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from local-m2-cache in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from local-m2-cache in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from local-m2-cache in [default]\n",
      "\tcom.google.re2j#re2j;1.1 from local-m2-cache in [default]\n",
      "\tcom.jcraft#jsch;0.1.55 from local-m2-cache in [default]\n",
      "\tcom.nimbusds#nimbus-jose-jwt;9.8.1 from central in [default]\n",
      "\tcom.squareup.okhttp#okhttp;2.7.5 from local-m2-cache in [default]\n",
      "\tcom.squareup.okio#okio;1.6.0 from local-m2-cache in [default]\n",
      "\tcom.sun.jersey#jersey-client;1.19 from local-m2-cache in [default]\n",
      "\tcom.sun.jersey#jersey-core;1.19 from local-m2-cache in [default]\n",
      "\tcom.sun.jersey#jersey-json;1.19 from local-m2-cache in [default]\n",
      "\tcom.sun.jersey#jersey-server;1.19 from local-m2-cache in [default]\n",
      "\tcom.sun.jersey#jersey-servlet;1.19 from local-m2-cache in [default]\n",
      "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from local-m2-cache in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.3 from local-m2-cache in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.9.4 from local-m2-cache in [default]\n",
      "\tcommons-cli#commons-cli;1.2 from local-m2-cache in [default]\n",
      "\tcommons-codec#commons-codec;1.11 from local-m2-cache in [default]\n",
      "\tcommons-collections#commons-collections;3.2.2 from local-m2-cache in [default]\n",
      "\tcommons-io#commons-io;2.8.0 from local-m2-cache in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from local-m2-cache in [default]\n",
      "\tcommons-net#commons-net;3.6 from local-m2-cache in [default]\n",
      "\tdnsjava#dnsjava;2.1.7 from local-m2-cache in [default]\n",
      "\tio.delta#delta-core_2.12;1.2.1 from central in [default]\n",
      "\tio.delta#delta-storage;1.2.1 from central in [default]\n",
      "\tjakarta.activation#jakarta.activation-api;1.2.1 from local-m2-cache in [default]\n",
      "\tjakarta.xml.bind#jakarta.xml.bind-api;2.3.2 from local-m2-cache in [default]\n",
      "\tjavax.servlet#javax.servlet-api;3.1.0 from local-m2-cache in [default]\n",
      "\tjavax.servlet.jsp#jsp-api;2.1 from local-m2-cache in [default]\n",
      "\tjavax.ws.rs#jsr311-api;1.1.1 from local-m2-cache in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.11 from local-m2-cache in [default]\n",
      "\tlog4j#log4j;1.2.17 from local-m2-cache in [default]\n",
      "\tnet.minidev#accessors-smart;2.4.2 from central in [default]\n",
      "\tnet.minidev#json-smart;2.4.2 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from local-m2-cache in [default]\n",
      "\torg.apache.avro#avro;1.7.7 from local-m2-cache in [default]\n",
      "\torg.apache.commons#commons-compress;1.19 from local-m2-cache in [default]\n",
      "\torg.apache.commons#commons-configuration2;2.1.1 from local-m2-cache in [default]\n",
      "\torg.apache.commons#commons-lang3;3.7 from local-m2-cache in [default]\n",
      "\torg.apache.commons#commons-math3;3.1.1 from local-m2-cache in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from local-m2-cache in [default]\n",
      "\torg.apache.commons#commons-text;1.4 from local-m2-cache in [default]\n",
      "\torg.apache.curator#curator-client;4.2.0 from local-m2-cache in [default]\n",
      "\torg.apache.curator#curator-framework;4.2.0 from local-m2-cache in [default]\n",
      "\torg.apache.curator#curator-recipes;4.2.0 from local-m2-cache in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-auth;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from local-m2-cache in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from local-m2-cache in [default]\n",
      "\torg.apache.hadoop#hadoop-common;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-hdfs-client;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-mapreduce-client-common;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-mapreduce-client-core;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-mapreduce-client-jobclient;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-client;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-common;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from local-m2-cache in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from local-m2-cache in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from local-m2-cache in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-admin;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-client;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-common;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-core;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-crypto;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-identity;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-server;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-simplekdc;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerb-util;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerby-asn1;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerby-config;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerby-pkix;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerby-util;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#kerby-xdr;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.kerby#token-provider;1.0.1 from local-m2-cache in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.0 from local-m2-cache in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.0 from local-m2-cache in [default]\n",
      "\torg.apache.yetus#audience-annotations;0.5.0 from local-m2-cache in [default]\n",
      "\torg.apache.zookeeper#zookeeper;3.5.6 from local-m2-cache in [default]\n",
      "\torg.apache.zookeeper#zookeeper-jute;3.5.6 from local-m2-cache in [default]\n",
      "\torg.checkerframework#checker-qual;2.5.2 from local-m2-cache in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from local-m2-cache in [default]\n",
      "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from local-m2-cache in [default]\n",
      "\torg.codehaus.jackson#jackson-xc;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from local-m2-cache in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.17 from local-m2-cache in [default]\n",
      "\torg.codehaus.woodstox#stax2-api;4.2.1 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-client;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-http;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-io;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-security;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-server;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-servlet;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-webapp;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-xml;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty.websocket#websocket-api;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty.websocket#websocket-client;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty.websocket#websocket-common;9.4.40.v20210413 from central in [default]\n",
      "\torg.jline#jline;3.9.0 from local-m2-cache in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from local-m2-cache in [default]\n",
      "\torg.ow2.asm#asm;5.0.4 from local-m2-cache in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from local-m2-cache in [default]\n",
      "\torg.slf4j#slf4j-log4j12;1.7.30 from local-m2-cache in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from local-m2-cache in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from local-m2-cache in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from local-m2-cache in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.2] in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 by [org.xerial.snappy#snappy-java;1.1.8.4] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |  129  |   0   |   0   |   2   ||  127  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: WARNINGS\n",
      "\t\t[NOT FOUND  ] org.apache.commons#commons-math3;3.1.1!commons-math3.jar (3ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/home/ubuntu/.m2/repository/org/apache/commons/commons-math3/3.1.1/commons-math3-3.1.1.jar\n",
      "\n",
      "\t\t[NOT FOUND  ] commons-net#commons-net;3.6!commons-net.jar (1ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/home/ubuntu/.m2/repository/commons-net/commons-net/3.6/commons-net-3.6.jar\n",
      "\n",
      "\t\t[NOT FOUND  ] org.apache.commons#commons-lang3;3.7!commons-lang3.jar (0ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/home/ubuntu/.m2/repository/org/apache/commons/commons-lang3/3.7/commons-lang3-3.7.jar\n",
      "\n",
      "\t\t[NOT FOUND  ] org.apache.commons#commons-text;1.4!commons-text.jar (0ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/home/ubuntu/.m2/repository/org/apache/commons/commons-text/1.4/commons-text-1.4.jar\n",
      "\n",
      "\t\t[NOT FOUND  ] org.apache.avro#avro;1.7.7!avro.jar(bundle) (0ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/home/ubuntu/.m2/repository/org/apache/avro/avro/1.7.7/avro-1.7.7.jar\n",
      "\n",
      "\t\t[NOT FOUND  ] org.apache.curator#curator-recipes;4.2.0!curator-recipes.jar(bundle) (0ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/home/ubuntu/.m2/repository/org/apache/curator/curator-recipes/4.2.0/curator-recipes-4.2.0.jar\n",
      "\n",
      "\t\t[NOT FOUND  ] com.google.code.findbugs#jsr305;3.0.2!jsr305.jar (0ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/home/ubuntu/.m2/repository/com/google/code/findbugs/jsr305/3.0.2/jsr305-3.0.2.jar\n",
      "\n",
      "\t\t[NOT FOUND  ] org.apache.zookeeper#zookeeper;3.5.6!zookeeper.jar (0ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/home/ubuntu/.m2/repository/org/apache/zookeeper/zookeeper/3.5.6/zookeeper-3.5.6.jar\n",
      "\n",
      "\t\t[NOT FOUND  ] org.checkerframework#checker-qual;2.5.2!checker-qual.jar (0ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/home/ubuntu/.m2/repository/org/checkerframework/checker-qual/2.5.2/checker-qual-2.5.2.jar\n",
      "\n",
      "\t\t[NOT FOUND  ] com.thoughtworks.paranamer#paranamer;2.3!paranamer.jar (1ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/home/ubuntu/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar\n",
      "\n",
      "\t\t[NOT FOUND  ] org.apache.curator#curator-framework;4.2.0!curator-framework.jar(bundle) (0ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/home/ubuntu/.m2/repository/org/apache/curator/curator-framework/4.2.0/curator-framework-4.2.0.jar\n",
      "\n",
      "\t\t[NOT FOUND  ] org.apache.zookeeper#zookeeper-jute;3.5.6!zookeeper-jute.jar (1ms)\n",
      "\n",
      "\t==== local-m2-cache: tried\n",
      "\n",
      "\t  file:/home/ubuntu/.m2/repository/org/apache/zookeeper/zookeeper-jute/3.5.6/zookeeper-jute-3.5.6.jar\n",
      "\n",
      "\t\t::::::::::::::::::::::::::::::::::::::::::::::\n",
      "\n",
      "\t\t::              FAILED DOWNLOADS            ::\n",
      "\n",
      "\t\t:: ^ see resolution messages for details  ^ ::\n",
      "\n",
      "\t\t::::::::::::::::::::::::::::::::::::::::::::::\n",
      "\n",
      "\t\t:: com.google.code.findbugs#jsr305;3.0.2!jsr305.jar\n",
      "\n",
      "\t\t:: org.checkerframework#checker-qual;2.5.2!checker-qual.jar\n",
      "\n",
      "\t\t:: org.apache.commons#commons-math3;3.1.1!commons-math3.jar\n",
      "\n",
      "\t\t:: commons-net#commons-net;3.6!commons-net.jar\n",
      "\n",
      "\t\t:: org.apache.commons#commons-lang3;3.7!commons-lang3.jar\n",
      "\n",
      "\t\t:: org.apache.commons#commons-text;1.4!commons-text.jar\n",
      "\n",
      "\t\t:: org.apache.avro#avro;1.7.7!avro.jar(bundle)\n",
      "\n",
      "\t\t:: com.thoughtworks.paranamer#paranamer;2.3!paranamer.jar\n",
      "\n",
      "\t\t:: org.apache.zookeeper#zookeeper;3.5.6!zookeeper.jar\n",
      "\n",
      "\t\t:: org.apache.zookeeper#zookeeper-jute;3.5.6!zookeeper-jute.jar\n",
      "\n",
      "\t\t:: org.apache.curator#curator-framework;4.2.0!curator-framework.jar(bundle)\n",
      "\n",
      "\t\t:: org.apache.curator#curator-recipes;4.2.0!curator-recipes.jar(bundle)\n",
      "\n",
      "\t\t::::::::::::::::::::::::::::::::::::::::::::::\n",
      "\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      "Exception in thread \"main\" java.lang.RuntimeException: [download failed: com.google.code.findbugs#jsr305;3.0.2!jsr305.jar, download failed: org.checkerframework#checker-qual;2.5.2!checker-qual.jar, download failed: org.apache.commons#commons-math3;3.1.1!commons-math3.jar, download failed: commons-net#commons-net;3.6!commons-net.jar, download failed: org.apache.commons#commons-lang3;3.7!commons-lang3.jar, download failed: org.apache.commons#commons-text;1.4!commons-text.jar, download failed: org.apache.avro#avro;1.7.7!avro.jar(bundle), download failed: com.thoughtworks.paranamer#paranamer;2.3!paranamer.jar, download failed: org.apache.zookeeper#zookeeper;3.5.6!zookeeper.jar, download failed: org.apache.zookeeper#zookeeper-jute;3.5.6!zookeeper-jute.jar, download failed: org.apache.curator#curator-framework;4.2.0!curator-framework.jar(bundle), download failed: org.apache.curator#curator-recipes;4.2.0!curator-recipes.jar(bundle)]\n",
      "\tat org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:1447)\n",
      "\tat org.apache.spark.util.DependencyUtils$.resolveMavenDependencies(DependencyUtils.scala:185)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:308)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java gateway process exited before sending its port number\n",
      "local variable 'spark' referenced before assignment\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark, sc = init_spark()\n",
    "    test = spark.read.format(\"delta\").option(\"header\", True).load(\"s3a://reddit-stevenhurwitt/\" + subreddit)\n",
    "    test_pandas = test.toPandas()\n",
    "    print(\"read delta table to df.\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"loading data took too long... cancelled.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>approved_at_utc</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>saved</th>\n",
       "      <th>mod_reason_title</th>\n",
       "      <th>gilded</th>\n",
       "      <th>clicked</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit_name_prefixed</th>\n",
       "      <th>...</th>\n",
       "      <th>author_flair_text_color</th>\n",
       "      <th>permalink</th>\n",
       "      <th>parent_whitelist_status</th>\n",
       "      <th>stickied</th>\n",
       "      <th>url</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>media</th>\n",
       "      <th>is_video</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>technology</td>\n",
       "      <td></td>\n",
       "      <td>t2_hqstdwu0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Elon Musk is using a 'dog ate the homework' ex...</td>\n",
       "      <td>r/technology</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/technology/comments/uqwpba/elon_musk_is_usi...</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>False</td>\n",
       "      <td>https://markets.businessinsider.com/news/stock...</td>\n",
       "      <td>11994694</td>\n",
       "      <td>1.652710e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>technology</td>\n",
       "      <td></td>\n",
       "      <td>t2_7ccf</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Royal Mail drone fleet takes to the air for re...</td>\n",
       "      <td>r/technology</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/technology/comments/uo0r18/royal_mail_drone...</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.edinburghnews.scotsman.com/news/pe...</td>\n",
       "      <td>11965116</td>\n",
       "      <td>1.652361e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>technology</td>\n",
       "      <td></td>\n",
       "      <td>t2_7ccf</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>These Nanobots Can Swim Around a Wound and Kil...</td>\n",
       "      <td>r/technology</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/technology/comments/uow5as/these_nanobots_c...</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.wired.com/story/these-nanobots-can...</td>\n",
       "      <td>11973872</td>\n",
       "      <td>1.652461e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>technology</td>\n",
       "      <td></td>\n",
       "      <td>t2_6utha</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Did Twitch Violate Texas Social Media Law By ...</td>\n",
       "      <td>r/technology</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/technology/comments/ur2490/did_twitch_viola...</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.techdirt.com/2022/05/16/did-twitch...</td>\n",
       "      <td>11996188</td>\n",
       "      <td>1.652724e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>technology</td>\n",
       "      <td></td>\n",
       "      <td>t2_lz41dbnl</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>real. fucking. conservative. civil liberties, ...</td>\n",
       "      <td>r/technology</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/technology/comments/uo8w92/real_fucking_con...</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>False</td>\n",
       "      <td>https://arstechnica.com/tech-policy/2022/05/te...</td>\n",
       "      <td>11967337</td>\n",
       "      <td>1.652383e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>NaN</td>\n",
       "      <td>technology</td>\n",
       "      <td></td>\n",
       "      <td>t2_83wk7s3r</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Novel Diamond Semiconductors Operate at Highes...</td>\n",
       "      <td>r/technology</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/technology/comments/uqwzkm/novel_diamond_se...</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.ad-na.com/magazine_en/archives/44</td>\n",
       "      <td>11994761</td>\n",
       "      <td>1.652711e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>NaN</td>\n",
       "      <td>technology</td>\n",
       "      <td></td>\n",
       "      <td>t2_1argepoh</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Apple iPod creator warns the metaverse will en...</td>\n",
       "      <td>r/technology</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/technology/comments/uo6vg8/apple_ipod_creat...</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.bbc.com/news/business-61423268</td>\n",
       "      <td>11966776</td>\n",
       "      <td>1.652378e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>NaN</td>\n",
       "      <td>technology</td>\n",
       "      <td></td>\n",
       "      <td>t2_a5jwjm1s</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>A colony of blue-green algae can power a compu...</td>\n",
       "      <td>r/technology</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/technology/comments/uo75rs/a_colony_of_blue...</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>False</td>\n",
       "      <td>https://interestingengineering.com/blue-green-...</td>\n",
       "      <td>11966852</td>\n",
       "      <td>1.652379e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>NaN</td>\n",
       "      <td>technology</td>\n",
       "      <td></td>\n",
       "      <td>t2_bf38q9nm</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Musk, Twitter CEO spar over bot accounts</td>\n",
       "      <td>r/technology</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/technology/comments/ur2j9k/musk_twitter_ceo...</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>False</td>\n",
       "      <td>https://thehill.com/policy/technology/</td>\n",
       "      <td>11996262</td>\n",
       "      <td>1.652726e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>NaN</td>\n",
       "      <td>technology</td>\n",
       "      <td></td>\n",
       "      <td>t2_jrya2nvx</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Humans vs. DALLE  Where do human artists fit...</td>\n",
       "      <td>r/technology</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>/r/technology/comments/uow3vs/humans_vs_dalle_...</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>False</td>\n",
       "      <td>https://www.surgehq.ai/blog/humans-vs-dall-e</td>\n",
       "      <td>11973859</td>\n",
       "      <td>1.652461e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78 rows  99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    approved_at_utc   subreddit selftext author_fullname  saved  \\\n",
       "0               NaN  technology              t2_hqstdwu0  False   \n",
       "1               NaN  technology                  t2_7ccf  False   \n",
       "2               NaN  technology                  t2_7ccf  False   \n",
       "3               NaN  technology                 t2_6utha  False   \n",
       "4               NaN  technology              t2_lz41dbnl  False   \n",
       "..              ...         ...      ...             ...    ...   \n",
       "73              NaN  technology              t2_83wk7s3r  False   \n",
       "74              NaN  technology              t2_1argepoh  False   \n",
       "75              NaN  technology              t2_a5jwjm1s  False   \n",
       "76              NaN  technology              t2_bf38q9nm  False   \n",
       "77              NaN  technology              t2_jrya2nvx  False   \n",
       "\n",
       "   mod_reason_title  gilded  clicked  \\\n",
       "0              None       0    False   \n",
       "1              None       0    False   \n",
       "2              None       0    False   \n",
       "3              None       0    False   \n",
       "4              None       0    False   \n",
       "..              ...     ...      ...   \n",
       "73             None       0    False   \n",
       "74             None       0    False   \n",
       "75             None       0    False   \n",
       "76             None       0    False   \n",
       "77             None       0    False   \n",
       "\n",
       "                                                title subreddit_name_prefixed  \\\n",
       "0   Elon Musk is using a 'dog ate the homework' ex...            r/technology   \n",
       "1   Royal Mail drone fleet takes to the air for re...            r/technology   \n",
       "2   These Nanobots Can Swim Around a Wound and Kil...            r/technology   \n",
       "3   Did Twitch Violate Texas Social Media Law By ...            r/technology   \n",
       "4   real. fucking. conservative. civil liberties, ...            r/technology   \n",
       "..                                                ...                     ...   \n",
       "73  Novel Diamond Semiconductors Operate at Highes...            r/technology   \n",
       "74  Apple iPod creator warns the metaverse will en...            r/technology   \n",
       "75  A colony of blue-green algae can power a compu...            r/technology   \n",
       "76           Musk, Twitter CEO spar over bot accounts            r/technology   \n",
       "77  Humans vs. DALLE  Where do human artists fit...            r/technology   \n",
       "\n",
       "    ...  author_flair_text_color  \\\n",
       "0   ...                     None   \n",
       "1   ...                     None   \n",
       "2   ...                     None   \n",
       "3   ...                     None   \n",
       "4   ...                     None   \n",
       "..  ...                      ...   \n",
       "73  ...                     None   \n",
       "74  ...                     None   \n",
       "75  ...                     None   \n",
       "76  ...                     None   \n",
       "77  ...                     None   \n",
       "\n",
       "                                            permalink parent_whitelist_status  \\\n",
       "0   /r/technology/comments/uqwpba/elon_musk_is_usi...                 all_ads   \n",
       "1   /r/technology/comments/uo0r18/royal_mail_drone...                 all_ads   \n",
       "2   /r/technology/comments/uow5as/these_nanobots_c...                 all_ads   \n",
       "3   /r/technology/comments/ur2490/did_twitch_viola...                 all_ads   \n",
       "4   /r/technology/comments/uo8w92/real_fucking_con...                 all_ads   \n",
       "..                                                ...                     ...   \n",
       "73  /r/technology/comments/uqwzkm/novel_diamond_se...                 all_ads   \n",
       "74  /r/technology/comments/uo6vg8/apple_ipod_creat...                 all_ads   \n",
       "75  /r/technology/comments/uo75rs/a_colony_of_blue...                 all_ads   \n",
       "76  /r/technology/comments/ur2j9k/musk_twitter_ceo...                 all_ads   \n",
       "77  /r/technology/comments/uow3vs/humans_vs_dalle_...                 all_ads   \n",
       "\n",
       "    stickied                                                url  \\\n",
       "0      False  https://markets.businessinsider.com/news/stock...   \n",
       "1      False  https://www.edinburghnews.scotsman.com/news/pe...   \n",
       "2      False  https://www.wired.com/story/these-nanobots-can...   \n",
       "3      False  https://www.techdirt.com/2022/05/16/did-twitch...   \n",
       "4      False  https://arstechnica.com/tech-policy/2022/05/te...   \n",
       "..       ...                                                ...   \n",
       "73     False      https://www.ad-na.com/magazine_en/archives/44   \n",
       "74     False         https://www.bbc.com/news/business-61423268   \n",
       "75     False  https://interestingengineering.com/blue-green-...   \n",
       "76     False             https://thehill.com/policy/technology/   \n",
       "77     False       https://www.surgehq.ai/blog/humans-vs-dall-e   \n",
       "\n",
       "   subreddit_subscribers   created_utc num_crossposts  media is_video  \n",
       "0               11994694  1.652710e+09              0   None    False  \n",
       "1               11965116  1.652361e+09              0   None    False  \n",
       "2               11973872  1.652461e+09              0   None    False  \n",
       "3               11996188  1.652724e+09              0   None    False  \n",
       "4               11967337  1.652383e+09              0   None    False  \n",
       "..                   ...           ...            ...    ...      ...  \n",
       "73              11994761  1.652711e+09              0   None    False  \n",
       "74              11966776  1.652378e+09              0   None    False  \n",
       "75              11966852  1.652379e+09              0   None    False  \n",
       "76              11996262  1.652726e+09              0   None    False  \n",
       "77              11973859  1.652461e+09              0   None    False  \n",
       "\n",
       "[78 rows x 99 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark = SparkSession.builder.appName(\"reddit_\" + subreddit) \\\n",
    "                .master(\"spark://localhost:7077\") \\\n",
    "                .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0,org.apache.hadoop:hadoop-common:3.3.1,org.apache.hadoop:hadoop-aws:3.3.1,org.apache.hadoop:hadoop-client:3.3.1,io.delta:delta-core_2.12:1.2.1\") \\\n",
    "                .config(\"spark.hadoop.fs.s3a.access.key\", aws_client) \\\n",
    "                .config(\"spark.hadoop.fs.s3a.secret.key\", aws_secret) \\\n",
    "                .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "                .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "                .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "                .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "                .enableHiveSupport() \\\n",
    "                .getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # .master(\"spark://{}:7077\".format(spark_host)) \\\n",
    "    # .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "    # .config(\"spark.eventLog.dir\", \"file:///opt/workspace/events\") \\\n",
    "\n",
    "    sc.setLogLevel('WARN')\n",
    "    # sc._jsc.hadoopConfiguration().set(\"fs.s3a.awsAccessKeyId\", aws_client)\n",
    "    # sc._jsc.hadoopConfiguration().set(\"fs.s3a.awsSecretAccessKey\", aws_secret)\n",
    "    # sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.us-east-2.amazonaws.com\")\n",
    "    print(\"created spark successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_df = read_kafka_stream(spark, sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/19 15:03:21 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-3078029a-9880-4986-9050-c88d9fcc246c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/05/19 15:03:21 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9359/1914995332.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"update\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"console\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"truncate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1306\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stage_df.writeStream \\\n",
    "        .trigger(processingTime='30 seconds') \\\n",
    "        .outputMode(\"update\") \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", \"true\") \\\n",
    "        .start() \\\n",
    "        .awaitTermination()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/19 14:56:43 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-8f7ad4d0-1dbe-4b97-b868-8841064b10e5. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/05/19 14:56:43 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopped streaming.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    write_stream(stage_df)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"stopped streaming.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9359/80116066.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stopped spark successfully.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m         \"\"\"\n\u001b[1;32m    797\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m         \u001b[0;31m# We should clean the default session up. See SPARK-23228.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclearDefaultSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_jsc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m                 \u001b[0;31m# Case: SPARK-18523\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1306\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"stopped spark successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
