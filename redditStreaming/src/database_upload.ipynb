{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# database upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported modules.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import psycopg2\n",
    "import datetime as dt\n",
    "from delta import *\n",
    "import boto3\n",
    "import pprint\n",
    "import yaml\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "import ast\n",
    "import os\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent = 1)\n",
    "print(\"imported modules.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read creds.json.\n",
      "read creds successfully.\n"
     ]
    }
   ],
   "source": [
    "creds_path = os.path.join(\"/opt\", \"workspace\", \"redditStreaming\", \"creds.json\")\n",
    "\n",
    "try:\n",
    "    with open(creds_path, \"r\") as f:\n",
    "        creds = json.load(f)\n",
    "        print(\"read creds.json.\")\n",
    "        f.close()\n",
    "\n",
    "except:\n",
    "    creds_path = \"/home/steven/Documents/reddit-streaming/redditStreaming/creds.json\"\n",
    "    with open(creds_path, \"r\") as f:\n",
    "        creds = json.load(f)\n",
    "        print(\"read creds.json.\")\n",
    "        f.close()\n",
    "\n",
    "print(\"read creds successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/12 01:59:10 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "created spark successfully\n"
     ]
    }
   ],
   "source": [
    "spark_host = \"spark-master\"\n",
    "# spark_host = \"spark-master\"\n",
    "aws_client = creds[\"aws_client\"]\n",
    "aws_secret = creds[\"aws_secret\"]\n",
    "index = 0\n",
    "subreddit = \"technology\"\n",
    "\n",
    "# initialize spark session\n",
    "try:\n",
    "    spark = SparkSession.builder.appName(\"reddit_{}\".format(subreddit)) \\\n",
    "                .master(\"spark://{}:7077\".format(spark_host)) \\\n",
    "                .config(\"spark.scheduler.mode\", \"FAIR\") \\\n",
    "                .config(\"spark.scheduler.allocation.file\", \"file:///opt/workspace/redditStreaming/fairscheduler.xml\") \\\n",
    "                .config(\"spark.executor.memory\", \"4096m\") \\\n",
    "                .config(\"spark.executor.cores\", \"4\") \\\n",
    "                .config(\"spark.local.dir\", \"/opt/workspace/tmp/driver/{}/\".format(subreddit)) \\\n",
    "                .config(\"spark.worker.dir\", \"/opt/workspace/tmp/executor/{}/\".format(subreddit)) \\\n",
    "                .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "                .config(\"spark.eventLog.dir\", \"file:///opt/workspace/events/{}/\".format(subreddit)) \\\n",
    "                .config(\"spark.sql.debug.maxToStringFields\", 1000) \\\n",
    "                .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,org.apache.hadoop:hadoop-common:3.3.1,org.apache.hadoop:hadoop-aws:3.3.1,org.apache.hadoop:hadoop-client:3.3.1,io.delta:delta-core_2.12:1.2.1,org.postgresql:postgresql:42.5.0\") \\\n",
    "                .config(\"spark.hadoop.fs.s3a.access.key\", aws_client) \\\n",
    "                .config(\"spark.hadoop.fs.s3a.secret.key\", aws_secret) \\\n",
    "                .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "                .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "                .config('spark.hadoop.fs.s3a.buffer.dir', '/opt/workspace/tmp/blocks') \\\n",
    "                .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "                .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "                .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "                .enableHiveSupport() \\\n",
    "                .getOrCreate()\n",
    "\n",
    "    sc = spark.sparkContext\n",
    "    # .config('spark.hadoop.fs.s3a.fast.upload.buffer', 'bytebuffer') \\\n",
    "\n",
    "    sc.setLogLevel('WARN')\n",
    "    sc.setLocalProperty(\"spark.scheduler.pool\", \"pool{}\".format(str(index)))\n",
    "    # sc._jsc.hadoopConfiguration().set(\"fs.s3a.awsAccessKeyId\", aws_client)\n",
    "    # sc._jsc.hadoopConfiguration().set(\"fs.s3a.awsSecretAccessKey\", aws_secret)\n",
    "    # sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.us-east-2.amazonaws.com\")\n",
    "    print(\"created spark successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read clean df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_spark_jdbc(subreddit):\n",
    "\n",
    "    creds_path = os.path.join(\"/opt\", \"workspace\", \"redditStreaming\", \"creds.json\")\n",
    "\n",
    "    try:\n",
    "        with open(creds_path, \"r\") as f:\n",
    "            creds = json.load(f)\n",
    "            print(\"read creds.json.\")\n",
    "            f.close()\n",
    "\n",
    "    except:\n",
    "        creds_path = \"/home/steven/Documents/reddit-streaming/redditStreaming/creds.json\"\n",
    "        with open(creds_path, \"r\") as f:\n",
    "            creds = json.load(f)\n",
    "            print(\"read creds.json.\")\n",
    "            f.close()\n",
    "\n",
    "    secretmanager_client = boto3.client(\"secretsmanager\", \n",
    "                                    region_name = \"us-east-2\", \n",
    "                                    aws_access_key_id = creds[\"aws_client\"], \n",
    "                                    aws_secret_access_key = creds[\"aws_secret\"])\n",
    "    \n",
    "    df = spark.read.format(\"delta\").option(\"header\", True).load(\"s3a://reddit-streaming-stevenhurwitt/\" + subreddit + \"_clean\")\n",
    "\n",
    "    db_creds = ast.literal_eval(secretmanager_client.get_secret_value(SecretId=\"dev/reddit/postgres\")[\"SecretString\"])\n",
    "    connect_str = \"jdbc:postgresql://{}:5432/reddit\".format(db_creds[\"host\"])\n",
    "\n",
    "    try:\n",
    "        df.write.format(\"jdbc\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"url\", connect_str) \\\n",
    "            .option(\"dbtable\", \"reddit.{}\".format(subreddit)) \\\n",
    "            .option(\"user\", db_creds[\"username\"]) \\\n",
    "            .option(\"password\", db_creds[\"password\"]) \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .save()\n",
    "\n",
    "        print(\"wrote df to postgresql table.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\") as g:\n",
    "    config = yaml.safe_load(g)\n",
    "    g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['technology',\n",
       " 'ProgrammerHumor',\n",
       " 'news',\n",
       " 'worldnews',\n",
       " 'BikiniBottomTwitter',\n",
       " 'BlackPeopleTwitter',\n",
       " 'WhitePeopleTwitter',\n",
       " 'aws']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"subreddit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subreddit: technology\n",
      "read creds.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote df to postgresql table.\n",
      "subreddit: ProgrammerHumor\n",
      "read creds.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote df to postgresql table.\n",
      "subreddit: news\n",
      "read creds.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote df to postgresql table.\n",
      "subreddit: worldnews\n",
      "read creds.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote df to postgresql table.\n",
      "subreddit: BikiniBottomTwitter\n",
      "read creds.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote df to postgresql table.\n",
      "subreddit: BlackPeopleTwitter\n",
      "read creds.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote df to postgresql table.\n",
      "subreddit: WhitePeopleTwitter\n",
      "read creds.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote df to postgresql table.\n",
      "subreddit: aws\n",
      "read creds.json.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote df to postgresql table.\n"
     ]
    }
   ],
   "source": [
    "for s in config[\"subreddit\"]:\n",
    "    print(\"subreddit: {}\".format(s))\n",
    "    write_spark_jdbc(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write to postgres table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\", \"r\") as g:\n",
    "    config = yaml.safe_load(g)\n",
    "    g.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host = config[\"postgres_host\"], user = config[\"postgres_user\"], password = config[\"postgres_password\"], database=\"postgres\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote df to postgresql table.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "connect_str = \"jdbc:postgresql://{}:5432/postgres\".format(config[\"postgres_host\"])\n",
    "\n",
    "try:\n",
    "    df.write.format(\"jdbc\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"url\", connect_str) \\\n",
    "        .option(\"dbtable\", \"public.{}\".format(subreddit)) \\\n",
    "        .option(\"user\", config[\"postgres_user\"]) \\\n",
    "        .option(\"password\", config[\"postgres_password\"]) \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .save()\n",
    "\n",
    "    print(\"wrote df to postgresql table.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stop spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession does not exist in the JVM\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.stop()\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
