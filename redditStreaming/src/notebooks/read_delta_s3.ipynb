{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9dc4867",
   "metadata": {},
   "source": [
    "# Read Delta Tables from S3\n",
    "\n",
    "This notebook reads reddit data stored in Delta format on S3, unions the data from multiple subreddits, and sorts by most recent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /opt/workspace\n",
      "Found credentials at: redditStreaming/creds.json\n",
      "Found config at: redditStreaming/config.yaml\n",
      "Subreddits to process: ['technology', 'ProgrammerHumor', 'news', 'worldnews']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_unixtime\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Function to read credentials and config\n",
    "def find_file(filename, search_paths):\n",
    "    # Check specific paths first\n",
    "    for path in search_paths:\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "    \n",
    "    # Walk up from CWD\n",
    "    curr = os.getcwd()\n",
    "    while True:\n",
    "        f = os.path.join(curr, filename)\n",
    "        if os.path.exists(f):\n",
    "            return f\n",
    "        parent = os.path.dirname(curr)\n",
    "        if parent == curr:\n",
    "            break\n",
    "        curr = parent\n",
    "    return None\n",
    "\n",
    "def read_config_creds():\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "    \n",
    "    # Creds paths to search\n",
    "    creds_paths = [\n",
    "        \"creds.json\",\n",
    "        \"redditStreaming/creds.json\",\n",
    "        \"/home/steven/reddit-streaming/creds.json\",\n",
    "        \"/opt/workspace/creds.json\",\n",
    "        \"/opt/workspace/redditStreaming/creds.json\"\n",
    "    ]\n",
    "    \n",
    "    creds_file = find_file(\"creds.json\", creds_paths)\n",
    "    if not creds_file:\n",
    "         raise FileNotFoundError(\"Could not find creds.json in search paths or parent directories.\")\n",
    "    \n",
    "    print(f\"Found credentials at: {creds_file}\")\n",
    "    with open(creds_file, \"r\") as f:\n",
    "        creds = json.load(f)\n",
    "\n",
    "    # Config paths to search\n",
    "    config_paths = [\n",
    "        \"config.yaml\",\n",
    "        \"redditStreaming/config.yaml\",\n",
    "        \"/home/steven/reddit-streaming/config.yaml\",\n",
    "        \"/home/steven/reddit-streaming/redditStreaming/config.yaml\",\n",
    "        \"/opt/workspace/redditStreaming/config.yaml\"\n",
    "    ]\n",
    "    \n",
    "    config_file = find_file(\"config.yaml\", config_paths)\n",
    "    if not config_file:\n",
    "         print(\"Warning: Could not find config.yaml, using defaults or trying alternate locations.\")\n",
    "         # Fallback search if needed, but for now we raise error if strictly required\n",
    "         # In the original code config was critical for bucket/spark host, but here we need subreddits.\n",
    "         pass\n",
    "         \n",
    "    if config_file:\n",
    "        print(f\"Found config at: {config_file}\")\n",
    "        with open(config_file, \"r\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "    else:\n",
    "        # Fallback config if file missing\n",
    "        config = {\"subreddit\": [\"technology\", \"ProgrammerHumor\", \"news\", \"worldnews\"]}\n",
    "        print(\"Using default configuration for subreddits.\")\n",
    "        \n",
    "    return creds, config\n",
    "\n",
    "creds, config = read_config_creds()\n",
    "\n",
    "# Extract values\n",
    "aws_client = creds.get(\"aws_client\")\n",
    "aws_secret = creds.get(\"aws_secret\")\n",
    "subreddits = config.get(\"subreddit\", [\"technology\", \"ProgrammerHumor\", \"news\", \"worldnews\"])\n",
    "bucket_name = \"reddit-streaming-stevenhurwitt-2\" \n",
    "\n",
    "print(f\"Subreddits to process: {subreddits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1746c5",
   "metadata": {},
   "source": [
    "## start spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "296bd736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session created.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session with Delta and S3 support\n",
    "# Using the same jars as in the main application\n",
    "extra_jar_list = \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3,org.apache.hadoop:hadoop-common:3.3.1,org.apache.hadoop:hadoop-aws:3.3.1,org.apache.hadoop:hadoop-client:3.3.1,io.delta:delta-spark_2.12:3.2.0,org.postgresql:postgresql:42.5.0\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RedditDeltaRead\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", extra_jar_list) \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", aws_client) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", aws_secret) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"Spark Session created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8317ec9e",
   "metadata": {},
   "source": [
    "## read data from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c42bf89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read from: s3a://reddit-streaming-stevenhurwitt-2/technology\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read data for technology. Count: 1841\n",
      "Attempting to read from: s3a://reddit-streaming-stevenhurwitt-2/ProgrammerHumor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read data for ProgrammerHumor. Count: 1796\n",
      "Attempting to read from: s3a://reddit-streaming-stevenhurwitt-2/news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read data for news. Count: 1203\n",
      "Attempting to read from: s3a://reddit-streaming-stevenhurwitt-2/worldnews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 331:===================================================>   (47 + 3) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read data for worldnews. Count: 2266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read Delta tables for each subreddit\n",
    "dfs = []\n",
    "\n",
    "for sub in subreddits:\n",
    "    path = f\"s3a://{bucket_name}/{sub}\"\n",
    "    print(f\"Attempting to read from: {path}\")\n",
    "    try:\n",
    "        # Read Delta table\n",
    "        df = spark.read.format(\"delta\").load(path)\n",
    "        dfs.append(df)\n",
    "        print(f\"Successfully read data for {sub}. Count: {df.count()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {sub} (might not exist yet): {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08400cd",
   "metadata": {},
   "source": [
    "## union and sort dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5360ea9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Union all dataframes and sort\n",
    "if dfs:\n",
    "    # Union all dataframes found\n",
    "    # takes 10 min\n",
    "    full_df = reduce(DataFrame.unionAll, dfs)\n",
    "    \n",
    "    # Sort by created_utc (most recent first)\n",
    "    # created_utc is likely a float timestamp\n",
    "    sorted_df = full_df.orderBy(col(\"created_utc\").desc())\n",
    "    \n",
    "    # Show result (selecting a few relevant columns)\n",
    "    # takes 10 min\n",
    "    display_df = sorted_df.withColumn(\"created_time\", from_unixtime(\"created_utc\")) \\\n",
    "                          .select(\"subreddit\", \"created_time\", \"title\", \"score\", \"author\")\n",
    "    \n",
    "    print(f\"Total records: {sorted_df.count()}\")\n",
    "    display_df.show(100, truncate=False)\n",
    "else:\n",
    "    print(\"No data available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a364e870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
