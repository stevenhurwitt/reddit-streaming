{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5360ea9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:=================================>                    (124 + 4) / 201]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 6437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 6437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 80:=====================================================>(200 + 1) / 201]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 6437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 80:=====================================================>(200 + 1) / 201]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+--------------------+\n",
      "|subreddit      |created_time       |title                                                                                                                                                                                                              |score|author              |\n",
      "+---------------+-------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+--------------------+\n",
      "|technology     |2026-01-20 21:13:36|Affordable London to New York flights in under four hours: Boom Supersonic outlines Concorde-esque ambitions                                                                                                       |1    |Disastrous_Award_789|\n",
      "|worldnews      |2026-01-20 21:05:04|Dow tumbles more than 850 points and dollar slides over Greenland and tariff threats                                                                                                                               |1    |StevenSanders90210  |\n",
      "|ProgrammerHumor|2026-01-20 21:05:04|chooseYourDrug                                                                                                                                                                                                     |1    |AndreaBroccoli      |\n",
      "|technology     |2026-01-20 20:58:40|30 safety features every senior should know (includes phone, internet/cybersecurity, accessibility and device usability, caregiver and emergency response)                                                         |1    |countdookee         |\n",
      "|worldnews      |2026-01-20 20:52:16|Era of ‘global water bankruptcy’ is here, UN report says                                                                                                                                                           |1    |Dr_Neurol           |\n",
      "|worldnews      |2026-01-20 20:48:00|Canada's Carney aims to lead new global trading order less reliant on US                                                                                                                                           |5    |joe4942             |\n",
      "|technology     |2026-01-20 20:41:36|Christmas 2025 Was the Biggest Streaming Day Ever in the United States, Says Nielsen                                                                                                                               |1    |bummed_athlete      |\n",
      "|worldnews      |2026-01-20 20:37:20|British prison officer jailed after having sex with inmate in 'chapel area'                                                                                                                                        |1    |Ill-Pressure-9181   |\n",
      "|news           |2026-01-20 20:26:40|New Hampshire bishop warns clergy to prepare for 'new era of martyrdom'                                                                                                                                            |3    |graveyardofgoodsense|\n",
      "|technology     |2026-01-20 20:24:32|Netflix to pay all cash for Warner Bros. to fend off Paramount hostile takeover | Netflix and Warner seek quick shareholder vote as Paramount tries to upend deal.                                                 |5    |ControlCAD          |\n",
      "|technology     |2026-01-20 20:24:32|Netflix to pay all cash for Warner Bros. to fend off Paramount hostile takeover | Netflix and Warner seek quick shareholder vote as Paramount tries to upend deal.                                                 |60   |ControlCAD          |\n",
      "|worldnews      |2026-01-20 20:22:24|North Korea’s Kim Jong Un fires vice premier, publicly rebukes officials | Kim Jong Un News | Al Jazeera                                                                                                           |3    |Evening_Lawyer6570  |\n",
      "|worldnews      |2026-01-20 20:20:16|European leaders have vowed to hit back at the US if Donald Trump goes ahead with his threat to impose a 10% tariff to force the sale of Greenland. Denmark's prime minister said sovereignty was not up for grabs.|7    |DouglasDriveN       |\n",
      "|news           |2026-01-20 20:18:08|Texas to defend law requiring Ten Commandments in classrooms                                                                                                                                                       |2    |FakeOkie            |\n",
      "|news           |2026-01-20 20:18:08|Texas to defend law requiring Ten Commandments in classrooms                                                                                                                                                       |117  |FakeOkie            |\n",
      "|worldnews      |2026-01-20 20:11:44|Carney says the old world order 'is not coming back' in Davos speech                                                                                                                                               |2    |GoldernEmerald      |\n",
      "|news           |2026-01-20 20:05:20|Looming water supply 'bankruptcy' puts billions at risk, UN report warns                                                                                                                                           |1    |FakeOkie            |\n",
      "|worldnews      |2026-01-20 19:33:20|From 'grievances' to 'terrorism’: how Tehran reframed dissent                                                                                                                                                      |2    |Christian-Rep-Perisa|\n",
      "|news           |2026-01-20 19:31:12|Cow astonishes scientists with ability to use tools                                                                                                                                                                |3    |kiyomoris           |\n",
      "|news           |2026-01-20 19:29:04|Israel demolishes structures inside UNRWA compound                                                                                                                                                                 |5    |Tris_Memba          |\n",
      "+---------------+-------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Union all dataframes and sort\n",
    "if dfs:\n",
    "    # Union all dataframes found\n",
    "    full_df = reduce(DataFrame.unionAll, dfs)\n",
    "    \n",
    "    # Sort by created_utc (most recent first)\n",
    "    # created_utc is likely a float timestamp\n",
    "    sorted_df = full_df.orderBy(col(\"created_utc\").desc())\n",
    "    \n",
    "    # Show result (selecting a few relevant columns)\n",
    "    display_df = sorted_df.withColumn(\"created_time\", from_unixtime(\"created_utc\")) \\\n",
    "                          .select(\"subreddit\", \"created_time\", \"title\", \"score\", \"author\")\n",
    "    \n",
    "    print(f\"Total records: {sorted_df.count()}\")\n",
    "    display_df.show(20, truncate=False)\n",
    "else:\n",
    "    print(\"No data available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e7621d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to read from: s3a://reddit-streaming-stevenhurwitt-2/technology\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/20 21:07:44 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "26/01/20 21:08:22 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read data for technology. Count: 1668\n",
      "Attempting to read from: s3a://reddit-streaming-stevenhurwitt-2/ProgrammerHumor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read data for ProgrammerHumor. Count: 1724\n",
      "Attempting to read from: s3a://reddit-streaming-stevenhurwitt-2/news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read data for news. Count: 1059\n",
      "Attempting to read from: s3a://reddit-streaming-stevenhurwitt-2/worldnews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:=====================================================>  (48 + 2) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read data for worldnews. Count: 1985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read Delta tables for each subreddit\n",
    "dfs = []\n",
    "\n",
    "for sub in subreddits:\n",
    "    path = f\"s3a://{bucket_name}/{sub}\"\n",
    "    print(f\"Attempting to read from: {path}\")\n",
    "    try:\n",
    "        # Read Delta table\n",
    "        df = spark.read.format(\"delta\").load(path)\n",
    "        dfs.append(df)\n",
    "        print(f\"Successfully read data for {sub}. Count: {df.count()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {sub} (might not exist yet): {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e0bdbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/venv/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-common added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.apache.hadoop#hadoop-client added as a dependency\n",
      "io.delta#delta-spark_2.12 added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-edd18444-00cf-4add-b061-d4a76a4402d2;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.3 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.3 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-common;3.3.1 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.3.1 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound com.google.guava#guava;27.0-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound org.checkerframework#checker-qual;2.5.2 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.17 in central\n",
      "\tfound commons-cli#commons-cli;1.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.1.1 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-codec#commons-codec;1.11 in central\n",
      "\tfound commons-io#commons-io;2.8.0 in central\n",
      "\tfound commons-net#commons-net;3.6 in central\n",
      "\tfound commons-collections#commons-collections;3.2.2 in central\n",
      "\tfound javax.servlet#javax.servlet-api;3.1.0 in central\n",
      "\tfound org.eclipse.jetty#jetty-server;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-http;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-io;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-servlet;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-security;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-webapp;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-xml;9.4.40.v20210413 in central\n",
      "\tfound com.sun.jersey#jersey-core;1.19 in central\n",
      "\tfound javax.ws.rs#jsr311-api;1.1.1 in central\n",
      "\tfound com.sun.jersey#jersey-servlet;1.19 in central\n",
      "\tfound com.sun.jersey#jersey-server;1.19 in central\n",
      "\tfound com.sun.jersey#jersey-json;1.19 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.11 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in central\n",
      "\tfound log4j#log4j;1.2.17 in central\n",
      "\tfound commons-beanutils#commons-beanutils;1.9.4 in central\n",
      "\tfound org.apache.commons#commons-configuration2;2.1.1 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.7 in central\n",
      "\tfound org.apache.commons#commons-text;1.4 in central\n",
      "\tfound org.slf4j#slf4j-log4j12;1.7.30 in central\n",
      "\tfound org.apache.avro#avro;1.7.7 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\n",
      "\tfound org.apache.commons#commons-compress;1.19 in central\n",
      "\tfound com.google.re2j#re2j;1.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound com.google.code.gson#gson;2.2.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-auth;3.3.1 in central\n",
      "\tfound com.nimbusds#nimbus-jose-jwt;9.8.1 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound net.minidev#json-smart;2.4.2 in central\n",
      "\tfound net.minidev#accessors-smart;2.4.2 in central\n",
      "\tfound org.ow2.asm#asm;5.0.4 in central\n",
      "\tfound org.apache.zookeeper#zookeeper;3.5.6 in central\n",
      "\tfound org.apache.zookeeper#zookeeper-jute;3.5.6 in central\n",
      "\tfound org.apache.yetus#audience-annotations;0.5.0 in central\n",
      "\tfound org.apache.curator#curator-framework;4.2.0 in central\n",
      "\tfound org.apache.curator#curator-client;4.2.0 in central\n",
      "\tfound org.apache.kerby#kerb-simplekdc;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-client;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-config;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-core;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-pkix;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-asn1;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-common;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-crypto;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#token-provider;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-admin;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-server;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-identity;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-xdr;1.0.1 in central\n",
      "\tfound com.jcraft#jsch;0.1.55 in central\n",
      "\tfound org.apache.curator#curator-recipes;4.2.0 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.10.5.1 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.10.5 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.10.5 in central\n",
      "\tfound org.codehaus.woodstox#stax2-api;4.2.1 in central\n",
      "\tfound com.fasterxml.woodstox#woodstox-core;5.3.0 in central\n",
      "\tfound dnsjava#dnsjava;2.1.7 in central\n",
      "\tfound jakarta.activation#jakarta.activation-api;1.2.1 in central\n",
      "\tfound javax.servlet.jsp#jsp-api;2.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.apache.hadoop#hadoop-client;3.3.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-hdfs-client;3.3.1 in central\n",
      "\tfound com.squareup.okhttp#okhttp;2.7.5 in central\n",
      "\tfound com.squareup.okio#okio;1.6.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-api;3.3.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-client;3.3.1 in central\n",
      "\tfound org.eclipse.jetty.websocket#websocket-client;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-client;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty.websocket#websocket-common;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty.websocket#websocket-api;9.4.40.v20210413 in central\n",
      "\tfound org.jline#jline;3.9.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-mapreduce-client-core;3.3.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-yarn-common;3.3.1 in central\n",
      "\tfound com.sun.jersey#jersey-client;1.19 in central\n",
      "\tfound com.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.10.5 in central\n",
      "\tfound jakarta.xml.bind#jakarta.xml.bind-api;2.3.2 in central\n",
      "\tfound com.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.10.5 in central\n",
      "\tfound com.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.10.5 in central\n",
      "\tfound org.apache.hadoop#hadoop-mapreduce-client-jobclient;3.3.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-mapreduce-client-common;3.3.1 in central\n",
      "\tfound io.delta#delta-spark_2.12;3.2.0 in central\n",
      "\tfound io.delta#delta-storage;3.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.postgresql#postgresql;42.5.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.5.0 in central\n",
      ":: resolution report :: resolve 12494ms :: artifacts dl 423ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.10.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.10.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.10.5.1 from central in [default]\n",
      "\tcom.fasterxml.jackson.jaxrs#jackson-jaxrs-base;2.10.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.jaxrs#jackson-jaxrs-json-provider;2.10.5 from central in [default]\n",
      "\tcom.fasterxml.jackson.module#jackson-module-jaxb-annotations;2.10.5 from central in [default]\n",
      "\tcom.fasterxml.woodstox#woodstox-core;5.3.0 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.2.4 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0 from central in [default]\n",
      "\tcom.google.guava#guava;27.0-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.1 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.55 from central in [default]\n",
      "\tcom.nimbusds#nimbus-jose-jwt;9.8.1 from central in [default]\n",
      "\tcom.squareup.okhttp#okhttp;2.7.5 from central in [default]\n",
      "\tcom.squareup.okio#okio;1.6.0 from central in [default]\n",
      "\tcom.sun.jersey#jersey-client;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-core;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-json;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-server;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-servlet;1.19 from central in [default]\n",
      "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.3 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.9.4 from central in [default]\n",
      "\tcommons-cli#commons-cli;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.11 from central in [default]\n",
      "\tcommons-collections#commons-collections;3.2.2 from central in [default]\n",
      "\tcommons-io#commons-io;2.8.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tcommons-net#commons-net;3.6 from central in [default]\n",
      "\tdnsjava#dnsjava;2.1.7 from central in [default]\n",
      "\tio.delta#delta-spark_2.12;3.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;3.2.0 from central in [default]\n",
      "\tjakarta.activation#jakarta.activation-api;1.2.1 from central in [default]\n",
      "\tjakarta.xml.bind#jakarta.xml.bind-api;2.3.2 from central in [default]\n",
      "\tjavax.servlet#javax.servlet-api;3.1.0 from central in [default]\n",
      "\tjavax.servlet.jsp#jsp-api;2.1 from central in [default]\n",
      "\tjavax.ws.rs#jsr311-api;1.1.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.11 from central in [default]\n",
      "\tlog4j#log4j;1.2.17 from central in [default]\n",
      "\tnet.minidev#accessors-smart;2.4.2 from central in [default]\n",
      "\tnet.minidev#json-smart;2.4.2 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.avro#avro;1.7.7 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.19 from central in [default]\n",
      "\torg.apache.commons#commons-configuration2;2.1.1 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.7 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.1.1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.commons#commons-text;1.4 from central in [default]\n",
      "\torg.apache.curator#curator-client;4.2.0 from central in [default]\n",
      "\torg.apache.curator#curator-framework;4.2.0 from central in [default]\n",
      "\torg.apache.curator#curator-recipes;4.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-auth;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-common;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-hdfs-client;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-mapreduce-client-common;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-mapreduce-client-core;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-mapreduce-client-jobclient;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-client;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-yarn-common;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-admin;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-client;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-common;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-core;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-crypto;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-identity;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-server;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-simplekdc;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-asn1;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-config;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-pkix;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-xdr;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#token-provider;1.0.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.3 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.3 from central in [default]\n",
      "\torg.apache.yetus#audience-annotations;0.5.0 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper;3.5.6 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper-jute;3.5.6 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.5.0 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-xc;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.17 from central in [default]\n",
      "\torg.codehaus.woodstox#stax2-api;4.2.1 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-client;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-http;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-io;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-security;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-server;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-servlet;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-webapp;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-xml;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty.websocket#websocket-api;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty.websocket#websocket-client;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty.websocket#websocket-common;9.4.40.v20210413 from central in [default]\n",
      "\torg.jline#jline;3.9.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.ow2.asm#asm;5.0.4 from central in [default]\n",
      "\torg.postgresql#postgresql;42.5.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.slf4j#slf4j-log4j12;1.7.30 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.2] in [default]\n",
      "\torg.checkerframework#checker-qual;2.5.2 by [org.checkerframework#checker-qual;3.5.0] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 by [org.slf4j#slf4j-api;2.0.7] in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 by [org.xerial.snappy#snappy-java;1.1.10.5] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |  131  |   0   |   0   |   4   ||  127  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-edd18444-00cf-4add-b061-d4a76a4402d2\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 127 already retrieved (0kB/186ms)\n",
      "26/01/20 21:06:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/20 21:06:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "26/01/20 21:07:10 ERROR Inbox: Ignoring error\n",
      "java.lang.NullPointerException: Cannot invoke \"org.apache.spark.storage.BlockManagerId.executorId()\" because \"idWithoutTopologyInfo\" is null\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:677)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "26/01/20 21:07:10 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.storage.BlockManagerId.executorId()\" because \"idWithoutTopologyInfo\" is null\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:677)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n",
      "26/01/20 21:07:19 ERROR Inbox: Ignoring error\n",
      "java.lang.NullPointerException: Cannot invoke \"org.apache.spark.storage.BlockManagerId.executorId()\" because \"idWithoutTopologyInfo\" is null\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:677)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "26/01/20 21:07:19 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.lang.NullPointerException: Cannot invoke \"org.apache.spark.storage.BlockManagerId.executorId()\" because \"idWithoutTopologyInfo\" is null\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:677)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\t... 3 more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session created.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark Session with Delta and S3 support\n",
    "# Using the same jars as in the main application\n",
    "extra_jar_list = \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3,org.apache.hadoop:hadoop-common:3.3.1,org.apache.hadoop:hadoop-aws:3.3.1,org.apache.hadoop:hadoop-client:3.3.1,io.delta:delta-spark_2.12:3.2.0,org.postgresql:postgresql:42.5.0\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RedditDeltaRead\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", extra_jar_list) \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", aws_client) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", aws_secret) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"Spark Session created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "129a3e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /opt/workspace\n",
      "Found credentials at: redditStreaming/creds.json\n",
      "Found config at: redditStreaming/config.yaml\n",
      "Subreddits to process: ['technology', 'ProgrammerHumor', 'news', 'worldnews']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_unixtime\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Function to read credentials and config\n",
    "def find_file(filename, search_paths):\n",
    "    # Check specific paths first\n",
    "    for path in search_paths:\n",
    "        if os.path.exists(path):\n",
    "            return path\n",
    "    \n",
    "    # Walk up from CWD\n",
    "    curr = os.getcwd()\n",
    "    while True:\n",
    "        f = os.path.join(curr, filename)\n",
    "        if os.path.exists(f):\n",
    "            return f\n",
    "        parent = os.path.dirname(curr)\n",
    "        if parent == curr:\n",
    "            break\n",
    "        curr = parent\n",
    "    return None\n",
    "\n",
    "def read_config_creds():\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "    \n",
    "    # Creds paths to search\n",
    "    creds_paths = [\n",
    "        \"creds.json\",\n",
    "        \"redditStreaming/creds.json\",\n",
    "        \"/home/steven/reddit-streaming/creds.json\",\n",
    "        \"/opt/workspace/creds.json\",\n",
    "        \"/opt/workspace/redditStreaming/creds.json\"\n",
    "    ]\n",
    "    \n",
    "    creds_file = find_file(\"creds.json\", creds_paths)\n",
    "    if not creds_file:\n",
    "         raise FileNotFoundError(\"Could not find creds.json in search paths or parent directories.\")\n",
    "    \n",
    "    print(f\"Found credentials at: {creds_file}\")\n",
    "    with open(creds_file, \"r\") as f:\n",
    "        creds = json.load(f)\n",
    "\n",
    "    # Config paths to search\n",
    "    config_paths = [\n",
    "        \"config.yaml\",\n",
    "        \"redditStreaming/config.yaml\",\n",
    "        \"/home/steven/reddit-streaming/config.yaml\",\n",
    "        \"/home/steven/reddit-streaming/redditStreaming/config.yaml\",\n",
    "        \"/opt/workspace/redditStreaming/config.yaml\"\n",
    "    ]\n",
    "    \n",
    "    config_file = find_file(\"config.yaml\", config_paths)\n",
    "    if not config_file:\n",
    "         print(\"Warning: Could not find config.yaml, using defaults or trying alternate locations.\")\n",
    "         # Fallback search if needed, but for now we raise error if strictly required\n",
    "         # In the original code config was critical for bucket/spark host, but here we need subreddits.\n",
    "         pass\n",
    "         \n",
    "    if config_file:\n",
    "        print(f\"Found config at: {config_file}\")\n",
    "        with open(config_file, \"r\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "    else:\n",
    "        # Fallback config if file missing\n",
    "        config = {\"subreddit\": [\"technology\", \"ProgrammerHumor\", \"news\", \"worldnews\"]}\n",
    "        print(\"Using default configuration for subreddits.\")\n",
    "        \n",
    "    return creds, config\n",
    "\n",
    "creds, config = read_config_creds()\n",
    "\n",
    "# Extract values\n",
    "aws_client = creds.get(\"aws_client\")\n",
    "aws_secret = creds.get(\"aws_secret\")\n",
    "subreddits = config.get(\"subreddit\", [\"technology\", \"ProgrammerHumor\", \"news\", \"worldnews\"])\n",
    "bucket_name = \"reddit-streaming-stevenhurwitt-2\" \n",
    "\n",
    "print(f\"Subreddits to process: {subreddits}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aa93ab",
   "metadata": {},
   "source": [
    "# Read Delta Tables from S3\n",
    "\n",
    "This notebook reads reddit data stored in Delta format on S3, unions the data from multiple subreddits, and sorts by most recent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
