{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Read CSV Data and Sort, Count and View Shuffles and Execution Plans, Filter Operations #\n",
    "Examples based on *Spark: Definitive Guide: Big Data processing Made Simple*, by Mate Zaharia and Bill Chambers - Chapter 2.  \n",
    "\n",
    "In this example, sample flight data for 2010 to 2015 is processed and the execution plan for a wide transformation (shuffle) is demonstrated.  Simple Sort and Count operations are performed against the data.  **Prerequisite:** The sample data can be downloaded to `./datain/flight-data` with the `data-download.ipynb` notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/02 18:08:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-nb-2-execplan\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        config(\"spark.eventLog.enabled\", \"true\").\\\n",
    "        config(\"spark.eventLog.dir\", \"file:///opt/workspace/events\").\\\n",
    "        getOrCreate()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all the flight-data CSV files in the sample `../datain/flight-data` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "flightData = spark.read.option(\"inferSchema\", True).option(\"header\", True).csv(\"/opt/workspace/datain/flight-data/*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Data ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View a small sample of the data-set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=264),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=69),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=24),\n",
       " Row(DEST_COUNTRY_NAME='Equatorial Guinea', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get summary statistics about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+------------------+\n",
      "|summary|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|             count|\n",
      "+-------+-----------------+-------------------+------------------+\n",
      "|  count|             1502|               1502|              1502|\n",
      "|   mean|             null|               null|1718.3189081225032|\n",
      "| stddev|             null|               null|22300.368619668898|\n",
      "|    min|      Afghanistan|        Afghanistan|                 1|\n",
      "|    max|         Zimbabwe|           Zimbabwe|            370002|\n",
      "+-------+-----------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1502 Destination countries and 1502 origin countries listed, each with a count value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Execution Plans ##\n",
    "The Spark `explain()` method can be used to show the execution strategy that will be chosen by Spark to execute a statement.  \n",
    "  \n",
    "In the example below, the `sort()` action requires all data from all partitions to be compared - this causes a *shuffle* AKA *partition exchange* which is shown in the execution plan as *Exchange rangepartitioning*.  This happens after the previous *FileScan* operation which reads all the data in to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [count#12 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#12 ASC NULLS FIRST, 200)\n",
      "   +- *(1) FileScan csv [DEST_COUNTRY_NAME#10,ORIGIN_COUNTRY_NAME#11,count#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/opt/workspace/datain/flight-data/2010-summary.csv, file:/opt/workspace/da..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "flightData.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the execution plan, the stage\n",
    "```\n",
    " +- Exchange rangepartitioning(count#12 ASC NULLS FIRST, 200)\n",
    "```\n",
    "shows that data from 200 partitions is exchanged between cluster nodes to perform the sort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='United States', count=370002),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='United States', count=358354),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='United States', count=352742),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='United States', count=348113),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='United States', count=347452)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData.sort(\"count\",ascending=False).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Plan Exchange operations ####\n",
    "The term `Exchange` means *shuffle* data between physical nodes in the cluster.  In general, try to reduce the number of partition exchanges.\n",
    "*Exchange* is typically triggered by:\n",
    "+ **Repartition** - re-organising data in *n* partitions - triggers *RoundRobinPartitioning Exchange*  \n",
    "+ **Coalesce** - This reduces the number of partitions without a full re-partition.  EG, move all data to a single CSV file (single executor) will trigger *SinglePartitionExchange*  \n",
    "+ **Sort** - to sort the output data, a *RangePartitioning Exchange* is used  \n",
    "+ **Join** - when a Hash Join occurs, *Hash Partitioning Exchange* is triggered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the Shuffle Partition Configuration ####\n",
    "By setting the `spark.sql.shuffle.partitions` parameter, we can specify how many partitions to use in the data shuffle operation.  The default is 200 - we probably only need 2 for a 2 node cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [count#12 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#12 ASC NULLS FIRST, 2)\n",
      "   +- *(1) FileScan csv [DEST_COUNTRY_NAME#10,ORIGIN_COUNTRY_NAME#11,count#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/opt/workspace/datain/flight-data/2010-summary.csv, file:/opt/workspace/da..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "flightData.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see\n",
    "```\n",
    "+- Exchange rangepartitioning(count#12 ASC NULLS FIRST, 2)\n",
    "```\n",
    "in the execution plan (only 2 partitions are exchanged between cluster nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='United States', count=370002),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='United States', count=358354),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='United States', count=352742),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='United States', count=348113),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='United States', count=347452)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Due to Spark \"lazy execution\", our sort finally gets executed now (not at the Explain stage)\n",
    "flightData.sort(\"count\",ascending=False).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of a Narrow Execution followed by Wide  - sum up all the counts ####\n",
    "Count can be performed at each partition (a Narrow operation - no shuffle) then the results combined to a single count (a Wide operation / Shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[], functions=[sum(cast(count#12 as bigint))])\n",
      "+- Exchange SinglePartition\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_sum(cast(count#12 as bigint))])\n",
      "      +- *(1) FileScan csv [count#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/opt/workspace/datain/flight-data/2010-summary.csv, file:/opt/workspace/da..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<count:int>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "flightData.select(F.sum(\"count\")).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "flights = flightData.select(F.sum(\"count\")).withColumnRenamed(\"sum(count)\", \"flights_count\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(flights_count=2580915)]\n",
      "2580915\n"
     ]
    }
   ],
   "source": [
    "print(flights)\n",
    "print(flights[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Rows and Columns ##\n",
    "Filter out all the countries where Destination is not United States and then drop the DEST_COUNTRY_NAME column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook-2-filter-count\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        config(\"spark.eventLog.enabled\", \"true\").\\\n",
    "        config(\"spark.eventLog.dir\", \"file:///opt/workspace/events\").\\\n",
    "        getOrCreate()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "flightData = spark.read.option(\"inferSchema\", True).option(\"header\", True).csv(\"/opt/workspace/datain/flight-data/*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering Data ####\n",
    "\n",
    "Rows in Dataframes can be filtered using the `.filter` method in the Spark SQL API.  \n",
    "Use `pyspark.sql.functions.col()` to reference a column to be checked in a filter expression.   \n",
    "Columns in Dataframes can be filtered or dropped using the `drop` method.  \n",
    "\n",
    "#### Physical and Logical Execution Plans ####\n",
    "Set the Explain mode to *extended* by providing a Boolean `True` argument to the `explain()` operation causes the Logical execution plan as well as physical plan derived from the logical plan to be displayed.\n",
    "  \n",
    "In *Spark 3* additional modes can be specified - *simple*, *extended*, *codegen*, *cost*, *formatted* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Project [ORIGIN_COUNTRY_NAME#244, count#245]\n",
      "+- Filter (DEST_COUNTRY_NAME#243 = United States)\n",
      "   +- Relation[DEST_COUNTRY_NAME#243,ORIGIN_COUNTRY_NAME#244,count#245] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "ORIGIN_COUNTRY_NAME: string, count: int\n",
      "Project [ORIGIN_COUNTRY_NAME#244, count#245]\n",
      "+- Filter (DEST_COUNTRY_NAME#243 = United States)\n",
      "   +- Relation[DEST_COUNTRY_NAME#243,ORIGIN_COUNTRY_NAME#244,count#245] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [ORIGIN_COUNTRY_NAME#244, count#245]\n",
      "+- Filter (isnotnull(DEST_COUNTRY_NAME#243) && (DEST_COUNTRY_NAME#243 = United States))\n",
      "   +- Relation[DEST_COUNTRY_NAME#243,ORIGIN_COUNTRY_NAME#244,count#245] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [ORIGIN_COUNTRY_NAME#244, count#245]\n",
      "+- *(1) Filter (isnotnull(DEST_COUNTRY_NAME#243) && (DEST_COUNTRY_NAME#243 = United States))\n",
      "   +- *(1) FileScan csv [DEST_COUNTRY_NAME#243,ORIGIN_COUNTRY_NAME#244,count#245] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/opt/workspace/datain/flight-data/2010-summary.csv, file:/opt/workspace/da..., PartitionFilters: [], PushedFilters: [IsNotNull(DEST_COUNTRY_NAME), EqualTo(DEST_COUNTRY_NAME,United States)], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "# Use Spark.SQL API syntax to filter - use col Function to reference col-name (needs wrapping in parentheses)\n",
    "# To drop multiple columns, create a list and unpack in the function-call EG drop(*my_list_of_columns)\n",
    "flightDataToUSA = flightData.filter((F.col(\"DEST_COUNTRY_NAME\") == \"United States\")).drop(\"DEST_COUNTRY_NAME\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightDataToUSA = flightData.filter((F.col(\"DEST_COUNTRY_NAME\") == \"United States\")).drop(\"DEST_COUNTRY_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ORIGIN_COUNTRY_NAME='Romania', count=1),\n",
       " Row(ORIGIN_COUNTRY_NAME='Ireland', count=264),\n",
       " Row(ORIGIN_COUNTRY_NAME='India', count=69),\n",
       " Row(ORIGIN_COUNTRY_NAME='Singapore', count=25),\n",
       " Row(ORIGIN_COUNTRY_NAME='Grenada', count=54),\n",
       " Row(ORIGIN_COUNTRY_NAME='Marshall Islands', count=44),\n",
       " Row(ORIGIN_COUNTRY_NAME='Sint Maarten', count=53),\n",
       " Row(ORIGIN_COUNTRY_NAME='Afghanistan', count=2),\n",
       " Row(ORIGIN_COUNTRY_NAME='Russia', count=156),\n",
       " Row(ORIGIN_COUNTRY_NAME='Federated States of Micronesia', count=48)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightDataToUSA.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
