{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3770b947",
   "metadata": {},
   "source": [
    "# 3. Read CSV Data and perform Basic Data Analysis #\n",
    "Examples based on Chapter 2, \"Spark: Definitive Guide: Big Data processing Made Simple\"\n",
    "\n",
    "In this example, sample flight data for 2010 to 2015 is processed and Simple **MAX**, **Top-N** and **Group-By** operations are performed against the data using Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ecd3ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/08 16:50:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-nb-3-analysis\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        config(\"spark.eventLog.enabled\", \"true\").\\\n",
    "        config(\"spark.eventLog.dir\", \"file:///opt/workspace/events\").\\\n",
    "        getOrCreate()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "176d7b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read flight data in from CSV\n",
    "flightData = spark.read.option(\"inferSchema\", True).option(\"header\", True).csv(\"/opt/workspace/datain/flight-data/*.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f988ae9d",
   "metadata": {},
   "source": [
    "#### Spark SQL Views ####\n",
    "A temporary table-view can be constructed on a Spark data-frame with the `createOrReplaceTempView()` method.  \n",
    "\n",
    "This allows the data-frame data to be analysed using ANSI SQL via the temporary table construct, which is an in-memory non-persistant view of the data built on the Spark data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e1241ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData.createOrReplaceTempView(\"flight_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95538dac",
   "metadata": {},
   "source": [
    "#### Group-By Example ####\n",
    "Query the data-frame and count the number of flights grouped by destination country.  \n",
    "Using Spark SQL to query the temporary table `flight_data` creates another Spark data-frame (in this case, \"`results`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53073aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, count(1)\n",
    "FROM flight_data\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "931891ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#10, 200)\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [DEST_COUNTRY_NAME#10] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/opt/workspace/datain/flight-data/2010-summary.csv, file:/opt/workspace/da..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n"
     ]
    }
   ],
   "source": [
    "# View the execution plan for running this query.  \n",
    "results.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a95fd3",
   "metadata": {},
   "source": [
    "#### Max-Value Example ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28598568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get max flight destination from flights using Spark SQL\n",
    "spark.sql(\"SELECT max(count) from flight_data\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61bc8239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pyspark / Python equiv\n",
    "from pyspark.sql.functions import max\n",
    "flightData.select(max(\"count\")).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8635f74a",
   "metadata": {},
   "source": [
    "#### Top-N Example ####\n",
    "##### SQL Syntax #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5512939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:==============================================>        (170 + 2) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|          2348280|\n",
      "|           Canada|            49052|\n",
      "|           Mexico|            38075|\n",
      "|   United Kingdom|            10946|\n",
      "|            Japan|             9205|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Find top 5 destinations\n",
    "top_5_dest = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "FROM flight_data\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "top_5_dest.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e2c6eb",
   "metadata": {},
   "source": [
    "##### Spark SQL Dataframe API Syntax #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce4a0db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:===============================================>      (176 + 3) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|          2348280|\n",
      "|           Canada|            49052|\n",
      "|           Mexico|            38075|\n",
      "|   United Kingdom|            10946|\n",
      "|            Japan|             9205|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Find top 5 destinations - DataFrame syntax\n",
    "from pyspark.sql.functions import desc\n",
    "flightData.groupBy(\"DEST_COUNTRY_NAME\").sum(\"count\")\\\n",
    "    .withColumnRenamed(\"sum(count)\", \"destination_total\").sort(desc(\"destination_total\"))\\\n",
    "    .limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c99a616",
   "metadata": {},
   "source": [
    "#### Show Execution Path ####\n",
    "Aggregation happens in two parts in the execution plan - in the `partial_sum` and `sum` calls. This is because summing a list of numbers is *commutative* and Spark can perform the sum partition-by-partition.\n",
    "\n",
    "*Commutative*: \"condition that a group of quantities connected by operators gives the same result whatever the order of the quantities involved, e.g. a × b = b × a.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7436b1b5",
   "metadata": {},
   "source": [
    "##### SQL Execution Plan #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08ec3e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[aggOrder#74L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#10,destination_total#72L])\n",
      "+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[sum(cast(count#12 as bigint))])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#10, 200)\n",
      "      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[partial_sum(cast(count#12 as bigint))])\n",
      "         +- *(1) FileScan csv [DEST_COUNTRY_NAME#10,count#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/opt/workspace/datain/flight-data/2010-summary.csv, file:/opt/workspace/da..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "FROM flight_data\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5\n",
    "\"\"\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5425dfa3",
   "metadata": {},
   "source": [
    "##### Spark SQL DataFrame API Execution Plan #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57ac78f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[destination_total#87L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#10,destination_total#87L])\n",
      "+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[sum(cast(count#12 as bigint))])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#10, 200)\n",
      "      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[partial_sum(cast(count#12 as bigint))])\n",
      "         +- *(1) FileScan csv [DEST_COUNTRY_NAME#10,count#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/opt/workspace/datain/flight-data/2010-summary.csv, file:/opt/workspace/da..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "flightData.groupBy(\"DEST_COUNTRY_NAME\").sum(\"count\")\\\n",
    "    .withColumnRenamed(\"sum(count)\", \"destination_total\").sort(desc(\"destination_total\"))\\\n",
    "    .limit(5).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495a0834",
   "metadata": {},
   "source": [
    "\n",
    "+ *Exactly the same execution path is used*, whether performing the operation via Spark SQL or on the data-frame API using pyspark.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fd3ce1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982b2538",
   "metadata": {},
   "source": [
    "## Part 2 - Windowing Analytic Functions ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5485903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-nb-3-analysis-pt2\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        config(\"spark.eventLog.enabled\", \"true\").\\\n",
    "        config(\"spark.eventLog.dir\", \"file:///opt/workspace/events\").\\\n",
    "        getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec3a3bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# load data - data_download notebook downloads data locally \n",
    "retail_df = spark.read.option(\"inferSchema\", True).option(\"header\", True).csv(\"/opt/workspace/datain/retail-data/by-day/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f66af003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retail_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98072f53",
   "metadata": {},
   "source": [
    "#### Partition-Window Rank Example\n",
    "##### Find Most Popular Stock Code for each Country #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ddbf235",
   "metadata": {},
   "outputs": [],
   "source": [
    "retail_df.createOrReplaceTempView(\"retail_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddd9e58",
   "metadata": {},
   "source": [
    "Only show results for countries with more than 100 items to remove low-scores with lots of duplicates\n",
    "##### SQL Approach #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38a72cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-----------------------------------+-----+\n",
      "|Country        |StockCode|Description                        |count|\n",
      "+---------------+---------+-----------------------------------+-----+\n",
      "|Australia      |22492    |MINI PAINT SET VINTAGE             |2916 |\n",
      "|Austria        |21918    |SET 12 KIDS COLOUR  CHALK STICKS   |288  |\n",
      "|Belgium        |21212    |PACK OF 72 RETROSPOT CAKE CASES    |480  |\n",
      "|Canada         |37370    |RETRO COFFEE MUGS ASSORTED         |504  |\n",
      "|Channel Islands|21785    |RAIN PONCHO                        |407  |\n",
      "|Cyprus         |22335    |HEART DECORATION PAINTED ZINC      |384  |\n",
      "|Denmark        |21915    |RED  HARMONICA IN BOX              |288  |\n",
      "|EIRE           |21212    |PACK OF 72 RETROSPOT CAKE CASES    |1728 |\n",
      "|Finland        |84997D   |CHILDRENS CUTLERY POLKADOT PINK    |480  |\n",
      "|France         |23084    |RABBIT NIGHT LIGHT                 |4023 |\n",
      "|Germany        |22326    |ROUND SNACK BOXES SET OF4 WOODLAND |1218 |\n",
      "|Hong Kong      |22326    |ROUND SNACK BOXES SET OF4 WOODLAND |150  |\n",
      "|Iceland        |23076    |ICE CREAM SUNDAE LIP GLOSS         |240  |\n",
      "|Israel         |20719    |WOODLAND CHARLOTTE BAG             |130  |\n",
      "|Italy          |51014A   |FEATHER PEN,HOT PINK               |240  |\n",
      "|Japan          |23084    |RABBIT NIGHT LIGHT                 |3401 |\n",
      "|Netherlands    |23084    |RABBIT NIGHT LIGHT                 |4801 |\n",
      "|Norway         |16008    |SMALL FOLDING SCISSOR(POINTED EDGE)|576  |\n",
      "|Poland         |21232    |STRAWBERRY CERAMIC TRINKET BOX     |144  |\n",
      "|Portugal       |22740    |POLKADOT PEN                       |240  |\n",
      "|Singapore      |22339    |CHRISTMAS TREE PAINTED ZINC        |384  |\n",
      "|Spain          |84997D   |CHILDRENS CUTLERY POLKADOT PINK    |729  |\n",
      "|Sweden         |22492    |MINI PAINT SET VINTAGE             |2916 |\n",
      "|Switzerland    |22554    |PLASTERS IN TIN WOODLAND ANIMALS   |639  |\n",
      "|United Kingdom |84077    |WORLD WAR 2 GLIDERS ASSTD DESIGNS  |48326|\n",
      "+---------------+---------+-----------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT Country,StockCode, Description, count\n",
    "FROM (\n",
    "SELECT\n",
    "    Country,\n",
    "    StockCode,\n",
    "    Description,\n",
    "    sum(Quantity) AS count,\n",
    "    rank() OVER (PARTITION BY Country ORDER BY sum(Quantity) DESC) as rank\n",
    "  FROM retail_view\n",
    "  GROUP BY Country, StockCode, Description\n",
    "  ) WHERE RANK = 1 AND count > 100\n",
    "ORDER BY Country\n",
    "\"\"\").show(100,  truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a035fc11",
   "metadata": {},
   "source": [
    "##### DataFrame API Approach #####\n",
    "Partition data with a Window function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32d386e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Get the Country, Stock-Code, Description, Quantity-Count\n",
    "# 1. pre-processing aggregation\n",
    "country_sum_df = retail_df.groupBy(\"Country\",\"StockCode\", \"Description\").sum(\"Quantity\").withColumnRenamed(\"sum(Quantity)\", \"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dbb8046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=====================================================> (194 + 3) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+--------------------+-----+\n",
      "|        Country|StockCode|         Description|count|\n",
      "+---------------+---------+--------------------+-----+\n",
      "|      Australia|    22492|MINI PAINT SET VI...| 2916|\n",
      "|        Austria|    21918|SET 12 KIDS COLOU...|  288|\n",
      "|        Belgium|    21212|PACK OF 72 RETROS...|  480|\n",
      "|         Canada|    37370|RETRO COFFEE MUGS...|  504|\n",
      "|Channel Islands|    21785|        RAIN PONCHO |  407|\n",
      "|         Cyprus|    22335|HEART DECORATION ...|  384|\n",
      "|        Denmark|    21915|RED  HARMONICA IN...|  288|\n",
      "|           EIRE|    21212|PACK OF 72 RETROS...| 1728|\n",
      "|        Finland|   84997D|CHILDRENS CUTLERY...|  480|\n",
      "|         France|    23084|  RABBIT NIGHT LIGHT| 4023|\n",
      "|        Germany|    22326|ROUND SNACK BOXES...| 1218|\n",
      "|      Hong Kong|    22326|ROUND SNACK BOXES...|  150|\n",
      "|        Iceland|    23076|ICE CREAM SUNDAE ...|  240|\n",
      "|         Israel|    20719|WOODLAND CHARLOTT...|  130|\n",
      "|          Italy|   51014A|FEATHER PEN,HOT PINK|  240|\n",
      "|          Japan|    23084|  RABBIT NIGHT LIGHT| 3401|\n",
      "|    Netherlands|    23084|  RABBIT NIGHT LIGHT| 4801|\n",
      "|         Norway|    16008|SMALL FOLDING SCI...|  576|\n",
      "|         Poland|    21232|STRAWBERRY CERAMI...|  144|\n",
      "|       Portugal|    22740|        POLKADOT PEN|  240|\n",
      "|      Singapore|    22339|CHRISTMAS TREE PA...|  384|\n",
      "|          Spain|   84997D|CHILDRENS CUTLERY...|  729|\n",
      "|         Sweden|    22492|MINI PAINT SET VI...| 2916|\n",
      "|    Switzerland|    22554|PLASTERS IN TIN W...|  639|\n",
      "| United Kingdom|    84077|WORLD WAR 2 GLIDE...|48326|\n",
      "+---------------+---------+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# get the most popular product for each country\n",
    "# 2. Define a Window\n",
    "window = Window.partitionBy(\"Country\").orderBy(col(\"count\").desc())\n",
    "# 3. Rank over a window\n",
    "country_sum_df.withColumn(\"row\",row_number().over(window)) \\\n",
    "  .filter(col(\"row\") == 1).drop(\"row\") \\\n",
    "  .filter(col(\"count\") >= 100) \\\n",
    "  .orderBy(col(\"Country\")) \\\n",
    "  .show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9427884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000f0955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
