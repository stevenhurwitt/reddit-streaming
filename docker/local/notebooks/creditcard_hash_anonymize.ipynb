{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "391f2753",
   "metadata": {},
   "source": [
    "# Hash-Anonymize Data with Spark UDF and Python Hashlib SHA-256 #\n",
    "Example loading some simulated sensitive data, anonymizing it and saving it in Parquet format partitioned by year and month."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b0d004",
   "metadata": {},
   "source": [
    "### Stage the Data ###\n",
    "Source data from web-site:  \n",
    "https://eforexcel.com/wp/downloads-17-sample-csv-files-data-sets-for-testing-credit-card/  \n",
    "(download a 1000 record sample-set).  \n",
    "\n",
    "Copy the data in to Docker:\n",
    "```\n",
    "docker cp 1000-CC-Records.csv jupyterlab:/opt/workspace/datain/credit-cards/\n",
    "```\n",
    "(it may be necessary to create the `datain/credit-cards` folder if it doesn't already exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c23820a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8118ce1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/23 08:03:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"hash_anon_udf\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        config(\"spark.eventLog.enabled\", \"true\").\\\n",
    "        config(\"spark.eventLog.dir\", \"file:///opt/workspace/events\").\\\n",
    "        getOrCreate()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "411ee6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read the data in from a CSV\n",
    "df = spark.read.option(\"inferSchema\", True).option(\"header\", True)\\\n",
    "    .csv(\"/opt/workspace/datain/credit-cards/1000-CC-Records.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc61cc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the columns\n",
    "old_columns = df.columns\n",
    "new_columns = [\"card_type\", \"bank\", \"card_number\", \"card_holder\", \"cvv\", \"issue_date\", \"expiry_date\", \"billing_date\", \"card_pin\", \"credit_limit\"]\n",
    "\n",
    "def renameCols(df, old_columns, new_columns):\n",
    "    for old_col,new_col in zip(old_columns,new_columns):\n",
    "        df = df.withColumnRenamed(old_col,new_col)\n",
    "    return df\n",
    "\n",
    "df = renameCols(df, old_columns, new_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daa3dae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- bank: string (nullable = true)\n",
      " |-- card_number: long (nullable = true)\n",
      " |-- card_holder: string (nullable = true)\n",
      " |-- cvv: integer (nullable = true)\n",
      " |-- issue_date: string (nullable = true)\n",
      " |-- expiry_date: string (nullable = true)\n",
      " |-- billing_date: integer (nullable = true)\n",
      " |-- card_pin: integer (nullable = true)\n",
      " |-- credit_limit: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b688ea23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+----------------+---------------+----+----------+-----------+------------+--------+------------+\n",
      "|          card_type|            bank|     card_number|    card_holder| cvv|issue_date|expiry_date|billing_date|card_pin|credit_limit|\n",
      "+-------------------+----------------+----------------+---------------+----+----------+-----------+------------+--------+------------+\n",
      "|               Visa|           Chase|4431465245886276|  Frank Q Ortiz| 362|   09/2016|    09/2034|           7|    1247|      103700|\n",
      "|           Discover|        Discover|6224764404044446|Tony E Martinez|  35|   06/2012|    06/2030|          23|    6190|       92900|\n",
      "|Japan Credit Bureau|             JCB|3541789329050940|    Ana M Downs| 945|   03/2017|    03/2021|          10|    8550|       71500|\n",
      "|   American Express|American Express| 371306399244328| Calvin T House|3868|   09/2007|    09/2018|          26|    1777|      190500|\n",
      "|               Visa|           Chase|4332985341176660|   Gilda J Wade| 237|   12/2012|    12/2023|          26|    5551|      179700|\n",
      "+-------------------+----------------+----------------+---------------+----+----------+-----------+------------+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d05dd42",
   "metadata": {},
   "source": [
    "## Create a new `df_anon` with Anonymized Names, and Card-Number, remove the PIN ##\n",
    "\n",
    "Use Python `hashlib` to tokenise / anonymize the data.  A given value will always have the same hash-value, so analysis, aggregations etc can be performed against the anonymized data.\n",
    "\n",
    "https://docs.python.org/3/library/hashlib.html\n",
    "\n",
    "`hashlib.pbkdf2_hmac(hash_name, password, salt, iterations, dklen=None)`\n",
    "\n",
    "\n",
    "EG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e358dc49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0394a2ede332c9a13eb82e9b24631604c31df978b4e2f0fbd2c549944f9d79a5'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example using pbkdf2_hmac - this could be used to generate a salted password / secret\n",
    "import hashlib\n",
    "dk = hashlib.pbkdf2_hmac('sha256', b'password', b'salt', 100000)\n",
    "dk.hex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc100827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secret to make this hash different to other hashes for a given value (not salted)\n",
    "SECRET = \"ZZxxyy##1234\"\n",
    "# Iterations chosen based on the hash algorithm and computing power. As of 2013, at least 100,000 iterations of SHA-256 are suggested\n",
    "ITERATIONS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d602479f",
   "metadata": {},
   "source": [
    "#### Create a Spark UDF that calls Anonymized Data fn ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a699f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "\n",
    "# function to be called by UDF to sha256-hash a string\n",
    "def hash_anonymized(payload):\n",
    "    return hashlib.pbkdf2_hmac('sha256', payload.encode('utf-8'), SECRET.encode('utf-8'), ITERATIONS).hex()\n",
    "    #return None\n",
    "\n",
    "# Anon data using a UDF\n",
    "spark_udf = udf(hash_anonymized, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f70582f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to anonymize specified columns in a dataframe\n",
    "def anonymize_data_frame(df, list_of_columns, replace_column = True):\n",
    "    for col in list_of_columns:\n",
    "        if replace_column:\n",
    "            df = df.withColumn(col, spark_udf(col))\n",
    "        else:\n",
    "            df = df.withColumn(col + \"_anon\", spark_udf(col))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5558490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spark.apache.org/docs/2.1.1/api/python/_modules/pyspark/sql/types.html\n",
    "from pyspark.sql.types import StringType, IntegerType,BooleanType,DateType, LongType, DecimalType, TimestampType\n",
    "\n",
    "# because the card  number is not a string, need to cast it to string first before it can be hash-tokenized\n",
    "df = df.withColumn(\"card_number\", df[\"card_number\"].cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe02c259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the issue_date into Year and Month - could just keep this as year_month, depending on access patterns\n",
    "from pyspark.sql.functions import split\n",
    "split_col = split(df[\"issue_date\"], \"/\")\n",
    "df = df.withColumn(\"issue_year\", split_col.getItem(1))\n",
    "df = df.withColumn(\"issue_month\", split_col.getItem(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c194c091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- bank: string (nullable = true)\n",
      " |-- card_number: string (nullable = true)\n",
      " |-- card_holder: string (nullable = true)\n",
      " |-- cvv: integer (nullable = true)\n",
      " |-- issue_date: string (nullable = true)\n",
      " |-- expiry_date: string (nullable = true)\n",
      " |-- billing_date: integer (nullable = true)\n",
      " |-- card_pin: integer (nullable = true)\n",
      " |-- credit_limit: integer (nullable = true)\n",
      " |-- issue_year: string (nullable = true)\n",
      " |-- issue_month: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a8542a",
   "metadata": {},
   "source": [
    "#### Call the Anonymize UDF ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8723b625",
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_cols = [\"card_number\",\"card_holder\"]\n",
    "# create new DF based on the source data with [anon_cols] hashed\n",
    "anon_df = anonymize_data_frame(df, anon_cols, replace_column=True).drop(*[\"card_pin\",\"cvv\", \"issue_date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78239c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+--------------------+--------------------+-----------+------------+------------+----------+-----------+\n",
      "|          card_type|            bank|         card_number|         card_holder|expiry_date|billing_date|credit_limit|issue_year|issue_month|\n",
      "+-------------------+----------------+--------------------+--------------------+-----------+------------+------------+----------+-----------+\n",
      "|               Visa|           Chase|a07bf636334354c7a...|5b748745f61c945bc...|    09/2034|           7|      103700|      2016|         09|\n",
      "|           Discover|        Discover|9231ba4f885c17605...|d7775505e407aa08c...|    06/2030|          23|       92900|      2012|         06|\n",
      "|Japan Credit Bureau|             JCB|1d3b85e871e3fbabb...|0d4ee40547bf2820c...|    03/2021|          10|       71500|      2017|         03|\n",
      "|   American Express|American Express|783544b31ab243f14...|2392de94a79599647...|    09/2018|          26|      190500|      2007|         09|\n",
      "|               Visa|           Chase|dd4aea6bf1e7b4b1f...|bb95557c86d80df5a...|    12/2023|          26|      179700|      2012|         12|\n",
      "+-------------------+----------------+--------------------+--------------------+-----------+------------+------------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# sample of anonymized data frame\n",
    "anon_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1d646be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+----------------+---------------+----+----------+-----------+------------+--------+------------+----------+-----------+\n",
      "|          card_type|            bank|     card_number|    card_holder| cvv|issue_date|expiry_date|billing_date|card_pin|credit_limit|issue_year|issue_month|\n",
      "+-------------------+----------------+----------------+---------------+----+----------+-----------+------------+--------+------------+----------+-----------+\n",
      "|               Visa|           Chase|4431465245886276|  Frank Q Ortiz| 362|   09/2016|    09/2034|           7|    1247|      103700|      2016|         09|\n",
      "|           Discover|        Discover|6224764404044446|Tony E Martinez|  35|   06/2012|    06/2030|          23|    6190|       92900|      2012|         06|\n",
      "|Japan Credit Bureau|             JCB|3541789329050940|    Ana M Downs| 945|   03/2017|    03/2021|          10|    8550|       71500|      2017|         03|\n",
      "|   American Express|American Express| 371306399244328| Calvin T House|3868|   09/2007|    09/2018|          26|    1777|      190500|      2007|         09|\n",
      "|               Visa|           Chase|4332985341176660|   Gilda J Wade| 237|   12/2012|    12/2023|          26|    5551|      179700|      2012|         12|\n",
      "+-------------------+----------------+----------------+---------------+----+----------+-----------+------------+--------+------------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sample of original data\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8f9b30",
   "metadata": {},
   "source": [
    "## Write Data Out ##\n",
    "Partition the data by issue-date Year and Month.  Option to Overwrite or Append to the data\n",
    "\n",
    "Consider strategies how to add new data but not duplicate data: https://stackoverflow.com/questions/42317738/how-to-partition-and-write-dataframe-in-spark-without-deleting-partitions-with-n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41fc0a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "anon_df.write.format(\"parquet\")\\\n",
    "                .mode(\"overwrite\")\\\n",
    "                .partitionBy('issue_year', 'issue_month')\\\n",
    "                .save(\"/opt/workspace/dataout/credit-cards/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9046599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1185ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
