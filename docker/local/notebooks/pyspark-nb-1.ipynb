{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Frame Transformation #\n",
    "Example from *Spark: Definitive Guide: Big Data processing Made Simple*, by Mate Zaharia and Bill Chambers - Chapter 2.\n",
    "Create a Spark session using `pyspark.sql`, create a range of numbers in a DataFrame and then apply two transformations:\n",
    "+ Count all the even numbers\n",
    "+ Sort the numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_322\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_322-b06)\n",
      "OpenJDK 64-Bit Server VM (build 25.322-b06, mixed mode)\n",
      "Python 3.7.9\n"
     ]
    }
   ],
   "source": [
    "! java -version\n",
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! databricks-connect configure -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/28 02:22:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Exception in thread \"Thread-4\" ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1207, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1033, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\", line 1212, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while calling None.None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java.lang.ExceptionInInitializerError\n",
      "\tat org.apache.spark.api.python.Py4JServer.$anonfun$server$1(Py4JServer.scala:65)\n",
      "\tat java.util.Collections$SingletonList.forEach(Collections.java:4824)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:250)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.RuntimeException: Config file /root/.databricks-connect not found. Please run `databricks-connect configure` to accept the end user license agreement and configure Databricks Connect. A copy of the EULA is provided below:\n",
      "\n",
      "Copyright (2018) Databricks, Inc.\n",
      "\n",
      "This library (the \"Software\") may not be used except in connection with the Licensee's use of the Databricks Platform Services pursuant to an Agreement (defined below) between Licensee (defined below) and Databricks, Inc. (\"Databricks\"). This Software shall be deemed part of the “Subscription Services” under the Agreement, or if the Agreement does not define Subscription Services, then the term in such Agreement that refers to the applicable Databricks Platform Services (as defined below) shall be substituted herein for “Subscription Services.”  Licensee's use of the Software must comply at all times with any restrictions applicable to the Subscription Services, generally, and must be used in accordance with any applicable documentation. If you have not agreed to an Agreement or otherwise do not agree to these terms, you may not use the Software.  This license terminates automatically upon the termination of the Agreement or Licensee's breach of these terms.\n",
      "\n",
      "Agreement: the agreement between Databricks and Licensee governing the use of the Databricks Platform Services, which shall be, with respect to Databricks, the Databricks Terms of Service located at www.databricks.com/termsofservice, and with respect to Databricks Community Edition, the Community Edition Terms of Service located at www.databricks.com/ce-termsofuse, in each case unless Licensee has entered into a separate written agreement with Databricks governing the use of the applicable Databricks Platform Services. Databricks Platform Services: the Databricks services or the Databricks Community Edition services, according to where the Software is used.\n",
      "\n",
      "Licensee: the user of the Software, or, if the Software is being used on behalf of a company, the company.\n",
      "\n",
      "To accept this agreement and start using Databricks Connect, run `databricks-connect configure` in a shell.\n",
      "\tat com.databricks.spark.util.DatabricksConnectConf$.checkEula(DatabricksConnectConf.scala:43)\n",
      "\tat org.apache.spark.SparkContext$.<init>(SparkContext.scala:2749)\n",
      "\tat org.apache.spark.SparkContext$.<clinit>(SparkContext.scala)\n",
      "\t... 4 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "try:\n",
    "    spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(\"pyspark-nb-1\") \\\n",
    "            .master(\"spark://spark-master:7077\") \\\n",
    "            .config(\"spark.executor.memory\", \"1024m\") \\\n",
    "            .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "            .config(\"spark.eventLog.dir\", \"file:///opt/workspace/events\") \\\n",
    "            .getOrCreate()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a range of 1000 numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myrange = spark.range(1000).toDF(\"number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "Find all the even numbers - this is an example of a *Narrow Transformation* (no shuffle) as no data has to be moved between partitions - just a **filter** on a per-partition basis.  Spark uses \"lazy evaluation\" so nothing is executed at this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisBy2 = myrange.where(\"number % 2 = 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "Count all the items in the result-set \"divisBy2\"; this is an Example of a *Wide Transformation*.  There is an **aggregation** (reduce) that performs several counts on a per-partition basis and then a collect into a final result-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisBy2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is an **action** and causes the previous lines of code to be executed.  Standard types of Actions are:\n",
    "- view data in console\n",
    "- collect data to native objects in respective app API language\n",
    "- write data to output destination\n",
    "   \n",
    "Next, sort the *divisBy2* dataframe and take the first 5 numbers from the sorted result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisBy2.sort(\"number\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
