#!/bin/bash
# prepare_for_tonight.sh - Prepare system for tonight's curation jobs

echo "ğŸ› ï¸  Preparing system for tonight's curation jobs..."

# Test and run the cleanup script
echo "ğŸ“‹ Testing cleanup script..."
cd /home/steven/reddit-streaming/local_jobs
./kill_stuck_jobs.sh --force

if [ $? -eq 0 ]; then
    echo "âœ… Cleanup completed successfully"
else
    echo "âš ï¸  Cleanup had issues, but continuing..."
fi

# Update cron jobs with new configuration
echo "ğŸ• Updating cron jobs..."
./setup_cron.sh

# Verify Docker containers are running
echo "ğŸ³ Checking Docker services..."
cd /home/steven/reddit-streaming

echo "ğŸ“Š Current container status:"
docker-compose ps

# Check if key services are running
SPARK_MASTER_STATUS=$(docker-compose ps spark-master | grep -c "Up")
SPARK_WORKER_STATUS=$(docker-compose ps | grep spark-worker | grep -c "Up")

if [ "$SPARK_MASTER_STATUS" -eq 1 ]; then
    echo "âœ… Spark Master is running"
else
    echo "âŒ Spark Master is not running - attempting restart..."
    docker-compose up -d spark-master
    sleep 10
fi

if [ "$SPARK_WORKER_STATUS" -ge 1 ]; then
    echo "âœ… Spark Workers are running ($SPARK_WORKER_STATUS workers)"
else
    echo "âŒ No Spark Workers running - attempting restart..."
    docker-compose up -d spark-worker-1 spark-worker-2
    sleep 15
fi

# Test Spark connectivity
echo "ğŸ”— Testing Spark cluster connectivity..."
docker exec reddit-spark-master python3 -c "
from pyspark.sql import SparkSession
try:
    spark = SparkSession.builder.appName('TestConnectivity').master('spark://spark-master:7077').getOrCreate()
    print('âœ… Spark connection successful')
    spark.stop()
except Exception as e:
    print(f'âŒ Spark connection failed: {e}')
    exit(1)
" 2>/dev/null

if [ $? -eq 0 ]; then
    echo "ğŸ¯ Spark cluster is ready for jobs"
else
    echo "âš ï¸  Spark connectivity test failed - may need manual intervention"
    echo "ğŸ’¡ Try: docker-compose restart spark-master spark-worker-1 spark-worker-2"
fi

# Check disk space
echo "ğŸ’¾ Checking disk space..."
DISK_USAGE=$(df /home | awk 'NR==2 {print $5}' | sed 's/%//')
if [ "$DISK_USAGE" -gt 85 ]; then
    echo "âš ï¸  Disk usage high: ${DISK_USAGE}%"
    echo "ğŸ§¹ Cleaning up old log files..."
    find /home/steven/reddit-streaming/local_jobs/logs -name "*.log" -mtime +3 -exec ls -la {} \; -exec rm {} \;
    find /tmp -name "spark-*" -mtime +1 -exec rm -rf {} \; 2>/dev/null || true
else
    echo "âœ… Disk usage OK: ${DISK_USAGE}%"
fi

# Check memory
echo "ğŸ§  Checking available memory..."
FREE_MEM=$(free -m | awk 'NR==2{printf "%.1f", $7/$2*100}')
echo "ğŸ’¾ Free memory: ${FREE_MEM}%"

# Show current cron jobs
echo "ğŸ“… Current cron jobs:"
crontab -l | grep REDDIT_CRON

echo ""
echo "ğŸ‰ Preparation complete!"
echo ""
echo "ğŸ“‹ Summary:"
echo "â€¢ Cleanup script installed and tested"
echo "â€¢ Cron jobs updated with timeouts and cleanup"
echo "â€¢ Docker services verified"
echo "â€¢ Spark cluster tested"
echo ""
echo "â° Tonight's schedule (UTC):"
echo "â€¢ 11:55 PM - Stop streaming"
echo "â€¢ 11:57 PM - Kill stuck jobs & cleanup"
echo "â€¢ 12:00 AM - News curation (1hr timeout)"
echo "â€¢ 12:30 AM - Technology curation (1hr timeout)"
echo "â€¢ 01:00 AM - ProgrammerHumor curation (1hr timeout)"
echo "â€¢ 01:30 AM - Worldnews curation (1hr timeout)"
echo "â€¢ 02:15 AM - Start streaming"
echo ""
echo "ğŸ” Monitor tonight with:"
echo "  watch -n 30 'tail -5 /home/steven/reddit-streaming/local_jobs/logs/*.log'"
echo ""
echo "ğŸš¨ Emergency cleanup (if jobs get stuck again):"
echo "  /home/steven/reddit-streaming/local_jobs/kill_stuck_jobs.sh --force"